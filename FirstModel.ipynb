{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (0.58.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from pooch>=1.0->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from pooch>=1.0->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T17:06:49.448546800Z",
     "start_time": "2023-12-14T17:06:46.857156500Z"
    }
   },
   "id": "b64dc8b0ef914a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pandas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3b990d2c0efed8"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ------------\n",
      "anyio                     4.1.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.1.0\n",
      "audioread                 3.0.1\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.2\n",
      "bleach                    6.1.0\n",
      "certifi                   2023.11.17\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.0\n",
      "contourpy                 1.2.0\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.0\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "exceptiongroup            1.2.0\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.0\n",
      "fonttools                 4.46.0\n",
      "fqdn                      1.5.1\n",
      "idna                      3.6\n",
      "ipykernel                 6.27.1\n",
      "ipython                   8.18.1\n",
      "ipywidgets                8.1.1\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.2\n",
      "joblib                    1.3.2\n",
      "json5                     0.9.14\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.20.0\n",
      "jsonschema-specifications 2023.11.2\n",
      "jupyter                   1.0.0\n",
      "jupyter_client            8.6.0\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.5.0\n",
      "jupyter-events            0.9.0\n",
      "jupyter-lsp               2.2.1\n",
      "jupyter_server            2.12.1\n",
      "jupyter_server_terminals  0.5.0\n",
      "jupyterlab                4.0.9\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.2\n",
      "jupyterlab-widgets        3.0.9\n",
      "kiwisolver                1.4.5\n",
      "lazy_loader               0.3\n",
      "librosa                   0.10.1\n",
      "llvmlite                  0.41.1\n",
      "MarkupSafe                2.1.3\n",
      "matplotlib                3.7.0\n",
      "matplotlib-inline         0.1.6\n",
      "mistune                   3.0.2\n",
      "msgpack                   1.0.7\n",
      "nbclient                  0.9.0\n",
      "nbconvert                 7.12.0\n",
      "nbformat                  5.9.2\n",
      "nest-asyncio              1.5.8\n",
      "notebook                  7.0.6\n",
      "notebook_shim             0.2.3\n",
      "numba                     0.58.1\n",
      "numpy                     1.26.2\n",
      "overrides                 7.4.0\n",
      "packaging                 23.2\n",
      "pandas                    2.1.4\n",
      "pandocfilters             1.5.0\n",
      "parso                     0.8.3\n",
      "Pillow                    10.1.0\n",
      "pip                       23.3.1\n",
      "platformdirs              4.1.0\n",
      "pooch                     1.8.0\n",
      "prometheus-client         0.19.0\n",
      "prompt-toolkit            3.0.42\n",
      "psutil                    5.9.6\n",
      "pure-eval                 0.2.2\n",
      "pycparser                 2.21\n",
      "Pygments                  2.17.2\n",
      "pyparsing                 3.1.1\n",
      "python-dateutil           2.8.2\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2023.3.post1\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.12\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "qtconsole                 5.5.1\n",
      "QtPy                      2.4.1\n",
      "referencing               0.32.0\n",
      "requests                  2.31.0\n",
      "resampy                   0.4.2\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.13.2\n",
      "scikit-learn              1.3.2\n",
      "scipy                     1.11.4\n",
      "Send2Trash                1.8.2\n",
      "setuptools                68.2.0\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.0\n",
      "soundfile                 0.12.1\n",
      "soupsieve                 2.5\n",
      "soxr                      0.3.7\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.0\n",
      "threadpoolctl             3.2.0\n",
      "tinycss2                  1.2.1\n",
      "tomli                     2.0.1\n",
      "tornado                   6.4\n",
      "traitlets                 5.14.0\n",
      "types-python-dateutil     2.8.19.14\n",
      "typing_extensions         4.9.0\n",
      "tzdata                    2023.3\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.1.0\n",
      "wcwidth                   0.2.12\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "wheel                     0.41.2\n",
      "widgetsnbextension        4.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:49:41.852777100Z",
     "start_time": "2023-12-15T14:49:40.581658700Z"
    }
   },
   "id": "ea27f82e71df0973"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "data, sampling_rate = librosa.load(\"Ravdess/03-01-01-01-01-01-21.wav\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T15:53:11.306599100Z",
     "start_time": "2024-03-14T15:53:04.631287700Z"
    }
   },
   "id": "259d10840aea6080"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test to see if dataset can be used"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed7117ba57cf8259"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:162: UserWarning: pylab import has clobbered these variables: ['display']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    },
    {
     "data": {
      "text/plain": "<librosa.display.AdaptiveWaveplot at 0x14b58c0f670>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x400 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+oAAAFzCAYAAACpaPNwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABczUlEQVR4nO3deXhU5fn/8c9MlgkhO4GEQCBssu8IggsuKLhVf6XWfaEWqxWr0q+t+FXcWtGv1mordd9qpVq17goCCiggqyAgoOxrEgJk32fm98dkhiyTZJLMzDkzeb+uKxfJmTNn7uEkM3Of53nu2+J0Op0CAAAAAACmYDU6AAAAAAAAcAKJOgAAAAAAJkKiDgAAAACAiZCoAwAAAABgIiTqAAAAAACYCIk6AAAAAAAmQqIOAAAAAICJkKgDAAAAAGAikUYHYASHw6FDhw4pPj5eFovF6HAAAAAAAGHO6XSqqKhIGRkZslqbHjNvl4n6oUOHlJmZaXQYAAAAAIB2Zv/+/erevXuT+7TLRD0+Pl6S6z8oISHB4GgAAAAAAOGusLBQmZmZnny0Ke0yUXdPd09ISCBRBwAAAAAEjS/LrykmBwAAAACAiZCoAwAAAABgIiTqAAAAAACYCIk6AAAAAAAmQqIOAAAAAICJkKgDAAAAAGAiJOoAAAAAAJgIiToAAAAAACZCog4AAAAAgImQqAMAAAAAYCIk6gAAACEiu6Bcn206bHQYAIAAI1EHAAAIEfd+sEm/fXO90WEAAAKMRB0A0C488tlWzf5gs9FhAG2y9XCR0SEAAIKARB0A0C68sGyX/vntXqPDAAAAaBaJOgAAAAAAJkKiDgAAAACAiZCoAwDanYpqu9EhAAAANIpEHQDQrjy7ZKeG3L/A6DAAnzmdTj258EftP1ZqdCgAgCCJNDoAAACC6V/f7lWV3Wl0GIDPiiqq9bfFP2l3XonRoQAAgoQRdQAAgBCQX1ppdAgAgCAhUQcAAAAAwERI1AEAYe3FZbu05VCB0WEAAAD4jDXqAICw9ufPtmpAerzRYQAAAPiMEXUAQNjbll1kdAgAAAA+I1EHAISdartDD368RYcLyowOBQAAoMWY+g4ACDu78kr06vI9ctKFDWFs3qp9GtsrWX27sLQDAMJNUEbU586dq6ysLMXExGjcuHFavXp1o/tu2bJFU6dOVVZWliwWi5566qk2HxMA0D6VVdqNDgEImHve36SZ/9lodBgAgAAIeKL+9ttva+bMmbr//vu1fv16DR8+XJMnT1Zubq7X/UtLS9W7d289+uijSk9P98sxAQAAwtGuIyVGhwAACICAJ+pPPvmkpk+frmnTpmnQoEF67rnnFBsbq1deecXr/ieffLIef/xxXXHFFbLZbH45JgAAAAAAoSKgiXplZaXWrVunSZMmnXhAq1WTJk3SypUrTXNMADBKSUW10SEAAADAZAKaqOfl5clutystLa3O9rS0NGVnZwftmBUVFSosLKzzBQBGe/+7Axp8/wIdK6k0OpR25WhJhdEhAAAANKldtGebM2eOEhMTPV+ZmZlGhwQA+vrHPElSfimJejA5HEZHAAAA0LSAJuqpqamKiIhQTk5One05OTmNFooLxDFnzZqlgoICz9f+/ftb9dgAgNAXHdkurlEDAIAQFtBPK9HR0Ro9erQWL17s2eZwOLR48WKNHz8+aMe02WxKSEio8wUAQGF5ldEhAG1STJ0LAAhLAR9WmDlzpl588UW9/vrr2rp1q2655RaVlJRo2rRpkqTrrrtOs2bN8uxfWVmpDRs2aMOGDaqsrNTBgwe1YcMG7dixw+djAgDQnLV7jmnYA1/ou33HjQ4FAACgjshAP8Dll1+uI0eOaPbs2crOztaIESM0f/58TzG4ffv2yWo9cb3g0KFDGjlypOfnJ554Qk888YQmTpyoJUuW+HRMAACa82NOsSRpR26xRvZINjgawDd2h1OSdLigzOBIAACBFPBEXZJmzJihGTNmeL3NnXy7ZWVlyel0tumYABBKiiuq9cSC7Zpxdl/FREUYHQ4Ak/p211FV2V2fkV5cttvgaAAAgURFHQAw2IcbDumZr3boq225RocCwMTcSboklVWxNh0AwhmJOgAYrKLaLkmqcjQ/mwj+syO32OgQAL9wOp0qKKUwIgCEExJ1AEC79Mpypg4jPLy9Zr+GP/QFyToAhBESdQAAgBD2zY48SVJxJdPhASBckKgDAAAAAGAiJOoAAAAAAJgIiToAGKSi2iFJKixjuioAAABOIFEHAIN0iouWJDmcVHsHAADACSTqAGCQCKvF6BAgKTKC8wAAAMyFRB0A0K7F2aKMDgEAAKAOEnUAQNg6VlppdAgAAAAtRqIOAAg7MZERkiRflv/bHdQIgDEcDqe2ZRfKSZ0KAEA9JOoAgLATUbPuPMLLu1xxRd0q+zf/a10wQgIa+HxztqY89bU2HigwOhQAgMlEGh0AALR3R4oqJIlRNaCd2XO0RNKJ14D6qu0OlVbZgxkSAMAkGFEHAINFR7peipNiow2OBICZ/OG973X6Y18ZHQYAwACMqAOASdAkDEBt/11/0OgQAAAGYUQdAAAAAAATIVEHAAAAAMBESNQBAABC2N6jpZIkK+tnACBskKgDgEEKyqoabCurpMKzEV5YtlPZBeVGhwG0SlJslCQpgkwdAMIGiToAGKSy2iFJiq5p9v39gXwNnD1f6/cdNzKsdumRz7bpiS+2Gx0G2imn06k9eSXall1odCgAAJMgUQcAg7gTdPco2I85xZKkbYeLDIsp3CzYkuPzvo31sgYCxeFwSpKOFFfol8+v1PlPf93mYx44Xup1tg4AILSQqAMAABgguWO0JNdFu9yiCjmdbT/mWU8s0c1vrGv7gQAAhiJRBwCD7DlaYnQIAMJMld2plbuOGh0GAKCNSNQBwCBJsdFGhwAAAAATIlEHgCDberhQP3vmG9aRAmigyu7Q/7yzUbuOFLf8zn6YOg8AMIdIowMAgPbmv+sP6PsDBUrsEGV0KABMJrugXO+uO9Ci9eq7jriW0Xy08VCAogIABBsj6gBgsC9+cFUmX7I91+BI8MyXP+mT708kO2WVdn3FeYEBKqrtTd5+pKjyxPfFro4FPxyivRsAhAsSdQAwicLyaqNDaPee+OJHzZj3nefnF5bt0rRX1+hgfpmBUQENLdrqe+tBAEDoIVEHAKARe2sq81dUNT26CQRSRZXD6BAAAEFGog4ACBu7jhRr3COL9GNOkdGhAH7zxQ/ZRocAAAgyEnUAQNj4bl++cgortGrXMaNDAfymtIIZHQDQ3pCoA4DJWC1GRwAAAAAjkagDgMnExUTqq+25qrKzLhUAAKA9IlEHAJNZteuYpr26pk6bMAAAALQfJOoAEGT7j7lafUVFeJ/jXlBWJUnKKawIWkwAgsfucMrpdHp+Lq6gNSMAoK5IowMAgPYmPsb10tvRFqm84kqDo4E35VV2lZA8IUAuf36lMlNiNbpnsiTpwY9/aHJ/R62kHgDQPpCoAwAgaemPRzzfXzp3ufYdK9WEPp0MjAjhau3e41q797gnUW9OUmxUgCMCAJgNiToAAPVsy3b1YS+vchX0i4pgpRiMY7E03Qqistr1e1rlYOQdAMIFnzwAAGhEdKTrbTKykXoCZlRayZT99qpTx2ijQwAA+AmJOgAAYWLzwQINmr1Aq3YdNToUGKCZgXcAQAghUQcAhA130a0jRe2rYv6mAwU65y9LtGbPMUnSlkOFBkcEX5RX2Y0OAQBgUiTqAGAyx0tdleCp9NxySbGuqb/Rke1raPGjjQe180iJ9h4tlSRlF5YbHBF88QMXVAAAjSBRBwCTcReOirdR77OlrDX5+Xf78g2NwygJNa3/OkRFGBwJfNEpjjXlAADvSNQBwKSoNN567qrtAAAAoYhPgQBgMqxbhT/YadUFAEDIIlEHAJNZvfuY0SGgnlW7QuucVDscGnDf53pn7X6jQ0ELFZRVSZLKqxxtOk5FNRf8ACCUkagDgMntPFKs0x77UvuPlRodisfhgrJ2VVn9jrc3GB1Ci1TZnaqyO/X2GhL1UBVna1udAWpRAkBoI1EHAJNbsCVbB46XaZUJRtqdTqeq7A797Jnluvqlb40OBwhpTSXTH2w45Pm+NcthnE5pW3ahnGTsABCSgpKoz507V1lZWYqJidG4ceO0evXqJvd/5513NGDAAMXExGjo0KH67LPP6tx+ww03yGKx1PmaMmVKIJ8CAARdQVmVrnzhW+UVudq1WU3QcexPn27VuU8u1ZGiCv2YU2x0OAEXauu8q0Ms3vbO2/mqsjec8l7Rimnwi7bmaMpTX2vd3uOtig0AYKyAJ+pvv/22Zs6cqfvvv1/r16/X8OHDNXnyZOXm5nrdf8WKFbryyit144036rvvvtOll16qSy+9VJs3b66z35QpU3T48GHP17///e9APxUA8Ct3z+vGbDpYoJW7juq99QeCFFFDFdV2zf5ws3Jq+nK//M1u7akV95ZDBSqrDN+1sEt/PGJ0CE2qtjvqLIk4Wuy6qGMxwUUd1OVwOHX9K6v17a6jTe63I9c/F8D21fxe5BS2nyUqABBOAp6oP/nkk5o+fbqmTZumQYMG6bnnnlNsbKxeeeUVr/s//fTTmjJliu666y4NHDhQDz/8sEaNGqVnnnmmzn42m03p6emer+Tk5EA/FQDwi1AZ9Hx+6U69t+6g/rlyr15fscfrPhf+7Rs9+vnW4AYGj2eX7NTp//eVKqpdI662KNfbemTN9IuSML6IEmoq7Q4t/fGIHl+w3ehQAAAhIKCJemVlpdatW6dJkyadeECrVZMmTdLKlSu93mflypV19pekyZMnN9h/yZIl6tKli/r3769bbrlFR482fYUaAIxWbXdo39FSr1NbzWjO59t0z/ubJMmTCHpjhrXz7dWKna73vip73as/7p/TE2xBjwlNqw6Rv38AgLECmqjn5eXJbrcrLS2tzva0tDRlZ2d7vU92dnaz+0+ZMkX//Oc/tXjxYj322GNaunSpzj//fNnt3kcOKioqVFhYWOcLAIJt7lc7dcbjX9E2CUETYbVo5c6jpuoYAAAAmhdpdACtccUVV3i+Hzp0qIYNG6Y+ffpoyZIlOueccxrsP2fOHD344IPBDBEAGli5K0+SVG33be67u58y0JjDBWU137l+p4rLqxvsc+WL32pAerzm33FGECODL4JRkb20suHvBADA/AI6op6amqqIiAjl5OTU2Z6Tk6P09HSv90lPT2/R/pLUu3dvpaamaseOHV5vnzVrlgoKCjxf+/fTVxaA+RV5SbqA2mKiXL22oyNcb+eNVX3fll0UtJjgu40HClp8n04do1u0/0Mf/9DixwAAGC+giXp0dLRGjx6txYsXe7Y5HA4tXrxY48eP93qf8ePH19lfkhYuXNjo/pJ04MABHT16VF27dvV6u81mU0JCQp0vADDKmj2+rel2twaLiqCEtxl8uS1HlU2s1TeCpaa8e6wtJCfItXut+X0a3C2xRfsXVXDBDwBCUcCrvs+cOVMvvviiXn/9dW3dulW33HKLSkpKNG3aNEnSddddp1mzZnn2v/322zV//nz95S9/0bZt2/TAAw9o7dq1mjFjhiSpuLhYd911l7799lvt2bNHixcv1iWXXKK+fftq8uTJgX46ANBmqXG+FfjadNA12taRJMwUfvXaWj27ZKfRYSBEuWfIlNaqxB9hbXgRLreIdmoAgCCsUb/88st15MgRzZ49W9nZ2RoxYoTmz5/vKRi3b98+Wa0nrhdMmDBB8+bN07333qt77rlH/fr10wcffKAhQ4ZIkiIiIvT999/r9ddfV35+vjIyMnTeeefp4Ycfls1GdVsA5tfSHtfuZaxG9kO2h0pPuQArLDdn3QB3n3vOknm516N3Teqgn2p6pcfHNPwY9uGGg35/7BU78mS1WnRK705+PzYAIDCCMkwzY8YMz4h4fUuWLGmw7bLLLtNll13mdf8OHTpowYIF/gwPAEwtKTZK+45JtsiAT4JqVEKHKO08UtzkPvuPleqNb/fq7ikDZPUyUojAOXDMVVSuqIkLCV9uy1FFlUPnD/W+TAzm4O2a2LzV+3y6b3ZBuef7I7VG5q96aZUkac+jF7YtOABA0Bj3qQ8A4BMzpLzPLdmpc/6y1OttecWuhODpxT/phWW7lFdi3Mh/e5s2vPWwq93o6pq6B0u2H5EklVU2bAH4q9fW6pY31wcvOHi17McjLb7P7rwSn/b7fPOJVravrdjT4scBAJgHiToAoFmV9uaLXjU1mhssc7/y3v0DCBcFpZVGhwAACAISdQAwOUtLF7UHmcVi0eebDqvKx/7wgZRfavzFAiCQ8opJ1AGgPaCUMAAEWUv7oyd0iApQJP5xpKiiwZTqKrtDURFcCzaa0/hrJ/CzSNo1AkC7wKcoAAgSd5Eo95puXx3OLwtANE1bt9e3Xu/e3PPfzbro79/4MRrfjeyRZMjjms2hmt8Zh5dM/e01+3T1S98GOyT4KJJCjAAAkagDQNC4P39bWziV3RYV/JfqzQcLW33fRVtztD27yI/R+O7M/l0MeVyz6WirO2Euptbv0H0fbNHyHUe19XChfv36GlVWN19/AMHT0tcHAEB4IlEHgCApLGvZlHe3Hw65kuZD+WW68oVvlVtU3sw9ABd3EcDqWvUD3NueXbJTi7bm6qiBVfrR0A+Hm75I1tKlMwCA0ESiDgBBktChdWVB3FPmv/ghRyt3HdXKnUf9GBXCUY+UWElSaU2btujIhm/39ppfrKLyat327+/q9N2GeRWWUTARANoDEnUACBHu9cbHSgJf9Znp0Oa2O69E76070Ox+7os8CTGNFyT8+qc8fbzxkD7ccNBf4SGA4mKoAwwA7QGv9gAQIuJjoiSVBWUNq7ciZDCPmf/ZoO/25Td6+zM1/eQ37m98n/o45cEXG93yj2G0IASA9oERdQAIEVG0ZTLcriPFRocgSfopp2VxZBc2X9fgu/3HNfKhL7Qj98SxX/5mt77cltPi+OCbBEbHAQCNIFEHgBDjXlscSOVVTH33Jgj/9QER0UTLL1vN+vUl24/oeGmVZsxbr7ziCjmdTj38yQ/61WtrgxVmWCs0SRE4J1MnACAkkKgDQBCU1RT1aouoCNdLdo4Po6NtVVQemtNr9x8rDejxoyKsKiitMizZWbAlW7//z4YW369znK3BNnfxOPdSCnf/7m3ZRRrzp0V6ZfmeVseJhsywnGR7dpEGzV6gLYcKjA4FANAMEnUACLAvt+Vo4Oz52ne0bUlkbHREzb+Bny7bOb5hYhcKEjo0XjTNHxZtzdHwh77Q37/c4fdjF5RW6fynl2nTgcaTqN//Z6PeW++fom/uUfbKatdFpCp73UTy4U9+8MvjwDy+P5Cvsip7k/UNAADmQKIOAAG2Zs9xSdKhAtdIeHUr50+3pDCYGdz7/iZ9tPGQ0WEExDMBSNR3Hy3R1sNFem9949Xciyv8N306pWO0JOnFr3dLksqrG5/1YXc4PZ0AgrH0AgCA9o5EHQACzD1lva3MssbVV/9atU+/+/d3QX3MYE1Jdye5RvFHwn6k2DX1/WB+maSmq77/7q3vNPXZFXp7zT4NuO9zVdupYRCKTDD7HgDgIxJ1AAiwOFuE0SGEvcMFZTplzmJtOsjaW2+8VX1fvfuYz/f/9PvD2nSwQHM+36Yqu1NF5dV66etdnlF2hIadJulaAABoHn1BAAB1FJRVKbem0Fio2J5dpOyCcmUXBL7QXnuWEBOl/NIqfbjhoP706VZVVDt061l9jQ4rJBwvqTQ6BL2yfLfRIQAAfMSIOgAEWKgt6Z3+z7V6+Rs+0Ie7+Fb08M4vdSWb7mnzjy/Y7teYwpnF0rBFXiAviMVENfyIV79gYEu9uGyXnuCcA0BQkKgDQIAlBrgSub+1ZEq0WQR7/b63qeShpjW1E9z/z//6dp9n2/g5i/XO2v1+i6s9+WZHXsCOHRPlWnKTkRjjt2P++bOteuYr/xdSBAA0RKIOAAEW4WUkDf41b9Veo0MIOcfaMBW7olaF+MMF5brr3e/9ERL8yP26M7ZXisGRAABag0QdABDyKGrWch2j/VvkcHdeiSnWYaOuvcdK/X7MF5btVNbdn2r9vuO6dd563f7WdyquqNas/27idwAA/IREHQAC7NmlO/16vJLKas367/cqKKvy63HN4oPvDmrf0eaTi9yicuUVV+ilr3dp/b78wAcWZkoqG++b3pxqL2udz3piiX7zxjo5nU56rZtIQkzDpTdtPT+PfLZNkrRk+xF9+v1hfbjhkJZsz9W/V+/TRxsPtenYAAAXqr4DQIDtzivx6/GWbM/VjznFSk/ooNsn9WvVMZxOp5bvOKqxvVIUHWmua7Z3vL1BY7NS9J+bxze538//sUK2SKt2HvHv/6+vPtt0WMdKKnXNKT0NeXwjJcVGK6+4YSG01XuO6dqXVyu7sFyLZk40IDLU560lm78upHzz0xHP919syZEkVXORBgD8wlyfzgAAzUpLcBWHioxo/dr3rYeLdM3Lq/T2mn3N72yAbdmFze5z4HiZYUm6JP32zfW694PNhj2+kYrKG5/N8c2OPO3ILZbT6VR5VetH7dE2R2umoKfG2Rrc5nD6nkw7nU79e/U+r+e89kyWtXtCrwglAJgZiToAtEMlla7q3fsCsH7VHwrLq2V3OPX3xT9pl5cRQRirwoeaAC99vVsD7psvByOsppMcG+3zvj/lFmvWfzd5prs35lBB6HdCAAAzIVEHgHbMbuIabB98d1B/WfijHvj4B6NDQSvMXeJq4+VwOikwZjItaURRVfMiYYbZEXaHU9V2h3KLyvWLZ1dov0kvNAKAP5CoA0CIcU9bPZhf1qr7Hy2u0OaDBZKkV5bv1sqdR/0Wmz8dOO56fmtCsK87pPxS11Tpr7Yf0ciHF2o159EQP+UUten+VTWFA60maDP5q9fW6IZX1+jbXce0du9x/aumLaOzBVP5ASBUkKgDQIhZvsOVWLe2vdb0f67Vg7VGqc26Tt0tpaPv03RhPou3uoqMMfppjLZU968tPsb4+sNLfzyib3bkKamDq5K9LcKqL7Zkq9esz3zqFAEAocT4V10ACGOlNWvBzeJ4SWWTrcwqqo2f3tocp9OpZ77cYXQYHkeKKhQfE6mYqNb3Ja+2O8J2VPCtNfuNDgH1HC8N7daO7sH9/6w9oFU1MzUOF5QpM6WDLCYY+QcAf2BEHQAC6LqXVxsdQh0ff994j+M1e46p/73zgxhN6+SXVukvC3/0epvNgFZzYx9ZpLve/b5NxzjnyaV69HNXsS5vCfvOI8W6bd76Nj2G0cx20QqBEcx2j9mF5Z5E3e50augDX+jddQeC9vgAEEgk6gAQQGv3Hg/4Y2zPLtJv31znKfrUlKb6J6/YEZi16uv3te7/4MdWrK31pRq5vzmd0scbG78A4ou9R0s9Ccf+4w1rDzy3ZKc+/v5wmx7DaPd9uCUkZmy0B76MOecVV2j/sVKta+FrWGwbZpY05s1Ve/XMlz81uY/d4VRxRbXeWLmnzvaPNh7SeyTvAEIQU98BoAn/+/4mpSXE6Hfn9DM6lAYOFZTrhWU7telgoT7blK37L6709Fj3ZsXOPL309e4G28urHJoxb33Apoy+tXqfeqbE6va3Nuivl49Q5/iGfZ29+XRTaCemrZXgZS3wzjBpUdf/3vl6+JLBunZ8ltGhoBlTn12hI0UVKq1Z497URb5A+9/3N9f5udLLBbnGVo787t/fSZKmju7u97gAIJAYUQeAJry5ap+ebGSatdE+/f6wHvlsm37M9m3k+aZ/rvNaKX75jjx98v3hNo8KN6ZbUqzeXXdA3+zI03vrmx7ZCtd12m2VWFM8Kxy8vZY160Z76JO6LQ+PFFWopKLu0oS9R0s9Sbokn2bsSNLv39nY9gCbUVTecBlFYblr3f3GAwUBf3wACAYSdQAIcdt9nCJeXOF9jXBRI9v9KbamQn1CTOMJZ3mVXd/78CG7PabyO4+UGB2C33SJb3zWB4Jn79ESHTjuqpQ+6cmluujv3zS5f6mfqscHSv1rfCt3HvW500B5lV1j/7xIq3adWP5TbXfoWEmlP0MEgBYhUQeAMFFUXqV31u4356h0zbT6poq9/eHd73XJ3OXNHuqGV81VoC8Y9tHaLKSVVFSbbn3+xMeX6LTHvpIkFZRVaXde0xeDYlvZDjIQvM0MerOmp7okbTpQoCtf/Fa/eWOdT8c7UlSh3KIK/a3WOvg/fbpVox5e2PZgAaCVSNQBIEw8vXiH7nr3e9MldXYfLxx81MjU+7KqugmOL6PuRvipFcXv0D5cOne5bnx9rdFheOXtwp63bVERvn9kPHC8VNe9vErHAzQi/fiC7Q22fbvrmOf7oppp8D8cLmzRcZM6RHu+f23FHkkN1+a/uny3/vGVedpDAghfJOoAECYO1kxjrbKba0Td/aG5tdKbKJDXGoHqsnzuX5dpd16JVuzIa9Nx9hw114UWf/tyW67OfPwrPfDRFqNDCZqfcou9FkAzg3vqFWqT6k4j71JT/PGNb/c22K8xL329W8t+ytOirTlateuoPmmiLaRZpSW4nvfhgrqj9w9+/IP+z8uFAgDwNxJ1AAgT6/flSzJfQbbltRLXBz7aoi+2ZLfo/i0dFWtOIP93bnx9ja56aVWbjrEjNzwqvDdlz9FSz4gljPXv1fvq/OyoN4J81bgeLT6mu3NBfEyULn/hW82Y913rAzRITmGFJOmP731vcCQA2isSdQAIMwVlVVr4Q06dhN3XokqB8GNOsQrLXKPqRRXVuumNdSqvMtd6XX/Z5Yeib8UV1XpqkTk7DSC87T9Wqv73fa41e05MI7e2oW1jblG5P8Lyi/pV7X1VbbIZSgDaDxJ1AAgzzy3dpen/XKvtOUWyO5ya8OhivfxNw/7pwVS/tZPJBv1N56lFPzW/E+BnC7Zkq8ru1Nc/nZgF8+66plsqevP9QVcdiU82Hm51LMUV1dp7tHUXvo6VNlwb/68WTN0HADMgUQeAACkzqJ3Roq05kiSLLKp2OHQov9zwacaR1rqjcit25qmgzPe16421lgPgP+6CcT1SYj3bWlOccsn2I5Kk1bVG5n/x3Aqt23ussbs0cNc7GzXx8SUtfmzJ+yyADfvzW3UsADAKiToABIjD4GHjl77epVeX7zE0BreKeoW0bnx9rS5/fqXP9x9y/wIt/fGIv8Myha+25RodgmHqV9SGsd5b7xo9j4o8kej2T4vzy7HX7jmuqc+u9HnZy+ebXbUsWrNM5uufGr5WdIhytZd76etd+rZWv/TaWvuafTC/rF3/HQMIjEijAwAABMY7rZiyGiibDjZsqfZjC9uZXf9KePZPX7HTe5X4Q/llykjqoKfDeL16n3s+055HLzQ6jIAyW//0pnhrfRhh9e+Yzr++3auJJ3XWK8v36MGfDVZ0pPfjWyyuJTKtWVv+79X7G2xLio2W3eHUnz7dKkm6Y1I/FZZVafbFgz37uC8O1LZqd/OzAH729290tKQy7H+XAQQXI+oA4IOL/vZ1q4sR4cRU2NrCeTC116xPtXaPb9N8GxvEO1Lkqjr91zBfr/7S17uMDiGg6s8mCQUrdpwYcY6K8G9Dwz99ulWvLN+tf6/eV6doXX3+npAUGWGpU1TzqUU/6ZWaGUelbVymdDRA/eIBtG8k6gDgg82HCltcVOmzTa0vpNReZN39qWctf+11saHO6ZR+8Zxran9uUXmDlle1HS4wT2VsI7hHOGEetWfjtGaNenOKK1x/841d/FxRq6VjtZ+u6Dmdzkantv9w+MRMArvD2aDFZWF5leZ8ttWwuiMA2icSdQBoRP3E/Lt9x1VWadf/vLNRB/PLtPdoiewOp/bklWjo/Qu072jdD7R3vUv/XV/M/nCz8ksrNaRbgtGh+N2KHXka++fFen5Z46PGecUVXrdf8/KqJkccERqOFYf2aGtHm/9XSX688ZAkKbuwXJVeZhzUXkN+7pNL/fKYuUUVsjTSau7Otzd6vq+yO/RjTnGd21/6ereeX7ZLi7fl+CUWAPAFiToANGJ+vfWKH2w4pM83H9a76w7o1Ee/1MTHl+g3b6zTOU8uVVFFtaY8vcygSEOb1WLRuEcW67NN2bL6d5at4a56aZUk6bH52/TNT3laubNhEavGEqGi8mpd9pzvBfdCWdbdn6qg1PcuAKGkqYs0oSCQo8izP9yik+79vMH2pNhoz/eF5f5ZcnQov0zHvExRr79E5YGPtuhwQVmdbX9b7Fp+UruafE5h0zNhKqrt2pFb3OQ+ANAUEnUAaERhecPEYeZ/Ntb5edHWHE/l6tJKe4MPeGjeZ5sPe9bx1v4gHGY5u655eZWufPFb/eq1NXpj5R7lFpXL6XTq4uFdjQ7NFKa9trrJJQKhqrW9wM0iGOuvF/2Qo6nPrlBltUNTnlqmtS1o4+arzQcL9cbKPQ22/6LexbC31uzXDa+uafZ4D33yg6S6XRu2ZRd6ps3f98FmTXpyqbLu/tRrFXoAaI7FWX8hTjtQWFioxMREFRQUKCEh/KZaAvCPK1/4VisbaePTlMtGd9f6fce180hof0A3QqTV4rc1qaHgzP6dddXYHrrpjXVGh2IKS+86U7M/3KIrx2ZqypDwuIAx9dkVWrf3uNFhhIT3bpmgqc+uMDoMn+2ec4F6zfqswfatD03RwNnz62zb9cgFsobblCEALdaSPDQoI+pz585VVlaWYmJiNG7cOK1e3XSLnXfeeUcDBgxQTEyMhg4dqs8+q/si6HQ6NXv2bHXt2lUdOnTQpEmT9NNP4V0VF0DwtbaI0jvrDviUpPORDUu2HyFJr2Xi40u09Mcjuvlf67X5YEGDHtpvrtqr3765TpXVDu07WqoHP96ijfvzjQnWR22tKN5aofj6EkpJuiSvSbqkBkm6JL2yfLckafPBAj296CeVVtJFBEDTAj6i/vbbb+u6667Tc889p3Hjxumpp57SO++8o+3bt6tLly4N9l+xYoXOOOMMzZkzRxdddJHmzZunxx57TOvXr9eQIUMkSY899pjmzJmj119/Xb169dJ9992nTZs26YcfflBMTEyzMTGiDsAXd7/3vd5as18WSe1njBdwMdPvfae4aF0+JlP/WLLT6+13nz9AE0/qrIFdT7ynO53ORouHBdPcr3bo8QXbjQ4DJnTX5P56fMF2vXfLBA3rnqhIq8UUv7MAAqcleWjAE/Vx48bp5JNP1jPPPCNJcjgcyszM1G233aa77767wf6XX365SkpK9Mknn3i2nXLKKRoxYoSee+45OZ1OZWRk6Pe//73+53/+R5JUUFCgtLQ0vfbaa7riiiuajYlEHe1Zfmmlth4u0vg+nYwOxfR+/fpaLdme266mYgPh6DcTe2vZj3m6/Zx+OmdgF5VW2rXwhxylJ8To/xZs00lp8Zp2apZOSotXVIRrsmFziX55lV2/+/d3Oq1fqq4bn9Xofh9vPKTb/v2dv5+SKZjpYk64+uDWU7Ujt1iXjshQWZVdURFWxURFGB0WgFYyTaJeWVmp2NhYvfvuu7r00ks926+//nrl5+frww8/bHCfHj16aObMmbrjjjs82+6//3598MEH2rhxo3bt2qU+ffrou+++04gRIzz7TJw4USNGjNDTTz/d4JgVFRWqqDjR/qawsFCZmZkhkahPenIpVUMBwMRIVgAAMIc9j15odAhNMs0a9by8PNntdqWlpdXZnpaWpuzsbK/3yc7ObnJ/978tOeacOXOUmJjo+crMzGzV8zECSToAmBtJOgAA5hBOddK9N28NM7NmzdLMmTM9P7tH1EOBt6tCDofTUznU6DV4TqdTdodTETXrqloSj/t+7uM4nWq0ImprnqfD4VSVwyGLLIqKcN23dozejll/W2W1Q9GR1kZvdz9Opd2hmKgIz4uDex/386p2OGW1SJE1UyqPlVQqqUOU5/k6HE65D1v7vmVVdsVGu/5Mi8qrFB1pldOpJqe9rdx5VMO6J3p6M7uPbbFY9PqKPbr/oy2SpFennaxBXRNUWFalbskdFBVh1dHiSnWJt3n2d9+/2uFUdKS1zrHc/xf2mufmdEp2p9PrGruKartska2bqlf7MSWp2u7w/D+6lVRUq0NURIPY3P+Pu/NKas6BRT07xaq4vFrZNT1w+6fFe/7v3ecqp7Bc+46V6uqXVinCavG0XwMQ/kb1SNK43p306vLdumpsT00a2EWp8TZJ0rq9xzUkI1EdbRG65V/rtT2nSJL0399O0JaDBeqeEqvSCruGZyaqU0ebiiqqdPMb67R+X76Bz8h/mD1iDrec2UfDuiUq1hapjtER2nSwQMO6J2n5jjxtPVyoG0/rpQ7REcotqtCZJ3Wu8169ft9x/ZRTrFP7pio2OkIpHaNV7XCqvGZavS3Sqo0HCjQgPV5VdodnKUjtzx3FFdWyO5xK7BAlp9OpimqHbJFWWSwWVVS7iie63/O9fW5yv2dbrRZPS8ban4esVovn82X99/v6+xvxOdjfj2n0Z3l/Ka+ye34P6qv/HOt/TmvstubUzonCUUAT9dTUVEVERCgnJ6fO9pycHKWnp3u9T3p6epP7u//NyclR165d6+xTeyp8bTabTTabrbVPw3Rq/0Ia/YdtsVgUGdG6eCLqPY+m7tqa52m1WmSzNkwO3cfydsz622on6Y3dx2q1KKbmcerf7n5e0fVeRFI6Rjc4hrdY3Em6JMXHRDXYx5v6a89rH/v6CVm6fkJWndvTEk4UYExPbFiM0Wq1eOL39rvnPo8Wi2RtpM5wa5P0+o8pqcGbtiTPRYn6sbm/7905rs7tyR2jlVzvHLj2dZ2rzJRYZabE6sbTeunlb3a3OnYgkEiaGlp211nall2oswd00ZMLf1RWp466dGQ3RUdaVVhepQQvr6NlNVXZO0TXfZ3645QBDfY9KS3e8/2CO8+oc8F5VI/kBvt3iI7QNaf0DJtEnd+3wPnNGb1lsVj0h8n9VVxZLYtcgwWd4pr//DomK0WSNLpn3d/BwbW+d/+eju6ZotE9U+rsFxVh8STkkjQiM0lS44MCcbXecy0WS5396r/fe/vcVPs9u/57vNV64jNa7c+Xje1vxOdgfz+m0Z/l/aWpQSRvn499ua054ZykSwFO1KOjozV69GgtXrzYs0bd4XBo8eLFmjFjhtf7jB8/XosXL66zRn3hwoUaP368JKlXr15KT0/X4sWLPYl5YWGhVq1apVtuuSWQTwdAO7P5YIHRIaCV2kMSG+7Pb/LgNF0/IUuDuyaqtKpanTradNK9n0uSTuubqv3HS7X3aKn+OGWAbjmzj+d+PTrFSpL+UC/R9pakSw0T9JaI8OFD4ocbDrX6+Agvd03ur7lf7VBppV1jeibr7d+M19o9xzS6Z3Kdi9CN/a4CaF8CPvV95syZuv766zVmzBiNHTtWTz31lEpKSjRt2jRJ0nXXXadu3bppzpw5kqTbb79dEydO1F/+8hddeOGFeuutt7R27Vq98MILklxXWe644w796U9/Ur9+/Tzt2TIyMuoUrAOAtjpSXNH8TvCrSKvFL1X2QymJHdotUZu4KCRJevjSIbrvg80anJGg568d49meKFfisu3hKSosr1KXeNfsn/rLk8woj9cRn835+VDN+u8mo8Pw2fu/naD/94+Gvd9nnNVXz3y1o862Nf87SZ3jbbr1rL51to/rTQcWAN4FPFG//PLLdeTIEc2ePVvZ2dkaMWKE5s+f7ykGt2/fPlmtJ95kJ0yYoHnz5unee+/VPffco379+umDDz7w9FCXpD/84Q8qKSnRTTfdpPz8fJ122mmaP3++Tz3UAcBXybHRkkpadJ+bJ/bRH6f0V1mVXYNmL/C6T/3R3to/NzUS3B5GiduT30zsrZvP6KPVe47pN2+sMzocU/jZsAxdNrq7IhsZqY6JiqgzvdLsSbokJXYI7uhoKL9OnNW/S0CPHxsdodKapQ6tMf30Xnrx6xPLoUb2SNZ5g9P0xRbXks1eqR319ytHaki3RH38/SHtPVoqSbpybA91jg+fJZgAgiPgfdTNiD7qAHzx69fXaNHW3DrbJg9O04ItOY3co24ByKy7Pw1YbOHk0hEZ+qBmerC/RtTN6KS0OP2YU6wNs89VfEyUZ9r0Qx//oFeWUwth0cwz1LdLfPM7hphrXlqlb3bkGR2Gqe2ec4HyS6uU3DFamw8WaO2eY3rg4x/8+hijeybr16f10i1vrq+zPT4mUkXl1XW23X3+AD36+bYGx/jH1aP025r7z/n5UF05toekE6/1u+dc4Flfuz27SDPmrddjvximwRkJbarVAiB8mKY9GwCEsoR6I2EdoyM089z+dba9c/N43TGpnyTpzkknBS22cOKU9NfLhystwRa2SXrv1I764s6J2vPohUqKja6ztnnr4cJG73f2gMCOMJrF3KtGhWWSLknXje/Z5mMYVS6pQ1SEuid3CNjx42Mi9dw1o2WxWDwFPod0S1RxRXUz92y5tASbenXu2GD7pgcm1/l5xd1n65R609EnDXT9HdZ+fXIn6bXVLoLVPz1eC2dO1KgeySTpAFqlXbRnA4DWeOiSIfrv+oOen6eO7q7+6fEN2iaenJWiq8b1UOd61Xn7dO6onUdaNnW+PbpsdKZO65eqjzceVk5hbvN3CCG751ygvy78UVNHd290n8amb39466kanpnULmZmXDisa/M7hahxvdq+Btmoy1dlVXbll1b59ZjxMZHqHGfTrrwSzfn5UE0Z0rALUO0K5C9dN0a//ufaNj9uUmx0ownzaX1TPbMeUuNsioup+/F4+um9tWhrrvp1ifN2dwAICEbUAaARcfXavl01ruEIiluX+JgGLUXeuml8QOIymj9H9z793Wk6rV+qJOnLbeGVpJ8/JF0Wi0Uzz+uvnp0ajuS5BXLEMhTUv8AVbiwh/kmrV2rjv7utUVRerayaYzbWdnTaqb0834/skeSXx42zRaqx1Z61W/FFR1obVF0f17uT9jx6oQZ2ZbkkgOAJ8bcPAAiORTMnakB6yz6kxbah7ZNZtbZQlbf/i6gIiwZnJLY5JjPa8uBkPXvNaJ/2bez3xD09PspLL+FwsuSuM40OIaACefYCdewHLh504jH8/CBTBqdr8mBXQeE+XqaiS4ErEphSM71ecl0gi68ZOU+KpR0aAPMhUQcAH/RlyqOk1k/B/dnwjAbbquwtO9qEPp0096pRrYwguDrafF9ZlhQb7XW7e/Ru1T2T/BKTGc29alSL/q9CUWOjxv4QqCnxibUSV39Pff/NxN66/OQe2v6nKeqeHNvs/lF+StqPl1R66kNcNrq73v7NeH0847Q6+3i7QHByVnKzx77nggE6vWZmEAD4C4k6AISpd28er7X3miPJqz+9++SsZP39ypE+3/+Fa0dr3vRTwnIt8zWneC825h5Rrz0KGG7C8XyGsvsuGtRg2+GCMr8d/6JhXTWyhyvxba7AWkLNaHfH6JZfyPnH1Y1f0Nvx5/P1f78Ypm5JHTxT8N3OHZjWYH+rD1MKbjqjj964cVyL4wSApoT3ZWwAMFDtfs/BlNghSgVlVYqwWhqstTSLf/5qnDq0YGnAeYMbFpwKF8Husw00przK1WP8UH65Z1tLZ7405us/nKX0xBif93/u2tFatetYnQ4JvqqsdjTYZq9Zn16/lggAmBUj6gAQIK35gOkPD10yWGOzUtSnS5yiI616/Vdj9cGtpxoSi1tppd3QxwfQvCHdXDUjahfS/O2ZfVp8nJvO6C1JevjSIZ5tmSmxdaq5N2dCn1TdeW7rWl52ims4C2XahF5e9gQA8yJRB4Aw06dznP5z83jPaPrEkzqrf5qxPaq7Jp2Y+h5ni5QtQMWiwsW2h6cYHQLaodP6puqxqUP1i1rtBFszM8g9S6SjQQU1I7yMmg/t3rrClRVeRucBIBiY+g4AYWJY90R9f6DA6wfrlkwz97cbJmR5vp/z86G6cmzjbe68GZBu7EWGlnj4ksFas+d4m45xer9Uw5ZNBJNBE05Qj3upjOSaBXT5yT3kcJyY7r7zSHGLj5lb6Jo6b3c49csx3XW4oLyZe5hPh6gIlVXZ9csxmUaHAqCdIlEHgDBh1FT75tiiToyeR7dg6qtbcUW1P8MJmMvHZOra8Vm6dnyW0aGY2tkDuujykzPVpzOdFMzgrZtO0flPf11nW+0B6Q83HJLk6mf+3b58n445skeyXl+5V/3S4nVZiCa6ZTXr9X85pnud7V0TY0z7WgsgvDD3EADCxK9P662+XeJaVLApGGKjfLsm3FgvY2+FoczosV8M88txOoVxlXe3yYPT21XLw7lXjdJtZ/c1Ogyv3G0Aa/NWcG1oN9+njv9seIYWzTxDw1s53bw5E/p0avL2akfrCuA5nCfu514uFFnv4uJnvzvd8JofANoHEnUACBNjspK1aObEOoWgzKapj88vXjdGneNtDbbXL0D1/LWj/RwVEFgXDuuq0/t1NjqMOk7pnaIrTvZ9tLsl/dStVov6dokPWIV1b9PR/3r5cM/3J2el6JIRGfq/qb5dPHPP+klLOHGR88nLh+veCwc22De5Y7RS4xq+TgGAv5n30xwAIGyU1Exfd6+F9ebkrBQtvetMDZq9oMljjc1K8WtsoeCs/p311fYjRofhFxXVdAAwg7duGu/5/uFLBqtHp45N7C1Fm7wAZKT1RHwdoiP09BUjJUl/eO/7Zu/bJT5G//nNeA2rNQNgcEaiBmcEZkYAAPjC3K+6AGCwqAiLenaKNToMrzrH2xQVYdE5A7r4tP/V47wXcRvVI0mSlJnSwevtbVVaWe2ZNpuZ3PRjxEZz/dgbP7WyNgXOsfGuqvdacO34LE08qekR/6gI30bHf3Vq4NugeWvzFltTMDPVS2s2X4ztldIuijgCCB28WwJAEz6//QxDK6Y35dIRGbrngoF6+ZvdWrwtt9kPmQ9fMkTdk2P12Pxtdbb3SInVP64erVeX79bzy3b5Pc44W6RO7ZuqVfecU2dqKbzzlpM7neGRqT986RCdOzDN6DDavb4+FPL725UjlVNQroU/5Gj1nmOyRfr2Ojg4o+Ga97YanpmkI4XlOlRTPT4+puHHV/eIf7ekuhcDT+3bSaWVzOIAEHpI1AGgCWYveGWxWDTt1F6aPDjd07u4MVarRTFR3idSpSfGqHfnpqe+ttZNE3tLUouT9DsnnaS/LvqxwXYzpqzN/d+3hLdCWOcNTtfXP+X57TGM8Msx3XXtKT2NDgM++tnwDEnSqJ7JmvrsCp/vF4i/z3/+aqyq7A6N+dMiSXWr0rtZ5NpYv5vAqzeMldOUrxoA0DSmvgNAAL103ZiAP0aE1aLMFN+m5w9Ib3y06+LhGV6LJ7WVryNxvoqNjmi0B7cRbZPuv3iQXrq+bef5uWtG6YnLXMWwOnspVHXNuB76dtY5bXoMo43r1XSlbgRPeQvqBLT0b+p4SWVLw2lWYoeoZgu4RUVY9PcrR+qeeq9h0ZFWv78GAUAwkKgDQABNGmSuab7jm2hrFBsdqV+f3juI0bROTFSEdj5ygdfb7K1sy9QW007tpZPbWOBuypCuTc7esFgspmu756tAtehC66UHcAmKNYgXy64e10N3Te4vyXVB4eLhGVRkBxA2SNQBIERVtbLC2IvXjdHArvGen3ulmnt6f6W9YR/1QLV9gv9NHpIuqW6PaqAt3GvO42IiddXYHnrwZ4M1skeywVEBgH+xRh0AQszp/VL19U95nt6/LXXuoDQlxUbpsudWSpJuOsOco+jugTkDZrPDj644uYeOFlfqZyMyjA6lXRqRmaQN+/NbfX/3BZYjxRV+iqj1/j39FDmdTs+o/aCuCUruGK3rJ2QZGxgABAAj6gAQohJi2l7AbPrpvUxb1X7q6O6KtFo046y+RoeCVpg00NU2MCEmUvddNIh1wiEqoabCeoQJZrGM79NJE/qmalyvFH0041RdPIyLPwDCFyPqANCOWU3w4dubhJhIZSR10LaHpyjSS89kmN/9Fw/WxP5dOH8mVNKCdmWpcTalxdv0/0Z100cbDwUwKt9ZLBYN655kdBgAEFAk6gAQYgrLqiS1rbf2wK4JOqt/Z10xtoe/wvIrd6VpMyd5J2clq6SiffZnTk+IUXZhudfboiIsqrI7lZkSSzs2EyipqG6wraq6Yd2HxiTFRmvV/05qdr+BXRO09XBhi2IDADSORB0AQoyrH3mBBme0vpp2nC1Sr04b67+g/GhQ1wTd4MOa08emDlWk1ar7P9qiYi/JSKC9fdN4r/2c24PjpY234HrxujF0rTaBTh2jdbSkUh1tgfuo97tz+ulvi3+SJF01rofu+2BzwB4LANobEnUACLDaH2b9YXBGov5+1ciwXfP72e2n+7Tf5Se7ZgP8fFQ3TX12hdbvyw9gVA0Fsw1VIERYLa1uZ+ftAsWdk05S//R4ndm/Sxsjgz9VeemaEB3ZtpkqqXE25RVXqHP8iVZoIzOTJLkutAEA2o5EHQACrHtSB78fM1yT9NawWCytblXXnsVEWlu0Vrk5t0/q57djwX/6donTlkP+nZK+eOZEPb9spy4b3V3jeqWo2u7UoIwEbX5wsuICOIIPAO0Jr6YAgJB3Zv/O2nSwwOgwQkpbkvTyqrqjtEmxbe9AgMAoLvf/spDE2Cj9YcoASdJJafGe7STpAOA/5q3SAwBhosiA9dPtzeieyUaHEHK6tWGmx4XDunq+f/umU/TtrHP8ERL8qKzKdSFm8bZcvx2zX5c4pcbZmt8RANBmJOoAEGB2h+8Vls3gxevGqHuy/6frh5P0hBijQ2izg/llLb5Pl5o1ye51yL1SO2pc706KiWIpRnOqvawV//nIbgF7PEcbukI0Zt70U/TRjFP9flwAQEMk6gCAOs4dlBZybbXcU24Z7QssdxGyCX06qUu8Ta+btHOAGSUGeXlA/eUJkvSz4RltOmbneJsyAlBzAwDQEIk6AARYAAa2UM+oHsl67prRmjoqcCOU4aYlRevdfe17pXaUJA3KSNDq/52kHp1iAxFaWIqKMP4j15BuVGQHgFBB1Q8ACLCjJY33nA535wwITqsuq9WiKUPS9cOh4BSUC8S04paIs0W2qHd8ekKMsgvL62w7f0hXfbrpsE/3/89vxqugrFJDuiXqm5/y6DoQopJjo40OAQDgIxJ1AAgw94fjtvStlqTT+6Xq65/y/BVWwH39h7OCXw3cW4PvAEjp6P+EJ87mSn6zgjRK7ZTrd3H66b304te7FR1pVWW193oKtYv1/XxU96DEBwBAe0aiDgABdtW4HrJYpMVbc7Rmz3FFWi2qbkXC7k7y3QlWILkrRrdFZkrwp0U7AzzSPax7oi4c2lWTB6f7/dh9u8Tr39NPabKC/UXDuuqT730bBW/OjtxiSSeqv9si6ibqFw/P0McbD/nlsWAO8TGuj30JHWinBwBmZ/yCKQAIc4kdonTzxD6edb6t5Z7q3JZReV+VhGhLudZcAGmJLvEx+s3EPsqqWavtb+P7dPIUbPPmsanDtOR/zmzxcb2dz35dXP2v42JcSVvtyQi/O6ef/jilf4sfB40zermEJJ0zME2P/2KYpgTgQhMAwL8YUQeAEBOMyuahWj090HEXllcF9PjN6WiLVEdby9+6i5q48JJf6qqhEGeLVGF5tR782WBdPyFLkpSZ3EHDM5NaEyrqSTTBKHZUhFWXjck0OgwAgA9I1AEA8FHfLnFGhyCp5Uvx0xJsyimsaORYroNdM76neqTE1hltXXDnGYo2QbXycBDh5aQFegYIACB08e4LACGisMw1mltR3fb142idDlHmqHb+xykD1K+JiwZXnOwaNe1ZU5jOouYz+yirVRcNy1BkrcQ8Njqyzs/wr7xi7xdPmhL0Ao0AAEPw7gsAIcI9+BaMaelNrZOG8a45pacWzpzY6O1pCTGSfKtO3z3ZVUyuX5o5ZgsAAACmvgNA0JRXeW995avuyR2071ipMmqqdAeSNUhtzhAYuUWukVr3dOv6PdRrG9Y9Uav/9xx1iY8JSmxom/xSY+skAACCgyETAAiS2OjWTZse0i1BknT2gC767Hena1yvFH+GhTDkrvLubscVE9Xw7X5kjyRJrun8JOnm4V6u0NrbAQDhgUQdAILEnTxZW9imraraNefdarFoUEaCp/hXILX2ooLRWrPmNxxV2RufvXHGSZ0lSTdMyNJX/3OmkmKbnx6P4OkYzWRHAABT3wEgaDrUJL/OFvZTdo+KBtNFwzJ017vft+q+r007WVV2Y6pZv71mvyGPazYx9Yre1V528eQvh2v/sVJFRljVK0D94NF6lU1cZAEAtB8k6gAQZD1SYrXzSInP+7d0BN4fOrRgRD3SatHw7knae6xEecWVGpSRYNhU6spqkhzJ1RNdkiK8/O6kxtmCUpAQgdHC63wAgBDF1HcAMLmKKnO3Y0uNs+m9307Q6J7JRocChL3ICAo9AkB7QKIOAGiT/LJKo0PwuGhYV6NDAAKqSzyzIQCgPSBRBwA069IRGbpufE+vt2V1cq1zPndQuiRji2GdOyjNsMc2QnpC3SUG8TVT3qMiGr69XzA0XWf0Sw1KXGjcSW3sVx8d2fiylNpLHdxV/QEAoYlEHQBMrrTSNfW9qUregZaWEKOHLhnS5D6/GN1d2x6eoo42yp8ES3JHV8V290yC8wa7LpZ4mx79zJWj9Nq0scELDl6lJ3Zo8X1+d3Zfn/arfTFt0sATF62mjuqmK07ObPHjAgCME9BE/dixY7r66quVkJCgpKQk3XjjjSouLm7yPuXl5br11lvVqVMnxcXFaerUqcrJyamzj8ViafD11ltvBfKpAIBh3FWg4wyo/u7maxX3+tXGw43dYc5KXt2TXb21m7qYY7VaDClMiLqOl5xYKuKt+KG3Eff4mKg2PeZffjlCj04d1qZjAACCK6CJ+tVXX60tW7Zo4cKF+uSTT7Rs2TLddNNNTd7nzjvv1Mcff6x33nlHS5cu1aFDh/Tzn/+8wX6vvvqqDh8+7Pm69NJLA/QsAMC/DuaX+bTfgPR4SSeqPHcwMAkOQut20xvaLVG/GN3d6DC84vyYn7dOCu7ZMrWd2b9LMMIBAJhcwIZntm7dqvnz52vNmjUaM2aMJOnvf/+7LrjgAj3xxBPKyMhocJ+CggK9/PLLmjdvns4++2xJroR84MCB+vbbb3XKKad49k1KSlJ6enqgwgeAgBmZmayVu442u5+1Jvs6WlwR6JDgg49vO83oEBDC3HUDmpvU0Nw1l9KKav8EBAAwtYCNqK9cuVJJSUmeJF2SJk2aJKvVqlWrVnm9z7p161RVVaVJkyZ5tg0YMEA9evTQypUr6+x76623KjU1VWPHjtUrr7wiZxONRSsqKlRYWFjnCwCMEutjj/KONtd+kV4KgwHSiWnUeUWuizlmnZoP73p37tji+6zde7xF+//q1F4tfgwAgPECNqKenZ2tLl3qTt+KjIxUSkqKsrOzG71PdHS0kpKS6mxPS0urc5+HHnpIZ599tmJjY/XFF1/ot7/9rYqLi/W73/3O63HnzJmjBx98sG1PCADaKKGF60zT6lX0BurrldpR2YXlnos5KTXF5Wq7dESGTundKdihwQepcYFvtTa6Z3LAHwMA4H8tTtTvvvtuPfbYY03us3Xr1lYH5Iv77rvP8/3IkSNVUlKixx9/vNFEfdasWZo5c6bn58LCQmVmUv0UQHD98fwB6pESq5wiprIjMLytVX/qipHBDwQAALRJixP13//+97rhhhua3Kd3795KT09Xbm5une3V1dU6duxYo2vL09PTVVlZqfz8/Dqj6jk5OU2uRx83bpwefvhhVVRUyGZreHXaZrN53Q4AwdSnc5zuvWiQ7njrO6ND8Ul8TKTG9eqkRVtzmtyPdmzGSejg/f/evRwsl4tCAACEpBZ/uurcubM6d+7c7H7jx49Xfn6+1q1bp9GjR0uSvvzySzkcDo0bN87rfUaPHq2oqCgtXrxYU6dOlSRt375d+/bt0/jx4xt9rA0bNig5OZlkHEBIiAqRNeff3XeusgvLtWhrjsb2SvG6zxUnZ+rKsT2CHBnc/jhlgHp26ihHvbXp7g4B0SHyu9YeuItDjuvdSRsPFBgcDQDA7AL2Dj5w4EBNmTJF06dP1+rVq7V8+XLNmDFDV1xxhafi+8GDBzVgwACtXr1akpSYmKgbb7xRM2fO1FdffaV169Zp2rRpGj9+vKfi+8cff6yXXnpJmzdv1o4dO/Tss8/qkUce0W233RaopwIAhuhUs964NQWn/CEywqruybHa+tAUTR7smtWUmdKhzj6PTh2m4ZlJBkQXHIO6JhgdQpN6d47TPRcM9Ex5d9dVLatq2PYLxoqOtGrBHWdo5rknNbmfv9atuy8M0LoPAEJTQOcrvvnmm5oxY4bOOeccWa1WTZ06VX/72988t1dVVWn79u0qLS31bPvrX//q2beiokKTJ0/WP/7xD8/tUVFRmjt3ru688045nU717dtXTz75pKZPnx7IpwIAftezU6z2Hi1t9PaBXRP02e9O17e7juq7ffnBC6ye2v2fX77+ZO3OK9Fv3lhnWDzBlJ4Yox8Oh06nkORYV8FCC9mZKfVPj6/zs9XLeeoc759E/ZIRGdqdV6wzTmp+FiQAwHwCmqinpKRo3rx5jd6elZXVoK1aTEyM5s6dq7lz53q9z5QpUzRlyhS/xgkAZjUoI0FbaxLFpNiWVY0PhJPS4nVSWrwevnSIOvrYZg6Ad02tTEiNsymv2FVjILFDy//2UzpG6/9+Mby1oQEADEYFIAAwuUtHdlNWakeN6pFkdCge157S0+gQgirUWly5x2l7dIo1NA60nHv8YlyvFH266bDrByZIAEC7Q6IOACYTb4tUUUW15+cIqyXkEsVw88xVodXiLCYqQn+5bDjTnkNQrM01U6WtqxcoJAgAoY1XcQAwmZEk5fCDqaO7+229M4LHXwm21cowPACEMhJ1AAAAAABMhEQdAEymstrVWiu/rMrgSELXlWMzjQ7BEBV2hyTJ2cx+MIcqO2cKAOAdiToAmIy/+ii3R8U1a/srqhwGRxJcmSmuonHe2n0BAIDQQzE5ADAZkq3Wq64ZoUzuGG1wJMF1+cmZ6pXaUSU1Fyq6JcUYHBF80bdLnNEhAABMikQdABB2otpZxWtbZIRO79dZ5VV23TW5vy4almF0SAAAoA1I1AEACBMxURG69ay+RocBgxSWVTe/EwAgJLSvIQcAAFrAPZW8vJ2teUdocjopTgcA4YIRdQAAJE08qbOW/nhEkvTidWN0uKBMG/blSyIBgv+1pF+6+4JRo8eKtKqymotJABBOSNQBIMgcNTlfNa2ZTOvcQWmSpA37NhgbCMLSGzeOVUJMlDYdLJAkTR6cpgVbchrdn9cKAGh/mPoOAEHmrOlyXdzMKBmA8HR6v84anpnk+XnSwDTjggEAmBIj6gAQZJ2aaR02tleKjpVU6qJhXYMUEQAAAMyERB0ATKajLUL/+vU4o8MAAACAQZj6DgAmU1ZJUSgAAID2jEQdABA2EjtESZJSOkYZHAkAAEDrkagDAMLGmf0767lrRmny4HSjQwH8ZkxWstEhAACCjEQdABA2IiOsmjKkqyJb0KO6KRaLxS/HAdqiT5c4o0MAAAQZxeQAAKjROd6mod0SPT//6rQsHS4oU0ZSBwOjAhqKt0WqiBaPABC2GFEHAION7uma1kovZeMtvPMM/ePqUZ6fB2ckat70UxQTFWFgVEBDFw0/0b7RPe/Dxu8pAIQNEnUACLKs1I6SpPgY16Qmd1/12Gg+ZBstKTaapByGirO5XhdO6d3J5/sMz0ySJF1zSo9AhAQAMACJOgAE2S/HZOq9W8brpLR4o0MBYDLJHaO17t5Junqc70m3LdL1ca5zvC1QYQEAgoxEHQCCLCrCqtE9U1Rlp186gIY6xdkoZAgA7RyJOgAYxMoHcQAAAHhB1XcAMIh7bTqA9qm00lW13enHY959/gD1o50bAIQ8EnUAMImYKNckpw7RTHbyl7G9UrR69zGjwwC8qrK7UvSoCItmXzRIx0sq23zMmyf2afMxAADGI1EHAIMU1/RALq92rVWfNDBNvVLjNGVw16buhhZIjo3yab+OtghdfnJmgKMBvIuzRen/jexudBgAABMhUQcAg3RL7iBJcjpdo2oxURG65UxGw4yw+YHJFO9CyHLUvIY4/TmHHgBgKOZXAgDaPZJ0hLLqmin01Q4ydQAIFyTqAAAAIYz+6QAQfkjUAQAAAAAwERJ1AAAAE0qNo4UjALRXFJMDAINV1FR9d/dUBgBJevG6MdqeXWR0GAAAA5CoA4DBEju4WohV2ikEBbQnEVZXEcPGShmO7JGskT2SVVheFbygAACmQKIOAAazRbIKKZhskVbPLAZJ+sVo+lfDGFNHddfO3GKN79PJ6FAAACZDog4ACDtlNcsISirsDW6LiqibqE8enB60uIDaOsfb9Phlw40OAwBgQgzjAADCjruddLekDsYGAgAA0Aok6gCAdq3K7mh+JwAAgCAiUQcAtGvFFVTbBwAA5kKiDgAGOVZSKUlyUuzdUI1V3AYAADAKiToAGCTC4koREzpQ1xMAAAAnkKgDgMHcvZQBAAAAiUQdAAAAAABTIVEHAAAIYcmx0ZKotwAA4YSFkQCAdumktDj9mFNsdBhAm804u6+6xNvUNTHG6FAAAH7CiDoAoF26YUIvo0MA/CItIUa3ndNPFgtj6gAQLkjUAcBgo3oky2qRBqbHGx0KAAAATIBEHQAMNiIzST/9+QL1SyNRB9C40/ulyhbp+uh2at9Ug6MBAARSwBL1Y8eO6eqrr1ZCQoKSkpJ04403qri46bWAL7zwgs4880wlJCTIYrEoPz/fL8cFALOjRVvwuRMeW1SEwZEAvkuNs0mSLhqWYXAkAIBACliifvXVV2vLli1auHChPvnkEy1btkw33XRTk/cpLS3VlClTdM899/j1uAAA1Hf+0HTdOekknTswzehQAAAA6ghI1fetW7dq/vz5WrNmjcaMGSNJ+vvf/64LLrhATzzxhDIyvF8FvuOOOyRJS5Ys8etxAQCoLzY6UrdP6md0GAAAAA0EZER95cqVSkpK8iTTkjRp0iRZrVatWrUq6MetqKhQYWFhnS8AAIBQF2ej0y4AhKOAJOrZ2dnq0qVLnW2RkZFKSUlRdnZ20I87Z84cJSYmer4yMzNbHQMA+EtSbLQkyUpLpaAqrqg2OgQAAIAmtShRv/vuu2WxWJr82rZtW6BibbVZs2apoKDA87V//36jQwIA3XJmH/1xygD17BRrdCjtSrekDkaHAAAA0KQWzZf6/e9/rxtuuKHJfXr37q309HTl5ubW2V5dXa1jx44pPT29xUG6tfa4NptNNput1Y8LAIHQOd6mW87sY3QYAAAAMJkWJeqdO3dW586dm91v/Pjxys/P17p16zR69GhJ0pdffimHw6Fx48a1LtIAHhcAAAAAALMIyBr1gQMHasqUKZo+fbpWr16t5cuXa8aMGbriiis8ldkPHjyoAQMGaPXq1Z77ZWdna8OGDdqxY4ckadOmTdqwYYOOHTvm83EBAADaCypcAEB4Clgf9TfffFMDBgzQOeecowsuuECnnXaaXnjhBc/tVVVV2r59u0pLSz3bnnvuOY0cOVLTp0+XJJ1xxhkaOXKkPvroI5+PCwAA0B50T+6gmeedZHQYAIAACFhPj5SUFM2bN6/R27OysuR0Outse+CBB/TAAw+06bgAAKR0dFXUP/2kVL29lgKiCE9f/+EsWegaAQBhKWAj6gAAGCU1zqZtD0/RRcNYFoXwRZIOAOGLRB0AEJZioiI838fZAjaBDAAAwO9I1AEAYS0pNko3T+xtdBgAAAA+I1EHAIS1Nf87STPO7md0GAAAAD4jUQcAhLWoCN7qAABAaOHTCwAAAAAAJkKiDgBoV3p37mh0CECL2CJdH9d+NpwuBgDQXlAGFwDQrjzy/4Zq5c6jRocB+MwWGaGf/ny+oiKsemrRT0aHAwAIAhJ1AEC7kpkSq8yUWKPDAFqEWgsA0L7wqg8AAAAAgImQqAMAAAAAYCJMfQcAtAudOkarrMpudBgAAADNIlEHALQL790yQdUOp9FhAG0SGWExOgQAQBAw9R0A0C5kpXZU3y5xRocBtMkfpwzQ8MxEo8MAAAQYI+oAAAAh4oKhXXXB0K5GhwEACDBG1AEAAAAAMBESdQAAAAAATIREHQAAAAAAEyFRBwAAAADAREjUAQAAAAAwERJ1AAAAAABMhEQdAAAAAAATIVEHAAAAAMBESNQBAAAAADAREnUAAAAAAEyERB0AAAAAABOJNDoAIzidTklSYWGhwZEAAAAAANoDd/7pzkeb0i4T9aKiIklSZmamwZEAAAAAANqToqIiJSYmNrmPxelLOh9mHA6HDh06pPj4eFksFqPDaVRhYaEyMzO1f/9+JSQkGB0OWoBzF5o4b6GJ8xaaOG+hifMWmjhvoYtzF5oaO29Op1NFRUXKyMiQ1dr0KvR2OaJutVrVvXt3o8PwWUJCAn+YIYpzF5o4b6GJ8xaaOG+hifMWmjhvoYtzF5q8nbfmRtLdKCYHAAAAAICJkKgDAAAAAGAiJOomZrPZdP/998tmsxkdClqIcxeaOG+hifMWmjhvoYnzFpo4b6GLcxea/HHe2mUxOQAAAAAAzIoRdQAAAAAATIREHQAAAAAAEyFRBwAAAADAREjUAQAAAAAwERJ1E5s7d66ysrIUExOjcePGafXq1UaHhBotOTevvfaaLBZLna+YmJggRovmLFu2TBdffLEyMjJksVj0wQcfGB0SarT03CxZsqTB35vFYlF2dnZwAkaz5syZo5NPPlnx8fHq0qWLLr30Um3fvt3osKDWnRve48zt2Wef1bBhw5SQkKCEhASNHz9en3/+udFhoUZLzw9/b6Hl0UcflcVi0R133NGq+5Oom9Tbb7+tmTNn6v7779f69es1fPhwTZ48Wbm5uUaH1u615twkJCTo8OHDnq+9e/cGMWI0p6SkRMOHD9fcuXONDgX1tPbcbN++vc7fXJcuXQIUIVpq6dKluvXWW/Xtt99q4cKFqqqq0nnnnaeSkhKjQ2v3WntueI8zr+7du+vRRx/VunXrtHbtWp199tm65JJLtGXLFqNDg1p3fvh7Cw1r1qzR888/r2HDhrX+IE6Y0tixY5233nqr52e73e7MyMhwzpkzx8Co4HS2/Ny8+uqrzsTExCBFh7aS5Hz//feNDgNe+HJuvvrqK6ck5/Hjx4MSE9ouNzfXKcm5dOlSo0NBPb6cG97jQk9ycrLzpZdeMjoMNKKp88PfW2goKipy9uvXz7lw4ULnxIkTnbfffnurjsOIuglVVlZq3bp1mjRpkmeb1WrVpEmTtHLlSgMjQ2vPTXFxsXr27KnMzEyuZANBMGLECHXt2lXnnnuuli9fbnQ4aEJBQYEkKSUlxeBIUJ+v54b3uNBgt9v11ltvqaSkROPHjzc6HNTj6/nh7838br31Vl144YV18oXWIFE3oby8PNntdqWlpdXZnpaWxjpLg7Xm3PTv31+vvPKKPvzwQ/3rX/+Sw+HQhAkTdODAgWCEDLQrXbt21XPPPaf33ntP7733njIzM3XmmWdq/fr1RocGLxwOh+644w6deuqpGjJkiNHhoBZfzw3vcea3adMmxcXFyWaz6eabb9b777+vQYMGGR0WarTk/PD3Zn5vvfWW1q9frzlz5rT5WJF+iAdAE8aPH1/nyuiECRM0cOBAPf/883r44YcNjAwIP/3791f//v09P0+YMEE7d+7UX//6V73xxhsGRgZvbr31Vm3evFnffPON0aGgHl/PDe9x5te/f39t2LBBBQUFevfdd3X99ddr6dKlJOsm0ZLzw9+bue3fv1+33367Fi5c6JcifyTqJpSamqqIiAjl5OTU2Z6Tk6P09HSDooLkn3MTFRWlkSNHaseOHYEIEUA9Y8eOJRE0oRkzZuiTTz7RsmXL1L17d6PDQS1tOTe8x5lPdHS0+vbtK0kaPXq01qxZo6efflrPP/+8wZFBatv54e/NXNatW6fc3FyNGjXKs81ut2vZsmV65plnVFFRoYiICJ+Px9R3E4qOjtbo0aO1ePFizzaHw6HFixezpshg/jg3drtdmzZtUteuXQMVJoBaNmzYwN+biTidTs2YMUPvv/++vvzyS/Xq1cvokFDDH+eG9zjzczgcqqioMDoMNKIl54e/N3M555xztGnTJm3YsMHzNWbMGF199dXasGFDi5J0iRF105o5c6auv/56jRkzRmPHjtVTTz2lkpISTZs2zejQ2r3mzs11112nbt26edamPPTQQzrllFPUt29f5efn6/HHH9fevXv161//2singVqKi4vrXI3evXu3NmzYoJSUFPXo0cPAyNDcuZk1a5YOHjyof/7zn5Kkp556Sr169dLgwYNVXl6ul156SV9++aW++OILo54C6rn11ls1b948ffjhh4qPj/fU90hMTFSHDh0Mjq598+Xc8B4XWmbNmqXzzz9fPXr0UFFRkebNm6clS5ZowYIFRocGNX9++HsLLfHx8Q1qenTs2FGdOnVqVR0WEnWTuvzyy3XkyBHNnj1b2dnZGjFihObPn9+giBmCr7lzs2/fPlmtJyarHD9+XNOnT1d2draSk5M1evRorVixgrVhJrJ27VqdddZZnp9nzpwpSbr++uv12muvGRQVpObPzeHDh7Vv3z7P7ZWVlfr973+vgwcPKjY2VsOGDdOiRYvqHAPGevbZZyVJZ555Zp3tr776qm644YbgBwQPX84N73GhJTc3V9ddd50OHz6sxMREDRs2TAsWLNC5555rdGhQ8+eHv7f2zeJ0Op1GBwEAAAAAAFxYow4AAAAAgImQqAMAAAAAYCIk6gAAAAAAmAiJOgAAAAAAJkKiDgAAAACAiZCoAwAAAABgIiTqAAAAAACYCIk6AACo44YbbtCll15qdBgAALRbkUYHAAAAgsdisTR5+/3336+nn35aTqczSBEBAID6SNQBAGhHDh8+7Pn+7bff1uzZs7V9+3bPtri4OMXFxRkRGgAAqMHUdwAA2pH09HTPV2JioiwWS51tcXFxDaa+n3nmmbrtttt0xx13KDk5WWlpaXrxxRdVUlKiadOmKT4+Xn379tXnn39e57E2b96s888/X3FxcUpLS9O1116rvLy8ID9jAABCD4k6AABo1uuvv67U1FStXr1at912m2655RZddtllmjBhgtavX6/zzjtP1157rUpLSyVJ+fn5OvvsszVy5EitXbtW8+fPV05Ojn75y18a/EwAADA/EnUAANCs4cOH695771W/fv00a9YsxcTEKDU1VdOnT1e/fv00e/ZsHT16VN9//70k6ZlnntHIkSP1yCOPaMCAARo5cqReeeUVffXVV/rxxx8NfjYAAJgba9QBAECzhg0b5vk+IiJCnTp10tChQz3b0tLSJEm5ubmSpI0bN+qrr77yut59586dOumkkwIcMQAAoYtEHQAANCsqKqrOzxaLpc42dzV5h8MhSSouLtbFF1+sxx57rMGxunbtGsBIAQAIfSTqAADA70aNGqX33ntPWVlZiozk4wYAAC3BGnUAAOB3t956q44dO6Yrr7xSa9as0c6dO7VgwQJNmzZNdrvd6PAAADA1EnUAAOB3GRkZWr58uex2u8477zwNHTpUd9xxh5KSkmS18vEDAICmWJxOp9PoIAAAAAAAgAuXtAEAAAAAMBESdQAAAAAATIREHQAAAAAAEyFRBwAAAADAREjUAQAAAAAwERJ1AAAAAABMhEQdAAAAAAATIVEHAAAAAMBESNQBAAAAADAREnUAAAAAAEyERB0AAAAAABMhUQcAAAAAwET+Px7sw0n4ViWmAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveshow(data, sr=sampling_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T03:54:21.297957Z",
     "start_time": "2024-02-28T03:54:20.869412100Z"
    }
   },
   "id": "f6204615669fe54e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T03:54:29.439852100Z",
     "start_time": "2024-02-28T03:54:24.304931900Z"
    }
   },
   "id": "89b108800c424b3b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T03:54:31.674478300Z",
     "start_time": "2024-02-28T03:54:29.440971100Z"
    }
   },
   "id": "c29847851fb2722d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-3.0.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting absl-py (from keras)\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from keras) (1.26.2)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.10.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting dm-tree (from keras)\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-win_amd64.whl (101 kB)\n",
      "     ---------------------------------------- 0.0/101.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 101.3/101.3 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from rich->keras) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading keras-3.0.1-py3-none-any.whl (999 kB)\n",
      "   ---------------------------------------- 0.0/999.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 999.1/999.1 kB 31.9 MB/s eta 0:00:00\n",
      "Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Downloading h5py-3.10.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.7/2.7 MB 51.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 42.6 MB/s eta 0:00:00\n",
      "Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 240.6/240.6 kB 14.4 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
      "Installing collected packages: namex, dm-tree, mdurl, h5py, absl-py, markdown-it-py, rich, keras\n",
      "Successfully installed absl-py-2.0.0 dm-tree-0.1.8 h5py-3.10.0 keras-3.0.1 markdown-it-py-3.0.0 mdurl-0.1.2 namex-0.0.7 rich-13.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T14:53:26.765980900Z",
     "start_time": "2023-12-15T14:53:16.870258Z"
    }
   },
   "id": "1e9271a42b4a0158"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ------------\n",
      "absl-py                      2.0.0\n",
      "anyio                        4.1.0\n",
      "argon2-cffi                  23.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.3.0\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.4\n",
      "attrs                        23.1.0\n",
      "audioread                    3.0.1\n",
      "Babel                        2.14.0\n",
      "beautifulsoup4               4.12.2\n",
      "bleach                       6.1.0\n",
      "cachetools                   5.3.2\n",
      "certifi                      2023.11.17\n",
      "cffi                         1.16.0\n",
      "charset-normalizer           3.3.2\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.0\n",
      "contourpy                    1.2.0\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.8.0\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "dm-tree                      0.1.8\n",
      "exceptiongroup               1.2.0\n",
      "executing                    2.0.1\n",
      "fastjsonschema               2.19.0\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.46.0\n",
      "fqdn                         1.5.1\n",
      "gast                         0.5.4\n",
      "google-auth                  2.25.2\n",
      "google-auth-oauthlib         1.2.0\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.60.0\n",
      "h5py                         3.10.0\n",
      "idna                         3.6\n",
      "ipykernel                    6.27.1\n",
      "ipython                      8.18.1\n",
      "ipywidgets                   8.1.1\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.1\n",
      "Jinja2                       3.1.2\n",
      "joblib                       1.3.2\n",
      "json5                        0.9.14\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.20.0\n",
      "jsonschema-specifications    2023.11.2\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               8.6.0\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.5.0\n",
      "jupyter-events               0.9.0\n",
      "jupyter-lsp                  2.2.1\n",
      "jupyter_server               2.12.1\n",
      "jupyter_server_terminals     0.5.0\n",
      "jupyterlab                   4.0.9\n",
      "jupyterlab_pygments          0.3.0\n",
      "jupyterlab_server            2.25.2\n",
      "jupyterlab-widgets           3.0.9\n",
      "keras                        2.15.0\n",
      "kiwisolver                   1.4.5\n",
      "lazy_loader                  0.3\n",
      "libclang                     16.0.6\n",
      "librosa                      0.10.1\n",
      "llvmlite                     0.41.1\n",
      "Markdown                     3.5.1\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.7.0\n",
      "matplotlib-inline            0.1.6\n",
      "mdurl                        0.1.2\n",
      "mistune                      3.0.2\n",
      "ml-dtypes                    0.2.0\n",
      "msgpack                      1.0.7\n",
      "namex                        0.0.7\n",
      "nbclient                     0.9.0\n",
      "nbconvert                    7.12.0\n",
      "nbformat                     5.9.2\n",
      "nest-asyncio                 1.5.8\n",
      "notebook                     7.0.6\n",
      "notebook_shim                0.2.3\n",
      "np-utils                     0.6.0\n",
      "numba                        0.58.1\n",
      "numpy                        1.26.2\n",
      "oauthlib                     3.2.2\n",
      "opt-einsum                   3.3.0\n",
      "overrides                    7.4.0\n",
      "packaging                    23.2\n",
      "pandas                       2.1.4\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "Pillow                       10.1.0\n",
      "pip                          23.3.1\n",
      "platformdirs                 4.1.0\n",
      "pooch                        1.8.0\n",
      "prometheus-client            0.19.0\n",
      "prompt-toolkit               3.0.42\n",
      "protobuf                     4.23.4\n",
      "psutil                       5.9.6\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.1\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "Pygments                     2.17.2\n",
      "pyparsing                    3.1.1\n",
      "python-dateutil              2.8.2\n",
      "python-json-logger           2.0.7\n",
      "pytz                         2023.3.post1\n",
      "pywin32                      306\n",
      "pywinpty                     2.0.12\n",
      "PyYAML                       6.0.1\n",
      "pyzmq                        25.1.2\n",
      "qtconsole                    5.5.1\n",
      "QtPy                         2.4.1\n",
      "referencing                  0.32.0\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "resampy                      0.4.2\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rich                         13.7.0\n",
      "rpds-py                      0.13.2\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.2\n",
      "scipy                        1.11.4\n",
      "Send2Trash                   1.8.2\n",
      "setuptools                   68.2.0\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "soundfile                    0.12.1\n",
      "soupsieve                    2.5\n",
      "soxr                         0.3.7\n",
      "stack-data                   0.6.3\n",
      "tensorboard                  2.15.1\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorflow                   2.15.0\n",
      "tensorflow-estimator         2.15.0\n",
      "tensorflow-intel             2.15.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.4.0\n",
      "terminado                    0.18.0\n",
      "threadpoolctl                3.2.0\n",
      "tinycss2                     1.2.1\n",
      "tomli                        2.0.1\n",
      "tornado                      6.4\n",
      "traitlets                    5.14.0\n",
      "types-python-dateutil        2.8.19.14\n",
      "typing_extensions            4.9.0\n",
      "tzdata                       2023.3\n",
      "uri-template                 1.3.0\n",
      "urllib3                      2.1.0\n",
      "wcwidth                      0.2.12\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.7.0\n",
      "Werkzeug                     3.0.1\n",
      "wheel                        0.41.2\n",
      "widgetsnbextension           4.0.9\n",
      "wrapt                        1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T16:34:49.421125600Z",
     "start_time": "2023-12-15T16:34:47.857607400Z"
    }
   },
   "id": "fb44735b06ae8c84"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp310-cp310-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-intel==2.15.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.15.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.10.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.2)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached protobuf-4.25.1-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.5/1.5 MB 31.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.4 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading google_auth-2.25.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading protobuf-4.23.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 181.3/181.3 kB ? eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 151.7/151.7 kB ? eta 0:00:00\n",
      "Downloading tensorflow-2.15.0-cp310-cp310-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.15.0-cp310-cp310-win_amd64.whl (300.9 MB)\n",
      "   ---------------------------------------- 0.0/300.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/300.9 MB 41.4 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 3.7/300.9 MB 39.3 MB/s eta 0:00:08\n",
      "    --------------------------------------- 5.2/300.9 MB 37.4 MB/s eta 0:00:08\n",
      "    --------------------------------------- 5.2/300.9 MB 37.4 MB/s eta 0:00:08\n",
      "    --------------------------------------- 5.2/300.9 MB 37.4 MB/s eta 0:00:08\n",
      "    --------------------------------------- 5.2/300.9 MB 37.4 MB/s eta 0:00:08\n",
      "    --------------------------------------- 5.2/300.9 MB 37.4 MB/s eta 0:00:08\n",
      "    --------------------------------------- 5.2/300.9 MB 37.4 MB/s eta 0:00:08\n",
      "    --------------------------------------- 6.1/300.9 MB 14.5 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 8.1/300.9 MB 17.3 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 10.1/300.9 MB 19.6 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 11.5/300.9 MB 19.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 12.3/300.9 MB 10.6 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 12.6/300.9 MB 10.4 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 12.6/300.9 MB 10.4 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 12.6/300.9 MB 10.4 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 12.6/300.9 MB 10.4 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 12.6/300.9 MB 10.4 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 12.6/300.9 MB 10.4 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 13.6/300.9 MB 8.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 13.6/300.9 MB 8.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 13.6/300.9 MB 8.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 13.6/300.9 MB 8.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 13.6/300.9 MB 8.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 13.6/300.9 MB 8.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 14.7/300.9 MB 6.8 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 14.7/300.9 MB 6.8 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 14.7/300.9 MB 6.8 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 14.7/300.9 MB 6.8 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 14.7/300.9 MB 6.8 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 15.0/300.9 MB 5.8 MB/s eta 0:00:49\n",
      "   -- ------------------------------------- 16.8/300.9 MB 6.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 18.5/300.9 MB 6.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 19.9/300.9 MB 6.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 19.9/300.9 MB 6.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 19.9/300.9 MB 6.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 19.9/300.9 MB 6.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 19.9/300.9 MB 6.8 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 20.9/300.9 MB 5.9 MB/s eta 0:00:48\n",
      "   -- ------------------------------------- 21.0/300.9 MB 5.9 MB/s eta 0:00:48\n",
      "   -- ------------------------------------- 21.0/300.9 MB 5.9 MB/s eta 0:00:48\n",
      "   -- ------------------------------------- 21.0/300.9 MB 5.9 MB/s eta 0:00:48\n",
      "   -- ------------------------------------- 21.0/300.9 MB 5.9 MB/s eta 0:00:48\n",
      "   -- ------------------------------------- 21.0/300.9 MB 5.2 MB/s eta 0:00:54\n",
      "   -- ------------------------------------- 22.0/300.9 MB 6.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 22.0/300.9 MB 6.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 22.0/300.9 MB 6.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 22.0/300.9 MB 6.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 22.0/300.9 MB 6.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 22.0/300.9 MB 6.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 22.0/300.9 MB 6.8 MB/s eta 0:00:41\n",
      "   --- ------------------------------------ 23.1/300.9 MB 6.7 MB/s eta 0:00:42\n",
      "   --- ------------------------------------ 25.0/300.9 MB 9.9 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 27.0/300.9 MB 9.8 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 28.3/300.9 MB 9.9 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 28.3/300.9 MB 9.9 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 28.3/300.9 MB 9.9 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 28.3/300.9 MB 9.9 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 28.3/300.9 MB 9.9 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 28.3/300.9 MB 9.9 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 29.3/300.9 MB 7.9 MB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 31.4/300.9 MB 12.1 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 32.5/300.9 MB 19.2 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 34.5/300.9 MB 19.3 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 34.6/300.9 MB 19.3 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 34.6/300.9 MB 19.3 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 34.6/300.9 MB 19.3 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 34.6/300.9 MB 19.3 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 34.6/300.9 MB 19.3 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 35.4/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.7/300.9 MB 13.1 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 36.9/300.9 MB 8.1 MB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 38.7/300.9 MB 10.2 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 40.6/300.9 MB 10.2 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 42.7/300.9 MB 10.2 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 43.0/300.9 MB 10.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 43.0/300.9 MB 10.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 43.0/300.9 MB 10.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 43.0/300.9 MB 10.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 43.0/300.9 MB 10.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 44.9/300.9 MB 10.9 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 47.1/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 49.0/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 51.0/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 52.4/300.9 MB 22.6 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 52.4/300.9 MB 22.6 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 52.4/300.9 MB 22.6 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 52.4/300.9 MB 22.6 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 52.4/300.9 MB 22.6 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 53.2/300.9 MB 14.9 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 55.4/300.9 MB 22.6 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 57.4/300.9 MB 22.6 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 58.7/300.9 MB 21.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 60.6/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 60.8/300.9 MB 6.1 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 60.8/300.9 MB 6.1 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 60.8/300.9 MB 6.1 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 60.8/300.9 MB 6.1 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 61.2/300.9 MB 5.3 MB/s eta 0:00:46\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 62.9/300.9 MB 6.0 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 63.8/300.9 MB 4.1 MB/s eta 0:00:58\n",
      "   -------- ------------------------------- 65.8/300.9 MB 4.1 MB/s eta 0:00:58\n",
      "   --------- ------------------------------ 68.1/300.9 MB 4.1 MB/s eta 0:00:57\n",
      "   --------- ------------------------------ 70.2/300.9 MB 8.2 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 72.6/300.9 MB 10.1 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 74.9/300.9 MB 46.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 76.1/300.9 MB 46.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 77.3/300.9 MB 40.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 79.8/300.9 MB 40.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 82.0/300.9 MB 40.9 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 83.6/300.9 MB 38.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 85.3/300.9 MB 36.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 86.9/300.9 MB 38.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 88.8/300.9 MB 38.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 91.1/300.9 MB 38.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 92.3/300.9 MB 40.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 93.6/300.9 MB 34.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 95.8/300.9 MB 36.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 97.9/300.9 MB 38.5 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 100.3/300.9 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 102.3/300.9 MB 40.9 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 103.8/300.9 MB 46.7 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 103.8/300.9 MB 46.7 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 103.8/300.9 MB 46.7 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 103.8/300.9 MB 46.7 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 103.8/300.9 MB 46.7 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 104.3/300.9 MB 21.8 MB/s eta 0:00:10\n",
      "   ------------- ------------------------- 106.3/300.9 MB 22.6 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 108.6/300.9 MB 21.8 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 110.3/300.9 MB 22.6 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 112.8/300.9 MB 21.9 MB/s eta 0:00:09\n",
      "   -------------- ------------------------ 114.4/300.9 MB 40.9 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 116.4/300.9 MB 43.5 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 116.4/300.9 MB 43.5 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 116.4/300.9 MB 43.5 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 116.4/300.9 MB 43.5 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 116.4/300.9 MB 43.5 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 116.4/300.9 MB 43.5 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 118.7/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   --------------- ----------------------- 120.4/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   --------------- ----------------------- 122.2/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 124.3/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 124.8/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 124.8/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 124.8/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 124.8/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 124.8/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 124.8/300.9 MB 21.1 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 125.3/300.9 MB 13.1 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ---------------------- 125.8/300.9 MB 12.8 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 127.4/300.9 MB 6.7 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 129.8/300.9 MB 6.7 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 131.8/300.9 MB 6.8 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 134.3/300.9 MB 6.8 MB/s eta 0:00:25\n",
      "   ----------------- --------------------- 136.6/300.9 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 138.9/300.9 MB 46.7 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 139.5/300.9 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 139.5/300.9 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 139.5/300.9 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 139.5/300.9 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 139.5/300.9 MB 50.4 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 141.7/300.9 MB 23.4 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 144.4/300.9 MB 24.2 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 146.0/300.9 MB 23.4 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 146.3/300.9 MB 23.4 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 146.9/300.9 MB 19.8 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 147.8/300.9 MB 19.3 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 148.7/300.9 MB 17.7 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 148.9/300.9 MB 17.7 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 148.9/300.9 MB 17.7 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 148.9/300.9 MB 17.7 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 148.9/300.9 MB 17.7 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 149.0/300.9 MB 12.6 MB/s eta 0:00:13\n",
      "   ------------------- ------------------- 151.1/300.9 MB 17.7 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 152.9/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 155.2/300.9 MB 17.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 155.7/300.9 MB 9.2 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 157.4/300.9 MB 10.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 158.3/300.9 MB 10.2 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 159.0/300.9 MB 4.3 MB/s eta 0:00:34\n",
      "   --------------------- ------------------ 160.9/300.9 MB 4.7 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 162.4/300.9 MB 4.7 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 162.5/300.9 MB 4.7 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 162.5/300.9 MB 4.7 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 162.5/300.9 MB 4.7 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 162.5/300.9 MB 4.7 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 162.5/300.9 MB 4.7 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 164.2/300.9 MB 4.2 MB/s eta 0:00:33\n",
      "   --------------------- ------------------ 165.2/300.9 MB 4.2 MB/s eta 0:00:33\n",
      "   ---------------------- ----------------- 166.6/300.9 MB 5.2 MB/s eta 0:00:26\n",
      "   --------------------- ----------------- 168.6/300.9 MB 19.3 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 170.8/300.9 MB 19.9 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 172.8/300.9 MB 38.6 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 174.9/300.9 MB 36.4 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 176.2/300.9 MB 43.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 177.2/300.9 MB 8.8 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 177.2/300.9 MB 8.8 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 177.2/300.9 MB 8.8 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 177.2/300.9 MB 8.8 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 177.2/300.9 MB 8.8 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 177.2/300.9 MB 8.8 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 178.3/300.9 MB 7.3 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 180.4/300.9 MB 7.3 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 182.4/300.9 MB 7.3 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 184.5/300.9 MB 7.2 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 186.3/300.9 MB 7.2 MB/s eta 0:00:16\n",
      "   ------------------------ -------------- 187.9/300.9 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 189.8/300.9 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 190.8/300.9 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 190.8/300.9 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 190.8/300.9 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 190.8/300.9 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 190.8/300.9 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 190.8/300.9 MB 36.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 191.9/300.9 MB 19.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 191.9/300.9 MB 19.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 191.9/300.9 MB 19.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 191.9/300.9 MB 19.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 191.9/300.9 MB 19.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 191.9/300.9 MB 19.8 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 192.7/300.9 MB 13.1 MB/s eta 0:00:09\n",
      "   ------------------------- ------------- 194.6/300.9 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------------- ------------- 196.5/300.9 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------------- ------------- 198.1/300.9 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------------- ------------- 199.7/300.9 MB 12.6 MB/s eta 0:00:09\n",
      "   -------------------------- ------------ 201.5/300.9 MB 18.7 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 203.1/300.9 MB 36.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 204.8/300.9 MB 36.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 206.4/300.9 MB 34.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 208.1/300.9 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 209.8/300.9 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 210.8/300.9 MB 36.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 211.2/300.9 MB 7.2 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 212.9/300.9 MB 7.2 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 212.9/300.9 MB 7.2 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 212.9/300.9 MB 7.2 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 212.9/300.9 MB 7.2 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 212.9/300.9 MB 7.2 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 213.9/300.9 MB 6.3 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 216.0/300.9 MB 6.4 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 216.0/300.9 MB 6.4 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 216.0/300.9 MB 6.4 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 216.0/300.9 MB 6.4 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 216.0/300.9 MB 6.4 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 216.0/300.9 MB 6.4 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 216.9/300.9 MB 5.5 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 218.6/300.9 MB 5.5 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 220.5/300.9 MB 5.5 MB/s eta 0:00:15\n",
      "   ---------------------------- ---------- 222.1/300.9 MB 13.9 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 223.8/300.9 MB 19.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 225.8/300.9 MB 19.3 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 227.7/300.9 MB 38.5 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 229.5/300.9 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 231.8/300.9 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 233.4/300.9 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 234.9/300.9 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 234.9/300.9 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 234.9/300.9 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 234.9/300.9 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 234.9/300.9 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 234.9/300.9 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 236.6/300.9 MB 20.5 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 238.1/300.9 MB 19.9 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 239.1/300.9 MB 19.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 239.1/300.9 MB 19.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 239.1/300.9 MB 19.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 239.1/300.9 MB 19.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 239.1/300.9 MB 19.8 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 240.1/300.9 MB 14.2 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 240.1/300.9 MB 14.2 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 240.1/300.9 MB 14.2 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 240.1/300.9 MB 14.2 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 240.1/300.9 MB 14.2 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 240.1/300.9 MB 14.2 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 240.9/300.9 MB 10.2 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 242.8/300.9 MB 10.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 245.2/300.9 MB 13.6 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 247.1/300.9 MB 13.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 248.8/300.9 MB 13.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 249.8/300.9 MB 18.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 251.2/300.9 MB 36.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 252.7/300.9 MB 36.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 253.6/300.9 MB 5.3 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 254.8/300.9 MB 5.4 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 255.8/300.9 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 257.5/300.9 MB 2.5 MB/s eta 0:00:18\n",
      "   ---------------------------------- ----- 259.4/300.9 MB 2.5 MB/s eta 0:00:17\n",
      "   ---------------------------------- ----- 261.4/300.9 MB 2.5 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 262.1/300.9 MB 2.5 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 262.1/300.9 MB 2.5 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 262.1/300.9 MB 2.5 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 262.1/300.9 MB 2.5 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 262.1/300.9 MB 2.5 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 262.1/300.9 MB 2.5 MB/s eta 0:00:16\n",
      "   ----------------------------------- ---- 264.0/300.9 MB 3.8 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 265.9/300.9 MB 6.0 MB/s eta 0:00:06\n",
      "   ---------------------------------- ---- 267.8/300.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 268.4/300.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 268.4/300.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 268.4/300.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 268.4/300.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 268.4/300.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 269.5/300.9 MB 13.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 269.5/300.9 MB 13.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 269.5/300.9 MB 13.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 269.5/300.9 MB 13.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 269.5/300.9 MB 13.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 269.5/300.9 MB 13.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 270.5/300.9 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 271.6/300.9 MB 6.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 271.6/300.9 MB 6.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 271.6/300.9 MB 6.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 271.6/300.9 MB 6.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 271.6/300.9 MB 6.7 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 273.3/300.9 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 275.4/300.9 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 277.5/300.9 MB 7.0 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 279.6/300.9 MB 8.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 280.2/300.9 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 281.1/300.9 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 281.6/300.9 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 282.6/300.9 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 284.6/300.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 286.3/300.9 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 287.9/300.9 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 290.1/300.9 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 290.9/300.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 291.9/300.9 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  294.2/300.9 MB 36.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  296.4/300.9 MB 36.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  298.3/300.9 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.2/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  300.9/300.9 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 300.9/300.9 MB 14.5 MB/s eta 0:00:00\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.60.0-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 43.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.7/3.7 MB 39.3 MB/s eta 0:00:00\n",
      "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 36.2 MB/s eta 0:00:00\n",
      "Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/24.4 MB 51.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.5/24.4 MB 44.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.3/24.4 MB 42.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.2/24.4 MB 42.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.4/24.4 MB 42.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.3/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.5/24.4 MB 43.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.4/24.4 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.5/24.4 MB 8.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.4/24.4 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.5/24.4 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.2.0-cp310-cp310-win_amd64.whl (938 kB)\n",
      "   ---------------------------------------- 0.0/938.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 938.6/938.6 kB 29.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 2.4/5.5 MB 51.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.2/5.5 MB 45.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 39.5 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.23.4-cp310-abi3-win_amd64.whl (422 kB)\n",
      "   ---------------------------------------- 0.0/422.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 422.5/422.5 kB ? eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "   ---------------------------------------- 0.0/442.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 442.0/442.0 kB 27.0 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading google_auth-2.25.2-py2.py3-none-any.whl (184 kB)\n",
      "   ---------------------------------------- 0.0/184.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 184.2/184.2 kB 10.9 MB/s eta 0:00:00\n",
      "Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "   ---------------------------------------- 0.0/102.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 102.2/102.2 kB ? eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "   ---------------------------------------- 0.0/226.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 226.7/226.7 kB ? eta 0:00:00\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.9/84.9 kB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.0.1\n",
      "    Uninstalling keras-3.0.1:\n",
      "      Successfully uninstalled keras-3.0.1\n",
      "Successfully installed astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.25.2 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-intel-2.15.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T15:01:01.036933100Z",
     "start_time": "2023-12-15T14:59:39.406298500Z"
    }
   },
   "id": "b63d3230885e25f6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ------------\n",
      "absl-py                      2.0.0\n",
      "anyio                        4.1.0\n",
      "argon2-cffi                  23.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.3.0\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.4\n",
      "attrs                        23.1.0\n",
      "audioread                    3.0.1\n",
      "Babel                        2.14.0\n",
      "beautifulsoup4               4.12.2\n",
      "bleach                       6.1.0\n",
      "cachetools                   5.3.2\n",
      "certifi                      2023.11.17\n",
      "cffi                         1.16.0\n",
      "charset-normalizer           3.3.2\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.0\n",
      "contourpy                    1.2.0\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.8.0\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "dm-tree                      0.1.8\n",
      "exceptiongroup               1.2.0\n",
      "executing                    2.0.1\n",
      "fastjsonschema               2.19.0\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.46.0\n",
      "fqdn                         1.5.1\n",
      "gast                         0.5.4\n",
      "google-auth                  2.25.2\n",
      "google-auth-oauthlib         1.2.0\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.60.0\n",
      "h5py                         3.10.0\n",
      "idna                         3.6\n",
      "ipykernel                    6.27.1\n",
      "ipython                      8.18.1\n",
      "ipywidgets                   8.1.1\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.1\n",
      "Jinja2                       3.1.2\n",
      "joblib                       1.3.2\n",
      "json5                        0.9.14\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.20.0\n",
      "jsonschema-specifications    2023.11.2\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               8.6.0\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.5.0\n",
      "jupyter-events               0.9.0\n",
      "jupyter-lsp                  2.2.1\n",
      "jupyter_server               2.12.1\n",
      "jupyter_server_terminals     0.5.0\n",
      "jupyterlab                   4.0.9\n",
      "jupyterlab_pygments          0.3.0\n",
      "jupyterlab_server            2.25.2\n",
      "jupyterlab-widgets           3.0.9\n",
      "keras                        2.15.0\n",
      "kiwisolver                   1.4.5\n",
      "lazy_loader                  0.3\n",
      "libclang                     16.0.6\n",
      "librosa                      0.10.1\n",
      "llvmlite                     0.41.1\n",
      "Markdown                     3.5.1\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.7.0\n",
      "matplotlib-inline            0.1.6\n",
      "mdurl                        0.1.2\n",
      "mistune                      3.0.2\n",
      "ml-dtypes                    0.2.0\n",
      "msgpack                      1.0.7\n",
      "namex                        0.0.7\n",
      "nbclient                     0.9.0\n",
      "nbconvert                    7.12.0\n",
      "nbformat                     5.9.2\n",
      "nest-asyncio                 1.5.8\n",
      "notebook                     7.0.6\n",
      "notebook_shim                0.2.3\n",
      "numba                        0.58.1\n",
      "numpy                        1.26.2\n",
      "oauthlib                     3.2.2\n",
      "opt-einsum                   3.3.0\n",
      "overrides                    7.4.0\n",
      "packaging                    23.2\n",
      "pandas                       2.1.4\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "Pillow                       10.1.0\n",
      "pip                          23.3.1\n",
      "platformdirs                 4.1.0\n",
      "pooch                        1.8.0\n",
      "prometheus-client            0.19.0\n",
      "prompt-toolkit               3.0.42\n",
      "protobuf                     4.23.4\n",
      "psutil                       5.9.6\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.1\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "Pygments                     2.17.2\n",
      "pyparsing                    3.1.1\n",
      "python-dateutil              2.8.2\n",
      "python-json-logger           2.0.7\n",
      "pytz                         2023.3.post1\n",
      "pywin32                      306\n",
      "pywinpty                     2.0.12\n",
      "PyYAML                       6.0.1\n",
      "pyzmq                        25.1.2\n",
      "qtconsole                    5.5.1\n",
      "QtPy                         2.4.1\n",
      "referencing                  0.32.0\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "resampy                      0.4.2\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rich                         13.7.0\n",
      "rpds-py                      0.13.2\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.2\n",
      "scipy                        1.11.4\n",
      "Send2Trash                   1.8.2\n",
      "setuptools                   68.2.0\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "soundfile                    0.12.1\n",
      "soupsieve                    2.5\n",
      "soxr                         0.3.7\n",
      "stack-data                   0.6.3\n",
      "tensorboard                  2.15.1\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorflow                   2.15.0\n",
      "tensorflow-estimator         2.15.0\n",
      "tensorflow-intel             2.15.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.4.0\n",
      "terminado                    0.18.0\n",
      "threadpoolctl                3.2.0\n",
      "tinycss2                     1.2.1\n",
      "tomli                        2.0.1\n",
      "tornado                      6.4\n",
      "traitlets                    5.14.0\n",
      "types-python-dateutil        2.8.19.14\n",
      "typing_extensions            4.9.0\n",
      "tzdata                       2023.3\n",
      "uri-template                 1.3.0\n",
      "urllib3                      2.1.0\n",
      "wcwidth                      0.2.12\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.7.0\n",
      "Werkzeug                     3.0.1\n",
      "wheel                        0.41.2\n",
      "widgetsnbextension           4.0.9\n",
      "wrapt                        1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T15:01:21.864281100Z",
     "start_time": "2023-12-15T15:01:18.104498200Z"
    }
   },
   "id": "4bf3bf58832c7f3c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting np_utils\n",
      "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
      "     ---------------------------------------- 0.0/62.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.0/62.0 kB 3.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.0 in c:\\users\\mirza\\pycharmprojects\\fyp(music recommender ser)\\.venv\\lib\\site-packages (from np_utils) (1.26.2)\n",
      "Building wheels for collected packages: np_utils\n",
      "  Building wheel for np_utils (setup.py): started\n",
      "  Building wheel for np_utils (setup.py): finished with status 'done'\n",
      "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56449 sha256=f3e511fc8c578b6b785db4bd321d8430b359afe09fb4f38826cdf079ea15ceec\n",
      "  Stored in directory: c:\\users\\mirza\\appdata\\local\\pip\\cache\\wheels\\b6\\c7\\50\\2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
      "Successfully built np_utils\n",
      "Installing collected packages: np_utils\n",
      "Successfully installed np_utils-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install np_utils"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T15:02:55.421627900Z",
     "start_time": "2023-12-15T15:02:51.165787600Z"
    }
   },
   "id": "a96bd6bcb519ddca"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ------------\n",
      "absl-py                      2.0.0\n",
      "anyio                        4.1.0\n",
      "argon2-cffi                  23.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.3.0\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.4\n",
      "attrs                        23.1.0\n",
      "audioread                    3.0.1\n",
      "Babel                        2.14.0\n",
      "beautifulsoup4               4.12.2\n",
      "bleach                       6.1.0\n",
      "cachetools                   5.3.2\n",
      "certifi                      2023.11.17\n",
      "cffi                         1.16.0\n",
      "charset-normalizer           3.3.2\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.0\n",
      "contourpy                    1.2.0\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.8.0\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "dm-tree                      0.1.8\n",
      "exceptiongroup               1.2.0\n",
      "executing                    2.0.1\n",
      "fastjsonschema               2.19.0\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.46.0\n",
      "fqdn                         1.5.1\n",
      "gast                         0.5.4\n",
      "google-auth                  2.25.2\n",
      "google-auth-oauthlib         1.2.0\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.60.0\n",
      "h5py                         3.10.0\n",
      "idna                         3.6\n",
      "ipykernel                    6.27.1\n",
      "ipython                      8.18.1\n",
      "ipywidgets                   8.1.1\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.1\n",
      "Jinja2                       3.1.2\n",
      "joblib                       1.3.2\n",
      "json5                        0.9.14\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.20.0\n",
      "jsonschema-specifications    2023.11.2\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               8.6.0\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.5.0\n",
      "jupyter-events               0.9.0\n",
      "jupyter-lsp                  2.2.1\n",
      "jupyter_server               2.12.1\n",
      "jupyter_server_terminals     0.5.0\n",
      "jupyterlab                   4.0.9\n",
      "jupyterlab_pygments          0.3.0\n",
      "jupyterlab_server            2.25.2\n",
      "jupyterlab-widgets           3.0.9\n",
      "keras                        2.15.0\n",
      "kiwisolver                   1.4.5\n",
      "lazy_loader                  0.3\n",
      "libclang                     16.0.6\n",
      "librosa                      0.10.1\n",
      "llvmlite                     0.41.1\n",
      "Markdown                     3.5.1\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.7.0\n",
      "matplotlib-inline            0.1.6\n",
      "mdurl                        0.1.2\n",
      "mistune                      3.0.2\n",
      "ml-dtypes                    0.2.0\n",
      "msgpack                      1.0.7\n",
      "namex                        0.0.7\n",
      "nbclient                     0.9.0\n",
      "nbconvert                    7.12.0\n",
      "nbformat                     5.9.2\n",
      "nest-asyncio                 1.5.8\n",
      "notebook                     7.0.6\n",
      "notebook_shim                0.2.3\n",
      "np-utils                     0.6.0\n",
      "numba                        0.58.1\n",
      "numpy                        1.26.2\n",
      "oauthlib                     3.2.2\n",
      "opt-einsum                   3.3.0\n",
      "overrides                    7.4.0\n",
      "packaging                    23.2\n",
      "pandas                       2.1.4\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "Pillow                       10.1.0\n",
      "pip                          23.3.1\n",
      "platformdirs                 4.1.0\n",
      "pooch                        1.8.0\n",
      "prometheus-client            0.19.0\n",
      "prompt-toolkit               3.0.42\n",
      "protobuf                     4.23.4\n",
      "psutil                       5.9.6\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.1\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "Pygments                     2.17.2\n",
      "pyparsing                    3.1.1\n",
      "python-dateutil              2.8.2\n",
      "python-json-logger           2.0.7\n",
      "pytz                         2023.3.post1\n",
      "pywin32                      306\n",
      "pywinpty                     2.0.12\n",
      "PyYAML                       6.0.1\n",
      "pyzmq                        25.1.2\n",
      "qtconsole                    5.5.1\n",
      "QtPy                         2.4.1\n",
      "referencing                  0.32.0\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "resampy                      0.4.2\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rich                         13.7.0\n",
      "rpds-py                      0.13.2\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.2\n",
      "scipy                        1.11.4\n",
      "Send2Trash                   1.8.2\n",
      "setuptools                   68.2.0\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "soundfile                    0.12.1\n",
      "soupsieve                    2.5\n",
      "soxr                         0.3.7\n",
      "stack-data                   0.6.3\n",
      "tensorboard                  2.15.1\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorflow                   2.15.0\n",
      "tensorflow-estimator         2.15.0\n",
      "tensorflow-intel             2.15.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.4.0\n",
      "terminado                    0.18.0\n",
      "threadpoolctl                3.2.0\n",
      "tinycss2                     1.2.1\n",
      "tomli                        2.0.1\n",
      "tornado                      6.4\n",
      "traitlets                    5.14.0\n",
      "types-python-dateutil        2.8.19.14\n",
      "typing_extensions            4.9.0\n",
      "tzdata                       2023.3\n",
      "uri-template                 1.3.0\n",
      "urllib3                      2.1.0\n",
      "wcwidth                      0.2.12\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.7.0\n",
      "Werkzeug                     3.0.1\n",
      "wheel                        0.41.2\n",
      "widgetsnbextension           4.0.9\n",
      "wrapt                        1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T15:03:11.521654700Z",
     "start_time": "2023-12-15T15:03:10.266179Z"
    }
   },
   "id": "feec9188419abd27"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import soundfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, LSTM, SimpleRNN\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T18:00:20.427676900Z",
     "start_time": "2024-03-20T18:00:02.450510500Z"
    }
   },
   "id": "34fad98bc05188ae"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def extract_features(file_name, **kwargs):\n",
    "    \n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        if X.ndim >= 2:\n",
    "            X = np.mean(X, 1)\n",
    "        sample_rate = sound_file.samplerate\n",
    "        result = np.array([])\n",
    "        if chroma or contrast:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr = sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectogram(X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T18:00:20.448008600Z",
     "start_time": "2024-03-20T18:00:20.425634300Z"
    }
   },
   "id": "29cd5e178ce2618c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "int2emotion = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# Only Need These Emotions\n",
    "UseEmotions = {\n",
    "    \"neutral\",\n",
    "    \"calm\",\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"angry\",\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T18:00:38.087509700Z",
     "start_time": "2024-03-20T18:00:38.066729100Z"
    }
   },
   "id": "b32d3ac6d169f92f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2):\n",
    "    X, y = [], []\n",
    "    for file in glob.glob(\"Ravdess/*.wav\"):\n",
    "        # get the base name of the audio file\n",
    "        basename = os.path.basename(file)\n",
    "        # get the emotion label\n",
    "        emotion = int2emotion[basename.split(\"-\")[2]]\n",
    "        # we allow only AVAILABLE_EMOTIONS we set\n",
    "        if emotion not in UseEmotions:\n",
    "            continue\n",
    "        # extract speech features\n",
    "        features = extract_features(file, mfcc=True, chroma=False, mel=False)\n",
    "        # add to data\n",
    "        X.append(features)\n",
    "        y.append(emotion)\n",
    "    # split the data to training and testing and return it\n",
    "    return train_test_split(np.array(X), y, test_size=test_size, random_state=8)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T18:00:40.302708900Z",
     "start_time": "2024-03-20T18:00:40.262949300Z"
    }
   },
   "id": "8228ee4a4a7d6457"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train1, y_test1 = load_data(test_size=0.30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T18:01:21.155925900Z",
     "start_time": "2024-03-20T18:00:48.607892Z"
    }
   },
   "id": "68200aa3faa0f9fe"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training samples: 1184\n",
      "Testing samples: 508\n",
      "Features: 40\n",
      "(1184, 40, 1)\n",
      "(508, 40, 1)\n",
      "1\n",
      "(1184, 5)\n",
      "(508, 5)\n",
      "(1184, 1, 40)\n",
      "(508, 1, 40)\n"
     ]
    }
   ],
   "source": [
    "# print some details\n",
    "# number of samples in training data\n",
    "print(\"[Training samples:\", X_train.shape[0])\n",
    "# number of samples in testing data\n",
    "print(\"Testing samples:\", X_test.shape[0])\n",
    "# number of features used\n",
    "# this is a vector of features extracted \n",
    "# using extract_features() function\n",
    "print(\"Features:\", X_train.shape[1])\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train=np.array(y_train1)\n",
    "y_test=np.array(y_test1)\n",
    "\n",
    "y_train = to_categorical(lb.fit_transform(y_train))\n",
    "y_test = to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "x_traindnn = np.expand_dims(X_train, axis=1)\n",
    "x_testdnn = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "\n",
    "print(x_traincnn.shape) \n",
    "print(x_testcnn.shape)\n",
    "print(x_traincnn.shape[2])\n",
    "print(y_train.shape) \n",
    "print(y_test.shape)\n",
    "print(x_traindnn.shape)\n",
    "print(x_testdnn.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T18:06:01.052436100Z",
     "start_time": "2024-03-20T18:06:01.015151200Z"
    }
   },
   "id": "d6297357d14f0a40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "CNN MODEL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59fe378d90f0eb13"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "74/74 [==============================] - 2s 7ms/step - loss: 6.0022 - accuracy: 0.2061 - val_loss: 1.6584 - val_accuracy: 0.2461\n",
      "Epoch 2/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 2.1525 - accuracy: 0.2525 - val_loss: 1.5406 - val_accuracy: 0.3012\n",
      "Epoch 3/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.7867 - accuracy: 0.2779 - val_loss: 1.4888 - val_accuracy: 0.3406\n",
      "Epoch 4/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 1.6325 - accuracy: 0.3218 - val_loss: 1.4244 - val_accuracy: 0.4114\n",
      "Epoch 5/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.5276 - accuracy: 0.3438 - val_loss: 1.4130 - val_accuracy: 0.3858\n",
      "Epoch 6/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 1.4366 - accuracy: 0.3995 - val_loss: 1.2837 - val_accuracy: 0.4744\n",
      "Epoch 7/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.3821 - accuracy: 0.4316 - val_loss: 1.2452 - val_accuracy: 0.5059\n",
      "Epoch 8/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.3163 - accuracy: 0.4519 - val_loss: 1.1720 - val_accuracy: 0.5295\n",
      "Epoch 9/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.2554 - accuracy: 0.4738 - val_loss: 1.1474 - val_accuracy: 0.5335\n",
      "Epoch 10/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.2319 - accuracy: 0.4941 - val_loss: 1.1128 - val_accuracy: 0.5256\n",
      "Epoch 11/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.1918 - accuracy: 0.5076 - val_loss: 1.0638 - val_accuracy: 0.5669\n",
      "Epoch 12/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.1579 - accuracy: 0.5228 - val_loss: 1.0639 - val_accuracy: 0.5748\n",
      "Epoch 13/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 1.1031 - accuracy: 0.5439 - val_loss: 1.0215 - val_accuracy: 0.5827\n",
      "Epoch 14/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 1.0857 - accuracy: 0.5465 - val_loss: 1.0111 - val_accuracy: 0.5709\n",
      "Epoch 15/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 1.0576 - accuracy: 0.5650 - val_loss: 0.9565 - val_accuracy: 0.5965\n",
      "Epoch 16/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 1.0523 - accuracy: 0.5591 - val_loss: 0.9586 - val_accuracy: 0.6024\n",
      "Epoch 17/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 1.0249 - accuracy: 0.5912 - val_loss: 0.9216 - val_accuracy: 0.6240\n",
      "Epoch 18/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 1.0154 - accuracy: 0.5861 - val_loss: 0.9057 - val_accuracy: 0.6516\n",
      "Epoch 19/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.9781 - accuracy: 0.6106 - val_loss: 0.8823 - val_accuracy: 0.6555\n",
      "Epoch 20/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.9400 - accuracy: 0.6174 - val_loss: 0.8814 - val_accuracy: 0.6614\n",
      "Epoch 21/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.9325 - accuracy: 0.6132 - val_loss: 0.8522 - val_accuracy: 0.6732\n",
      "Epoch 22/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.9110 - accuracy: 0.6419 - val_loss: 0.8591 - val_accuracy: 0.6476\n",
      "Epoch 23/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.8865 - accuracy: 0.6537 - val_loss: 0.8514 - val_accuracy: 0.6614\n",
      "Epoch 24/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.9160 - accuracy: 0.6394 - val_loss: 0.8164 - val_accuracy: 0.6831\n",
      "Epoch 25/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.8732 - accuracy: 0.6402 - val_loss: 0.7979 - val_accuracy: 0.7067\n",
      "Epoch 26/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.8271 - accuracy: 0.6613 - val_loss: 0.8208 - val_accuracy: 0.6831\n",
      "Epoch 27/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.8016 - accuracy: 0.6816 - val_loss: 0.7586 - val_accuracy: 0.6791\n",
      "Epoch 28/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.7842 - accuracy: 0.7027 - val_loss: 0.7467 - val_accuracy: 0.7067\n",
      "Epoch 29/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.7531 - accuracy: 0.7078 - val_loss: 0.7337 - val_accuracy: 0.7067\n",
      "Epoch 30/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.7963 - accuracy: 0.6782 - val_loss: 0.7695 - val_accuracy: 0.6850\n",
      "Epoch 31/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.7785 - accuracy: 0.6926 - val_loss: 0.7361 - val_accuracy: 0.6988\n",
      "Epoch 32/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.7611 - accuracy: 0.6951 - val_loss: 0.7947 - val_accuracy: 0.6909\n",
      "Epoch 33/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.7441 - accuracy: 0.7035 - val_loss: 0.7324 - val_accuracy: 0.7126\n",
      "Epoch 34/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.6753 - accuracy: 0.7441 - val_loss: 0.7356 - val_accuracy: 0.6988\n",
      "Epoch 35/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.7278 - accuracy: 0.7137 - val_loss: 0.6829 - val_accuracy: 0.7264\n",
      "Epoch 36/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.6799 - accuracy: 0.7297 - val_loss: 0.6929 - val_accuracy: 0.7264\n",
      "Epoch 37/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.6585 - accuracy: 0.7424 - val_loss: 0.6977 - val_accuracy: 0.7461\n",
      "Epoch 38/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.6955 - accuracy: 0.7238 - val_loss: 0.6822 - val_accuracy: 0.7303\n",
      "Epoch 39/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.6626 - accuracy: 0.7432 - val_loss: 0.6777 - val_accuracy: 0.7402\n",
      "Epoch 40/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.6443 - accuracy: 0.7534 - val_loss: 0.6645 - val_accuracy: 0.7520\n",
      "Epoch 41/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.6467 - accuracy: 0.7593 - val_loss: 0.6742 - val_accuracy: 0.7303\n",
      "Epoch 42/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.6171 - accuracy: 0.7686 - val_loss: 0.6402 - val_accuracy: 0.7421\n",
      "Epoch 43/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.6276 - accuracy: 0.7669 - val_loss: 0.7109 - val_accuracy: 0.7028\n",
      "Epoch 44/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.6161 - accuracy: 0.7559 - val_loss: 0.6233 - val_accuracy: 0.7638\n",
      "Epoch 45/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.5951 - accuracy: 0.7652 - val_loss: 0.7124 - val_accuracy: 0.7028\n",
      "Epoch 46/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.5832 - accuracy: 0.7829 - val_loss: 0.7017 - val_accuracy: 0.7126\n",
      "Epoch 47/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.5932 - accuracy: 0.7779 - val_loss: 0.6367 - val_accuracy: 0.7480\n",
      "Epoch 48/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.5617 - accuracy: 0.8032 - val_loss: 0.6408 - val_accuracy: 0.7500\n",
      "Epoch 49/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.5477 - accuracy: 0.7990 - val_loss: 0.5995 - val_accuracy: 0.7697\n",
      "Epoch 50/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.5155 - accuracy: 0.8108 - val_loss: 0.6069 - val_accuracy: 0.7598\n",
      "Epoch 51/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.5492 - accuracy: 0.7956 - val_loss: 0.6094 - val_accuracy: 0.7736\n",
      "Epoch 52/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.5265 - accuracy: 0.8049 - val_loss: 0.6203 - val_accuracy: 0.7539\n",
      "Epoch 53/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.5218 - accuracy: 0.8007 - val_loss: 0.5896 - val_accuracy: 0.7913\n",
      "Epoch 54/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.5188 - accuracy: 0.8142 - val_loss: 0.5983 - val_accuracy: 0.7500\n",
      "Epoch 55/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4855 - accuracy: 0.8235 - val_loss: 0.6030 - val_accuracy: 0.7539\n",
      "Epoch 56/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.5074 - accuracy: 0.8091 - val_loss: 0.6024 - val_accuracy: 0.7579\n",
      "Epoch 57/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.4900 - accuracy: 0.8218 - val_loss: 0.6521 - val_accuracy: 0.7382\n",
      "Epoch 58/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4771 - accuracy: 0.8184 - val_loss: 0.5928 - val_accuracy: 0.7736\n",
      "Epoch 59/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4778 - accuracy: 0.8209 - val_loss: 0.5974 - val_accuracy: 0.7618\n",
      "Epoch 60/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4546 - accuracy: 0.8311 - val_loss: 0.6079 - val_accuracy: 0.7461\n",
      "Epoch 61/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.4469 - accuracy: 0.8345 - val_loss: 0.5713 - val_accuracy: 0.7677\n",
      "Epoch 62/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.4698 - accuracy: 0.8243 - val_loss: 0.6132 - val_accuracy: 0.7539\n",
      "Epoch 63/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.4444 - accuracy: 0.8454 - val_loss: 0.5872 - val_accuracy: 0.7717\n",
      "Epoch 64/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4235 - accuracy: 0.8311 - val_loss: 0.5764 - val_accuracy: 0.7638\n",
      "Epoch 65/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4002 - accuracy: 0.8640 - val_loss: 0.5712 - val_accuracy: 0.7756\n",
      "Epoch 66/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4458 - accuracy: 0.8353 - val_loss: 0.5684 - val_accuracy: 0.7756\n",
      "Epoch 67/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.4061 - accuracy: 0.8539 - val_loss: 0.5993 - val_accuracy: 0.7736\n",
      "Epoch 68/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3991 - accuracy: 0.8497 - val_loss: 0.5737 - val_accuracy: 0.7657\n",
      "Epoch 69/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4048 - accuracy: 0.8590 - val_loss: 0.5875 - val_accuracy: 0.7618\n",
      "Epoch 70/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.3704 - accuracy: 0.8691 - val_loss: 0.6059 - val_accuracy: 0.7598\n",
      "Epoch 71/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.4185 - accuracy: 0.8446 - val_loss: 0.5636 - val_accuracy: 0.7618\n",
      "Epoch 72/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.4216 - accuracy: 0.8404 - val_loss: 0.5647 - val_accuracy: 0.7776\n",
      "Epoch 73/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.3640 - accuracy: 0.8666 - val_loss: 0.5903 - val_accuracy: 0.7736\n",
      "Epoch 74/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3607 - accuracy: 0.8691 - val_loss: 0.5512 - val_accuracy: 0.7835\n",
      "Epoch 75/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3499 - accuracy: 0.8725 - val_loss: 0.5816 - val_accuracy: 0.7736\n",
      "Epoch 76/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.3502 - accuracy: 0.8775 - val_loss: 0.6234 - val_accuracy: 0.7343\n",
      "Epoch 77/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3733 - accuracy: 0.8632 - val_loss: 0.5594 - val_accuracy: 0.7992\n",
      "Epoch 78/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3533 - accuracy: 0.8666 - val_loss: 0.5884 - val_accuracy: 0.7756\n",
      "Epoch 79/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3654 - accuracy: 0.8632 - val_loss: 0.6197 - val_accuracy: 0.7559\n",
      "Epoch 80/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3459 - accuracy: 0.8767 - val_loss: 0.5821 - val_accuracy: 0.7854\n",
      "Epoch 81/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.3249 - accuracy: 0.8877 - val_loss: 0.5425 - val_accuracy: 0.7913\n",
      "Epoch 82/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3473 - accuracy: 0.8767 - val_loss: 0.5819 - val_accuracy: 0.7756\n",
      "Epoch 83/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.3560 - accuracy: 0.8750 - val_loss: 0.5808 - val_accuracy: 0.7717\n",
      "Epoch 84/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.3094 - accuracy: 0.8953 - val_loss: 0.5635 - val_accuracy: 0.7835\n",
      "Epoch 85/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3001 - accuracy: 0.8877 - val_loss: 0.5800 - val_accuracy: 0.7815\n",
      "Epoch 86/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3297 - accuracy: 0.8834 - val_loss: 0.5441 - val_accuracy: 0.7992\n",
      "Epoch 87/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2885 - accuracy: 0.8944 - val_loss: 0.5491 - val_accuracy: 0.7874\n",
      "Epoch 88/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2975 - accuracy: 0.8910 - val_loss: 0.5503 - val_accuracy: 0.7894\n",
      "Epoch 89/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2891 - accuracy: 0.8860 - val_loss: 0.5597 - val_accuracy: 0.7835\n",
      "Epoch 90/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3107 - accuracy: 0.8818 - val_loss: 0.6313 - val_accuracy: 0.7598\n",
      "Epoch 91/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2940 - accuracy: 0.8961 - val_loss: 0.5627 - val_accuracy: 0.7894\n",
      "Epoch 92/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3017 - accuracy: 0.8885 - val_loss: 0.5622 - val_accuracy: 0.7677\n",
      "Epoch 93/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2732 - accuracy: 0.8961 - val_loss: 0.5933 - val_accuracy: 0.7874\n",
      "Epoch 94/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2993 - accuracy: 0.8919 - val_loss: 0.6372 - val_accuracy: 0.7638\n",
      "Epoch 95/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.3033 - accuracy: 0.8936 - val_loss: 0.5737 - val_accuracy: 0.7736\n",
      "Epoch 96/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2714 - accuracy: 0.9020 - val_loss: 0.5778 - val_accuracy: 0.7677\n",
      "Epoch 97/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2899 - accuracy: 0.9071 - val_loss: 0.5650 - val_accuracy: 0.7854\n",
      "Epoch 98/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2695 - accuracy: 0.9046 - val_loss: 0.6081 - val_accuracy: 0.7520\n",
      "Epoch 99/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.8986 - val_loss: 0.5893 - val_accuracy: 0.7933\n",
      "Epoch 100/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2489 - accuracy: 0.9046 - val_loss: 0.5585 - val_accuracy: 0.7835\n",
      "Epoch 101/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2664 - accuracy: 0.8970 - val_loss: 0.6490 - val_accuracy: 0.7638\n",
      "Epoch 102/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.2843 - accuracy: 0.8953 - val_loss: 0.5772 - val_accuracy: 0.7913\n",
      "Epoch 103/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.2680 - accuracy: 0.9088 - val_loss: 0.5627 - val_accuracy: 0.7953\n",
      "Epoch 104/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2535 - accuracy: 0.8936 - val_loss: 0.5940 - val_accuracy: 0.7736\n",
      "Epoch 105/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.2610 - accuracy: 0.9037 - val_loss: 0.5782 - val_accuracy: 0.7776\n",
      "Epoch 106/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.2397 - accuracy: 0.9155 - val_loss: 0.5582 - val_accuracy: 0.7933\n",
      "Epoch 107/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.2366 - accuracy: 0.9198 - val_loss: 0.5520 - val_accuracy: 0.7953\n",
      "Epoch 108/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2522 - accuracy: 0.9139 - val_loss: 0.5720 - val_accuracy: 0.7874\n",
      "Epoch 109/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2685 - accuracy: 0.9029 - val_loss: 0.5680 - val_accuracy: 0.7894\n",
      "Epoch 110/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2389 - accuracy: 0.9105 - val_loss: 0.5785 - val_accuracy: 0.7756\n",
      "Epoch 111/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2370 - accuracy: 0.9181 - val_loss: 0.5489 - val_accuracy: 0.7992\n",
      "Epoch 112/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2417 - accuracy: 0.9130 - val_loss: 0.5824 - val_accuracy: 0.7795\n",
      "Epoch 113/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2133 - accuracy: 0.9265 - val_loss: 0.6053 - val_accuracy: 0.7874\n",
      "Epoch 114/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2451 - accuracy: 0.9172 - val_loss: 0.5307 - val_accuracy: 0.7953\n",
      "Epoch 115/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2066 - accuracy: 0.9307 - val_loss: 0.5931 - val_accuracy: 0.7815\n",
      "Epoch 116/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2442 - accuracy: 0.9088 - val_loss: 0.5813 - val_accuracy: 0.7953\n",
      "Epoch 117/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.2195 - accuracy: 0.9189 - val_loss: 0.6100 - val_accuracy: 0.8110\n",
      "Epoch 118/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2036 - accuracy: 0.9274 - val_loss: 0.5877 - val_accuracy: 0.7874\n",
      "Epoch 119/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2213 - accuracy: 0.9198 - val_loss: 0.6176 - val_accuracy: 0.7835\n",
      "Epoch 120/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2064 - accuracy: 0.9240 - val_loss: 0.5856 - val_accuracy: 0.7972\n",
      "Epoch 121/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2373 - accuracy: 0.9181 - val_loss: 0.6630 - val_accuracy: 0.7795\n",
      "Epoch 122/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2328 - accuracy: 0.9206 - val_loss: 0.5882 - val_accuracy: 0.7795\n",
      "Epoch 123/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1710 - accuracy: 0.9434 - val_loss: 0.5948 - val_accuracy: 0.7835\n",
      "Epoch 124/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2220 - accuracy: 0.9181 - val_loss: 0.6018 - val_accuracy: 0.7933\n",
      "Epoch 125/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2134 - accuracy: 0.9189 - val_loss: 0.5704 - val_accuracy: 0.7953\n",
      "Epoch 126/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2197 - accuracy: 0.9206 - val_loss: 0.5573 - val_accuracy: 0.8169\n",
      "Epoch 127/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2172 - accuracy: 0.9248 - val_loss: 0.5901 - val_accuracy: 0.7835\n",
      "Epoch 128/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2124 - accuracy: 0.9172 - val_loss: 0.6252 - val_accuracy: 0.7795\n",
      "Epoch 129/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9350 - val_loss: 0.5590 - val_accuracy: 0.8091\n",
      "Epoch 130/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2082 - accuracy: 0.9350 - val_loss: 0.6396 - val_accuracy: 0.7835\n",
      "Epoch 131/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1954 - accuracy: 0.9307 - val_loss: 0.6618 - val_accuracy: 0.7618\n",
      "Epoch 132/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1750 - accuracy: 0.9426 - val_loss: 0.6029 - val_accuracy: 0.7894\n",
      "Epoch 133/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1963 - accuracy: 0.9316 - val_loss: 0.5691 - val_accuracy: 0.8012\n",
      "Epoch 134/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1760 - accuracy: 0.9307 - val_loss: 0.5865 - val_accuracy: 0.7933\n",
      "Epoch 135/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1775 - accuracy: 0.9502 - val_loss: 0.6106 - val_accuracy: 0.7874\n",
      "Epoch 136/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.2034 - accuracy: 0.9248 - val_loss: 0.6173 - val_accuracy: 0.7913\n",
      "Epoch 137/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2142 - accuracy: 0.9333 - val_loss: 0.5930 - val_accuracy: 0.7972\n",
      "Epoch 138/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1694 - accuracy: 0.9367 - val_loss: 0.6214 - val_accuracy: 0.7874\n",
      "Epoch 139/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9350 - val_loss: 0.5805 - val_accuracy: 0.7933\n",
      "Epoch 140/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1893 - accuracy: 0.9307 - val_loss: 0.5994 - val_accuracy: 0.8051\n",
      "Epoch 141/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1734 - accuracy: 0.9358 - val_loss: 0.5883 - val_accuracy: 0.7913\n",
      "Epoch 142/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1807 - accuracy: 0.9274 - val_loss: 0.6261 - val_accuracy: 0.7894\n",
      "Epoch 143/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.2295 - accuracy: 0.9189 - val_loss: 0.6368 - val_accuracy: 0.7835\n",
      "Epoch 144/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1893 - accuracy: 0.9375 - val_loss: 0.5806 - val_accuracy: 0.8031\n",
      "Epoch 145/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.1773 - accuracy: 0.9358 - val_loss: 0.6019 - val_accuracy: 0.7913\n",
      "Epoch 146/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1802 - accuracy: 0.9367 - val_loss: 0.6090 - val_accuracy: 0.8031\n",
      "Epoch 147/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1671 - accuracy: 0.9426 - val_loss: 0.5934 - val_accuracy: 0.8012\n",
      "Epoch 148/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1845 - accuracy: 0.9417 - val_loss: 0.6212 - val_accuracy: 0.7953\n",
      "Epoch 149/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1649 - accuracy: 0.9367 - val_loss: 0.6035 - val_accuracy: 0.7874\n",
      "Epoch 150/700\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 0.1837 - accuracy: 0.9367 - val_loss: 0.6020 - val_accuracy: 0.7953\n",
      "Epoch 151/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.1599 - accuracy: 0.9459 - val_loss: 0.5915 - val_accuracy: 0.7992\n",
      "Epoch 152/700\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 0.1647 - accuracy: 0.9409 - val_loss: 0.5936 - val_accuracy: 0.8130\n",
      "Epoch 153/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1586 - accuracy: 0.9392 - val_loss: 0.6035 - val_accuracy: 0.8130\n",
      "Epoch 154/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9561 - val_loss: 0.5560 - val_accuracy: 0.8031\n",
      "Epoch 155/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.1726 - accuracy: 0.9409 - val_loss: 0.5465 - val_accuracy: 0.8268\n",
      "Epoch 156/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1848 - accuracy: 0.9383 - val_loss: 0.6414 - val_accuracy: 0.7874\n",
      "Epoch 157/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1802 - accuracy: 0.9383 - val_loss: 0.5837 - val_accuracy: 0.8071\n",
      "Epoch 158/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1655 - accuracy: 0.9392 - val_loss: 0.5490 - val_accuracy: 0.8031\n",
      "Epoch 159/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1647 - accuracy: 0.9459 - val_loss: 0.6034 - val_accuracy: 0.8110\n",
      "Epoch 160/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.1745 - accuracy: 0.9485 - val_loss: 0.6277 - val_accuracy: 0.7776\n",
      "Epoch 161/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.1683 - accuracy: 0.9375 - val_loss: 0.5668 - val_accuracy: 0.8051\n",
      "Epoch 162/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1694 - accuracy: 0.9375 - val_loss: 0.6225 - val_accuracy: 0.7756\n",
      "Epoch 163/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9443 - val_loss: 0.5883 - val_accuracy: 0.8051\n",
      "Epoch 164/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1829 - accuracy: 0.9383 - val_loss: 0.6142 - val_accuracy: 0.7815\n",
      "Epoch 165/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1782 - accuracy: 0.9383 - val_loss: 0.6483 - val_accuracy: 0.7874\n",
      "Epoch 166/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1336 - accuracy: 0.9569 - val_loss: 0.6353 - val_accuracy: 0.7854\n",
      "Epoch 167/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9527 - val_loss: 0.6707 - val_accuracy: 0.7894\n",
      "Epoch 168/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9443 - val_loss: 0.6426 - val_accuracy: 0.7953\n",
      "Epoch 169/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1545 - accuracy: 0.9417 - val_loss: 0.6715 - val_accuracy: 0.7972\n",
      "Epoch 170/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9476 - val_loss: 0.6623 - val_accuracy: 0.8031\n",
      "Epoch 171/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1375 - accuracy: 0.9502 - val_loss: 0.6609 - val_accuracy: 0.7933\n",
      "Epoch 172/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1562 - accuracy: 0.9451 - val_loss: 0.6551 - val_accuracy: 0.8031\n",
      "Epoch 173/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9535 - val_loss: 0.6505 - val_accuracy: 0.8071\n",
      "Epoch 174/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1634 - accuracy: 0.9367 - val_loss: 0.6189 - val_accuracy: 0.7874\n",
      "Epoch 175/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1476 - accuracy: 0.9519 - val_loss: 0.6073 - val_accuracy: 0.7953\n",
      "Epoch 176/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1468 - accuracy: 0.9468 - val_loss: 0.6391 - val_accuracy: 0.7717\n",
      "Epoch 177/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9485 - val_loss: 0.6703 - val_accuracy: 0.7913\n",
      "Epoch 178/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1409 - accuracy: 0.9459 - val_loss: 0.6687 - val_accuracy: 0.7913\n",
      "Epoch 179/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1518 - accuracy: 0.9535 - val_loss: 0.6254 - val_accuracy: 0.8051\n",
      "Epoch 180/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1455 - accuracy: 0.9434 - val_loss: 0.6333 - val_accuracy: 0.7953\n",
      "Epoch 181/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 0.9535 - val_loss: 0.6526 - val_accuracy: 0.7835\n",
      "Epoch 182/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1299 - accuracy: 0.9586 - val_loss: 0.6224 - val_accuracy: 0.8012\n",
      "Epoch 183/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9527 - val_loss: 0.6962 - val_accuracy: 0.8051\n",
      "Epoch 184/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9544 - val_loss: 0.6153 - val_accuracy: 0.7992\n",
      "Epoch 185/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1595 - accuracy: 0.9443 - val_loss: 0.6601 - val_accuracy: 0.7953\n",
      "Epoch 186/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.1478 - accuracy: 0.9535 - val_loss: 0.6742 - val_accuracy: 0.7776\n",
      "Epoch 187/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1893 - accuracy: 0.9333 - val_loss: 0.6266 - val_accuracy: 0.7933\n",
      "Epoch 188/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9502 - val_loss: 0.6630 - val_accuracy: 0.7815\n",
      "Epoch 189/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1601 - accuracy: 0.9417 - val_loss: 0.6525 - val_accuracy: 0.7756\n",
      "Epoch 190/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9561 - val_loss: 0.6585 - val_accuracy: 0.7992\n",
      "Epoch 191/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9527 - val_loss: 0.6201 - val_accuracy: 0.7933\n",
      "Epoch 192/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1401 - accuracy: 0.9468 - val_loss: 0.6135 - val_accuracy: 0.7913\n",
      "Epoch 193/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9485 - val_loss: 0.6778 - val_accuracy: 0.7795\n",
      "Epoch 194/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9620 - val_loss: 0.6232 - val_accuracy: 0.7717\n",
      "Epoch 195/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1096 - accuracy: 0.9645 - val_loss: 0.6710 - val_accuracy: 0.7815\n",
      "Epoch 196/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1546 - accuracy: 0.9434 - val_loss: 0.6811 - val_accuracy: 0.7933\n",
      "Epoch 197/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1218 - accuracy: 0.9586 - val_loss: 0.6444 - val_accuracy: 0.7894\n",
      "Epoch 198/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1528 - accuracy: 0.9459 - val_loss: 0.6855 - val_accuracy: 0.7835\n",
      "Epoch 199/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9552 - val_loss: 0.6491 - val_accuracy: 0.7815\n",
      "Epoch 200/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1450 - accuracy: 0.9510 - val_loss: 0.5907 - val_accuracy: 0.7953\n",
      "Epoch 201/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1458 - accuracy: 0.9519 - val_loss: 0.6186 - val_accuracy: 0.8051\n",
      "Epoch 202/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1237 - accuracy: 0.9544 - val_loss: 0.5999 - val_accuracy: 0.7795\n",
      "Epoch 203/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1280 - accuracy: 0.9535 - val_loss: 0.6058 - val_accuracy: 0.7913\n",
      "Epoch 204/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1241 - accuracy: 0.9586 - val_loss: 0.6279 - val_accuracy: 0.8071\n",
      "Epoch 205/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1247 - accuracy: 0.9586 - val_loss: 0.6577 - val_accuracy: 0.7894\n",
      "Epoch 206/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1366 - accuracy: 0.9527 - val_loss: 0.6227 - val_accuracy: 0.7953\n",
      "Epoch 207/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9586 - val_loss: 0.6511 - val_accuracy: 0.7972\n",
      "Epoch 208/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1266 - accuracy: 0.9611 - val_loss: 0.6377 - val_accuracy: 0.8071\n",
      "Epoch 209/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1156 - accuracy: 0.9569 - val_loss: 0.6078 - val_accuracy: 0.7894\n",
      "Epoch 210/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.1078 - accuracy: 0.9569 - val_loss: 0.6275 - val_accuracy: 0.8031\n",
      "Epoch 211/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1462 - accuracy: 0.9468 - val_loss: 0.6768 - val_accuracy: 0.7972\n",
      "Epoch 212/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1291 - accuracy: 0.9561 - val_loss: 0.6725 - val_accuracy: 0.7953\n",
      "Epoch 213/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1241 - accuracy: 0.9569 - val_loss: 0.6816 - val_accuracy: 0.7913\n",
      "Epoch 214/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1355 - accuracy: 0.9569 - val_loss: 0.6084 - val_accuracy: 0.7953\n",
      "Epoch 215/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9628 - val_loss: 0.6116 - val_accuracy: 0.7874\n",
      "Epoch 216/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1297 - accuracy: 0.9527 - val_loss: 0.6445 - val_accuracy: 0.8031\n",
      "Epoch 217/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1397 - accuracy: 0.9493 - val_loss: 0.7270 - val_accuracy: 0.7756\n",
      "Epoch 218/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.9561 - val_loss: 0.6572 - val_accuracy: 0.7972\n",
      "Epoch 219/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1206 - accuracy: 0.9620 - val_loss: 0.6373 - val_accuracy: 0.7795\n",
      "Epoch 220/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9569 - val_loss: 0.6700 - val_accuracy: 0.7913\n",
      "Epoch 221/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9552 - val_loss: 0.6237 - val_accuracy: 0.7874\n",
      "Epoch 222/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9561 - val_loss: 0.6157 - val_accuracy: 0.7992\n",
      "Epoch 223/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1110 - accuracy: 0.9569 - val_loss: 0.6554 - val_accuracy: 0.8071\n",
      "Epoch 224/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1067 - accuracy: 0.9595 - val_loss: 0.6815 - val_accuracy: 0.7992\n",
      "Epoch 225/700\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 0.1062 - accuracy: 0.9679 - val_loss: 0.6639 - val_accuracy: 0.7874\n",
      "Epoch 226/700\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 0.1186 - accuracy: 0.9611 - val_loss: 0.7091 - val_accuracy: 0.7992\n",
      "Epoch 227/700\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 0.1324 - accuracy: 0.9578 - val_loss: 0.6561 - val_accuracy: 0.8031\n",
      "Epoch 228/700\n",
      "74/74 [==============================] - 1s 8ms/step - loss: 0.1087 - accuracy: 0.9654 - val_loss: 0.7074 - val_accuracy: 0.7894\n",
      "Epoch 229/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0977 - accuracy: 0.9696 - val_loss: 0.6520 - val_accuracy: 0.7913\n",
      "Epoch 230/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1119 - accuracy: 0.9654 - val_loss: 0.6526 - val_accuracy: 0.8031\n",
      "Epoch 231/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1539 - accuracy: 0.9552 - val_loss: 0.7246 - val_accuracy: 0.7776\n",
      "Epoch 232/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1216 - accuracy: 0.9679 - val_loss: 0.6908 - val_accuracy: 0.7933\n",
      "Epoch 233/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1116 - accuracy: 0.9611 - val_loss: 0.7497 - val_accuracy: 0.7874\n",
      "Epoch 234/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1176 - accuracy: 0.9595 - val_loss: 0.7257 - val_accuracy: 0.8051\n",
      "Epoch 235/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0897 - accuracy: 0.9688 - val_loss: 0.6443 - val_accuracy: 0.8091\n",
      "Epoch 236/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1281 - accuracy: 0.9595 - val_loss: 0.6871 - val_accuracy: 0.7992\n",
      "Epoch 237/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1463 - accuracy: 0.9552 - val_loss: 0.6775 - val_accuracy: 0.8012\n",
      "Epoch 238/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1418 - accuracy: 0.9561 - val_loss: 0.7027 - val_accuracy: 0.7736\n",
      "Epoch 239/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1275 - accuracy: 0.9586 - val_loss: 0.6895 - val_accuracy: 0.7835\n",
      "Epoch 240/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1423 - accuracy: 0.9569 - val_loss: 0.6566 - val_accuracy: 0.7933\n",
      "Epoch 241/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1066 - accuracy: 0.9603 - val_loss: 0.7227 - val_accuracy: 0.7539\n",
      "Epoch 242/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1114 - accuracy: 0.9654 - val_loss: 0.6693 - val_accuracy: 0.8012\n",
      "Epoch 243/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.1130 - accuracy: 0.9662 - val_loss: 0.7202 - val_accuracy: 0.7815\n",
      "Epoch 244/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0930 - accuracy: 0.9645 - val_loss: 0.6914 - val_accuracy: 0.7835\n",
      "Epoch 245/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1361 - accuracy: 0.9527 - val_loss: 0.7220 - val_accuracy: 0.7913\n",
      "Epoch 246/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9679 - val_loss: 0.6977 - val_accuracy: 0.7894\n",
      "Epoch 247/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9628 - val_loss: 0.7151 - val_accuracy: 0.7913\n",
      "Epoch 248/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1324 - accuracy: 0.9561 - val_loss: 0.6973 - val_accuracy: 0.7953\n",
      "Epoch 249/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9696 - val_loss: 0.6835 - val_accuracy: 0.7933\n",
      "Epoch 250/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1048 - accuracy: 0.9620 - val_loss: 0.6736 - val_accuracy: 0.7953\n",
      "Epoch 251/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1581 - accuracy: 0.9459 - val_loss: 0.6956 - val_accuracy: 0.7854\n",
      "Epoch 252/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9662 - val_loss: 0.6635 - val_accuracy: 0.7933\n",
      "Epoch 253/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9603 - val_loss: 0.7397 - val_accuracy: 0.7776\n",
      "Epoch 254/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1231 - accuracy: 0.9628 - val_loss: 0.7209 - val_accuracy: 0.7756\n",
      "Epoch 255/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9552 - val_loss: 0.6429 - val_accuracy: 0.7972\n",
      "Epoch 256/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0775 - accuracy: 0.9738 - val_loss: 0.6696 - val_accuracy: 0.7913\n",
      "Epoch 257/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1526 - accuracy: 0.9510 - val_loss: 0.6978 - val_accuracy: 0.7953\n",
      "Epoch 258/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1268 - accuracy: 0.9578 - val_loss: 0.6876 - val_accuracy: 0.7894\n",
      "Epoch 259/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1150 - accuracy: 0.9679 - val_loss: 0.7136 - val_accuracy: 0.7913\n",
      "Epoch 260/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9637 - val_loss: 0.6735 - val_accuracy: 0.8189\n",
      "Epoch 261/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1433 - accuracy: 0.9586 - val_loss: 0.7166 - val_accuracy: 0.7992\n",
      "Epoch 262/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1222 - accuracy: 0.9552 - val_loss: 0.7153 - val_accuracy: 0.7894\n",
      "Epoch 263/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.9620 - val_loss: 0.6719 - val_accuracy: 0.7933\n",
      "Epoch 264/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9679 - val_loss: 0.7231 - val_accuracy: 0.7835\n",
      "Epoch 265/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0844 - accuracy: 0.9772 - val_loss: 0.7568 - val_accuracy: 0.7736\n",
      "Epoch 266/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1018 - accuracy: 0.9654 - val_loss: 0.7331 - val_accuracy: 0.7972\n",
      "Epoch 267/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9654 - val_loss: 0.7447 - val_accuracy: 0.7835\n",
      "Epoch 268/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0989 - accuracy: 0.9696 - val_loss: 0.7581 - val_accuracy: 0.7677\n",
      "Epoch 269/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9671 - val_loss: 0.7439 - val_accuracy: 0.7795\n",
      "Epoch 270/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1082 - accuracy: 0.9671 - val_loss: 0.7266 - val_accuracy: 0.7854\n",
      "Epoch 271/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9611 - val_loss: 0.7265 - val_accuracy: 0.7854\n",
      "Epoch 272/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1043 - accuracy: 0.9628 - val_loss: 0.7499 - val_accuracy: 0.7795\n",
      "Epoch 273/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1380 - accuracy: 0.9595 - val_loss: 0.6970 - val_accuracy: 0.7795\n",
      "Epoch 274/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1071 - accuracy: 0.9696 - val_loss: 0.7035 - val_accuracy: 0.7854\n",
      "Epoch 275/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1324 - accuracy: 0.9493 - val_loss: 0.6650 - val_accuracy: 0.8031\n",
      "Epoch 276/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1080 - accuracy: 0.9611 - val_loss: 0.6844 - val_accuracy: 0.8051\n",
      "Epoch 277/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.1102 - accuracy: 0.9662 - val_loss: 0.7897 - val_accuracy: 0.7795\n",
      "Epoch 278/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1295 - accuracy: 0.9569 - val_loss: 0.6861 - val_accuracy: 0.8110\n",
      "Epoch 279/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0980 - accuracy: 0.9662 - val_loss: 0.7544 - val_accuracy: 0.7933\n",
      "Epoch 280/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9603 - val_loss: 0.7238 - val_accuracy: 0.7894\n",
      "Epoch 281/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1049 - accuracy: 0.9654 - val_loss: 0.8559 - val_accuracy: 0.7677\n",
      "Epoch 282/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0885 - accuracy: 0.9713 - val_loss: 0.7387 - val_accuracy: 0.7756\n",
      "Epoch 283/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0984 - accuracy: 0.9679 - val_loss: 0.7577 - val_accuracy: 0.7795\n",
      "Epoch 284/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9704 - val_loss: 0.8359 - val_accuracy: 0.7598\n",
      "Epoch 285/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0844 - accuracy: 0.9713 - val_loss: 0.7515 - val_accuracy: 0.7854\n",
      "Epoch 286/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1119 - accuracy: 0.9603 - val_loss: 0.7720 - val_accuracy: 0.7795\n",
      "Epoch 287/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1134 - accuracy: 0.9586 - val_loss: 0.7809 - val_accuracy: 0.7815\n",
      "Epoch 288/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0989 - accuracy: 0.9654 - val_loss: 0.7526 - val_accuracy: 0.7894\n",
      "Epoch 289/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9721 - val_loss: 0.7574 - val_accuracy: 0.7854\n",
      "Epoch 290/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.9780 - val_loss: 0.7535 - val_accuracy: 0.7874\n",
      "Epoch 291/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1011 - accuracy: 0.9654 - val_loss: 0.7540 - val_accuracy: 0.7953\n",
      "Epoch 292/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1099 - accuracy: 0.9561 - val_loss: 0.7501 - val_accuracy: 0.7874\n",
      "Epoch 293/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9603 - val_loss: 0.8217 - val_accuracy: 0.7657\n",
      "Epoch 294/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1155 - accuracy: 0.9637 - val_loss: 0.7815 - val_accuracy: 0.7697\n",
      "Epoch 295/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1217 - accuracy: 0.9544 - val_loss: 0.7942 - val_accuracy: 0.7854\n",
      "Epoch 296/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1224 - accuracy: 0.9620 - val_loss: 0.7542 - val_accuracy: 0.7736\n",
      "Epoch 297/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9561 - val_loss: 0.7829 - val_accuracy: 0.7657\n",
      "Epoch 298/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9611 - val_loss: 0.7778 - val_accuracy: 0.7776\n",
      "Epoch 299/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9696 - val_loss: 0.7371 - val_accuracy: 0.7933\n",
      "Epoch 300/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1090 - accuracy: 0.9628 - val_loss: 0.7467 - val_accuracy: 0.7933\n",
      "Epoch 301/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0965 - accuracy: 0.9679 - val_loss: 0.7846 - val_accuracy: 0.7717\n",
      "Epoch 302/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9654 - val_loss: 0.7327 - val_accuracy: 0.7874\n",
      "Epoch 303/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9671 - val_loss: 0.7402 - val_accuracy: 0.7992\n",
      "Epoch 304/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9620 - val_loss: 0.6712 - val_accuracy: 0.7933\n",
      "Epoch 305/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1035 - accuracy: 0.9713 - val_loss: 0.7417 - val_accuracy: 0.7795\n",
      "Epoch 306/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9730 - val_loss: 0.7204 - val_accuracy: 0.7854\n",
      "Epoch 307/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0839 - accuracy: 0.9696 - val_loss: 0.7236 - val_accuracy: 0.7874\n",
      "Epoch 308/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0775 - accuracy: 0.9704 - val_loss: 0.7326 - val_accuracy: 0.7913\n",
      "Epoch 309/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0984 - accuracy: 0.9603 - val_loss: 0.7352 - val_accuracy: 0.7756\n",
      "Epoch 310/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0981 - accuracy: 0.9713 - val_loss: 0.7285 - val_accuracy: 0.7835\n",
      "Epoch 311/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9645 - val_loss: 0.7574 - val_accuracy: 0.7776\n",
      "Epoch 312/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1188 - accuracy: 0.9561 - val_loss: 0.7242 - val_accuracy: 0.7776\n",
      "Epoch 313/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0998 - accuracy: 0.9595 - val_loss: 0.7554 - val_accuracy: 0.7835\n",
      "Epoch 314/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1155 - accuracy: 0.9611 - val_loss: 0.7257 - val_accuracy: 0.7854\n",
      "Epoch 315/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.9611 - val_loss: 0.7662 - val_accuracy: 0.8130\n",
      "Epoch 316/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0773 - accuracy: 0.9713 - val_loss: 0.7521 - val_accuracy: 0.7992\n",
      "Epoch 317/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9755 - val_loss: 0.7265 - val_accuracy: 0.8031\n",
      "Epoch 318/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1131 - accuracy: 0.9620 - val_loss: 0.7975 - val_accuracy: 0.7835\n",
      "Epoch 319/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1307 - accuracy: 0.9637 - val_loss: 0.6993 - val_accuracy: 0.8051\n",
      "Epoch 320/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0837 - accuracy: 0.9671 - val_loss: 0.6996 - val_accuracy: 0.7992\n",
      "Epoch 321/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1157 - accuracy: 0.9603 - val_loss: 0.7613 - val_accuracy: 0.7874\n",
      "Epoch 322/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0996 - accuracy: 0.9696 - val_loss: 0.7932 - val_accuracy: 0.7874\n",
      "Epoch 323/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9569 - val_loss: 0.7971 - val_accuracy: 0.7854\n",
      "Epoch 324/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0933 - accuracy: 0.9662 - val_loss: 0.7518 - val_accuracy: 0.7913\n",
      "Epoch 325/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9713 - val_loss: 0.7195 - val_accuracy: 0.7972\n",
      "Epoch 326/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1099 - accuracy: 0.9611 - val_loss: 0.7254 - val_accuracy: 0.8051\n",
      "Epoch 327/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1035 - accuracy: 0.9671 - val_loss: 0.7997 - val_accuracy: 0.7874\n",
      "Epoch 328/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9578 - val_loss: 0.7627 - val_accuracy: 0.7913\n",
      "Epoch 329/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0861 - accuracy: 0.9713 - val_loss: 0.7505 - val_accuracy: 0.7736\n",
      "Epoch 330/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0879 - accuracy: 0.9755 - val_loss: 0.7385 - val_accuracy: 0.7835\n",
      "Epoch 331/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9603 - val_loss: 0.7574 - val_accuracy: 0.7795\n",
      "Epoch 332/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0972 - accuracy: 0.9679 - val_loss: 0.7913 - val_accuracy: 0.7717\n",
      "Epoch 333/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9654 - val_loss: 0.7682 - val_accuracy: 0.7854\n",
      "Epoch 334/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.9721 - val_loss: 0.7036 - val_accuracy: 0.7854\n",
      "Epoch 335/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0705 - accuracy: 0.9764 - val_loss: 0.6830 - val_accuracy: 0.7953\n",
      "Epoch 336/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1100 - accuracy: 0.9586 - val_loss: 0.7915 - val_accuracy: 0.7815\n",
      "Epoch 337/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9730 - val_loss: 0.7665 - val_accuracy: 0.7874\n",
      "Epoch 338/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0660 - accuracy: 0.9738 - val_loss: 0.7324 - val_accuracy: 0.7874\n",
      "Epoch 339/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9721 - val_loss: 0.7472 - val_accuracy: 0.8012\n",
      "Epoch 340/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.9696 - val_loss: 0.7412 - val_accuracy: 0.7795\n",
      "Epoch 341/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0996 - accuracy: 0.9696 - val_loss: 0.7072 - val_accuracy: 0.7854\n",
      "Epoch 342/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.9611 - val_loss: 0.6909 - val_accuracy: 0.7933\n",
      "Epoch 343/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0699 - accuracy: 0.9772 - val_loss: 0.6920 - val_accuracy: 0.7953\n",
      "Epoch 344/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0828 - accuracy: 0.9730 - val_loss: 0.7590 - val_accuracy: 0.7953\n",
      "Epoch 345/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1019 - accuracy: 0.9662 - val_loss: 0.7343 - val_accuracy: 0.7835\n",
      "Epoch 346/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.9738 - val_loss: 0.7129 - val_accuracy: 0.7933\n",
      "Epoch 347/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0801 - accuracy: 0.9688 - val_loss: 0.6952 - val_accuracy: 0.7815\n",
      "Epoch 348/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9679 - val_loss: 0.7603 - val_accuracy: 0.7736\n",
      "Epoch 349/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1140 - accuracy: 0.9645 - val_loss: 0.7800 - val_accuracy: 0.7815\n",
      "Epoch 350/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9603 - val_loss: 0.8000 - val_accuracy: 0.7657\n",
      "Epoch 351/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0970 - accuracy: 0.9679 - val_loss: 0.7655 - val_accuracy: 0.7854\n",
      "Epoch 352/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9747 - val_loss: 0.9214 - val_accuracy: 0.7382\n",
      "Epoch 353/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0860 - accuracy: 0.9671 - val_loss: 0.8086 - val_accuracy: 0.7717\n",
      "Epoch 354/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0798 - accuracy: 0.9679 - val_loss: 0.8791 - val_accuracy: 0.7657\n",
      "Epoch 355/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0960 - accuracy: 0.9671 - val_loss: 0.8041 - val_accuracy: 0.7815\n",
      "Epoch 356/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1012 - accuracy: 0.9671 - val_loss: 0.8272 - val_accuracy: 0.7677\n",
      "Epoch 357/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0965 - accuracy: 0.9696 - val_loss: 0.7820 - val_accuracy: 0.7854\n",
      "Epoch 358/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0827 - accuracy: 0.9713 - val_loss: 0.7466 - val_accuracy: 0.7894\n",
      "Epoch 359/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9747 - val_loss: 0.7331 - val_accuracy: 0.7854\n",
      "Epoch 360/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1041 - accuracy: 0.9628 - val_loss: 0.7332 - val_accuracy: 0.7894\n",
      "Epoch 361/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0696 - accuracy: 0.9772 - val_loss: 0.7730 - val_accuracy: 0.7835\n",
      "Epoch 362/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9654 - val_loss: 0.7719 - val_accuracy: 0.7854\n",
      "Epoch 363/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9671 - val_loss: 0.7753 - val_accuracy: 0.7795\n",
      "Epoch 364/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0730 - accuracy: 0.9713 - val_loss: 0.8513 - val_accuracy: 0.7638\n",
      "Epoch 365/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1023 - accuracy: 0.9696 - val_loss: 0.7923 - val_accuracy: 0.7972\n",
      "Epoch 366/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9704 - val_loss: 0.7926 - val_accuracy: 0.7933\n",
      "Epoch 367/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.9704 - val_loss: 0.7833 - val_accuracy: 0.7756\n",
      "Epoch 368/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9755 - val_loss: 0.7739 - val_accuracy: 0.7854\n",
      "Epoch 369/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0819 - accuracy: 0.9747 - val_loss: 0.8408 - val_accuracy: 0.7795\n",
      "Epoch 370/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9654 - val_loss: 0.7489 - val_accuracy: 0.8012\n",
      "Epoch 371/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9688 - val_loss: 0.7734 - val_accuracy: 0.7835\n",
      "Epoch 372/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0877 - accuracy: 0.9755 - val_loss: 0.7385 - val_accuracy: 0.7972\n",
      "Epoch 373/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0717 - accuracy: 0.9764 - val_loss: 0.7864 - val_accuracy: 0.7854\n",
      "Epoch 374/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1063 - accuracy: 0.9578 - val_loss: 0.7471 - val_accuracy: 0.7854\n",
      "Epoch 375/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1207 - accuracy: 0.9662 - val_loss: 0.7505 - val_accuracy: 0.7835\n",
      "Epoch 376/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 0.9789 - val_loss: 0.7654 - val_accuracy: 0.7953\n",
      "Epoch 377/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0574 - accuracy: 0.9814 - val_loss: 0.7301 - val_accuracy: 0.7953\n",
      "Epoch 378/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0646 - accuracy: 0.9755 - val_loss: 0.7476 - val_accuracy: 0.8012\n",
      "Epoch 379/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.9747 - val_loss: 0.8034 - val_accuracy: 0.8051\n",
      "Epoch 380/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0802 - accuracy: 0.9772 - val_loss: 0.7879 - val_accuracy: 0.7894\n",
      "Epoch 381/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1063 - accuracy: 0.9671 - val_loss: 0.8110 - val_accuracy: 0.7677\n",
      "Epoch 382/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0733 - accuracy: 0.9738 - val_loss: 0.8356 - val_accuracy: 0.7854\n",
      "Epoch 383/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0857 - accuracy: 0.9696 - val_loss: 0.8079 - val_accuracy: 0.7972\n",
      "Epoch 384/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9713 - val_loss: 0.7938 - val_accuracy: 0.7815\n",
      "Epoch 385/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0762 - accuracy: 0.9780 - val_loss: 0.8280 - val_accuracy: 0.7815\n",
      "Epoch 386/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0912 - accuracy: 0.9688 - val_loss: 0.7585 - val_accuracy: 0.7874\n",
      "Epoch 387/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9696 - val_loss: 0.7365 - val_accuracy: 0.7953\n",
      "Epoch 388/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9611 - val_loss: 0.7496 - val_accuracy: 0.7933\n",
      "Epoch 389/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1003 - accuracy: 0.9688 - val_loss: 0.7916 - val_accuracy: 0.7815\n",
      "Epoch 390/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9628 - val_loss: 0.8162 - val_accuracy: 0.7657\n",
      "Epoch 391/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1294 - accuracy: 0.9552 - val_loss: 0.7574 - val_accuracy: 0.7756\n",
      "Epoch 392/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9637 - val_loss: 0.8184 - val_accuracy: 0.7657\n",
      "Epoch 393/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0989 - accuracy: 0.9645 - val_loss: 0.7277 - val_accuracy: 0.7874\n",
      "Epoch 394/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0873 - accuracy: 0.9688 - val_loss: 0.7750 - val_accuracy: 0.7795\n",
      "Epoch 395/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0799 - accuracy: 0.9721 - val_loss: 0.7479 - val_accuracy: 0.7894\n",
      "Epoch 396/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9637 - val_loss: 0.7992 - val_accuracy: 0.7933\n",
      "Epoch 397/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.9764 - val_loss: 0.8448 - val_accuracy: 0.7657\n",
      "Epoch 398/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0866 - accuracy: 0.9713 - val_loss: 0.7984 - val_accuracy: 0.7717\n",
      "Epoch 399/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9603 - val_loss: 0.7878 - val_accuracy: 0.7874\n",
      "Epoch 400/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0776 - accuracy: 0.9764 - val_loss: 0.7564 - val_accuracy: 0.7913\n",
      "Epoch 401/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1058 - accuracy: 0.9611 - val_loss: 0.7990 - val_accuracy: 0.7933\n",
      "Epoch 402/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9730 - val_loss: 0.7495 - val_accuracy: 0.7776\n",
      "Epoch 403/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0676 - accuracy: 0.9764 - val_loss: 0.8467 - val_accuracy: 0.7874\n",
      "Epoch 404/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0715 - accuracy: 0.9764 - val_loss: 0.8223 - val_accuracy: 0.7795\n",
      "Epoch 405/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0847 - accuracy: 0.9704 - val_loss: 0.7619 - val_accuracy: 0.7933\n",
      "Epoch 406/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0814 - accuracy: 0.9696 - val_loss: 0.7625 - val_accuracy: 0.7815\n",
      "Epoch 407/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0873 - accuracy: 0.9721 - val_loss: 0.7978 - val_accuracy: 0.7894\n",
      "Epoch 408/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0904 - accuracy: 0.9738 - val_loss: 0.7670 - val_accuracy: 0.8031\n",
      "Epoch 409/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9671 - val_loss: 0.7506 - val_accuracy: 0.7913\n",
      "Epoch 410/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0705 - accuracy: 0.9696 - val_loss: 0.7893 - val_accuracy: 0.7835\n",
      "Epoch 411/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0910 - accuracy: 0.9637 - val_loss: 0.8462 - val_accuracy: 0.7854\n",
      "Epoch 412/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0684 - accuracy: 0.9780 - val_loss: 0.8126 - val_accuracy: 0.7933\n",
      "Epoch 413/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9738 - val_loss: 0.7915 - val_accuracy: 0.7854\n",
      "Epoch 414/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.9772 - val_loss: 0.8702 - val_accuracy: 0.7854\n",
      "Epoch 415/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0606 - accuracy: 0.9772 - val_loss: 0.8479 - val_accuracy: 0.7795\n",
      "Epoch 416/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9637 - val_loss: 0.8811 - val_accuracy: 0.7756\n",
      "Epoch 417/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9704 - val_loss: 0.7979 - val_accuracy: 0.7874\n",
      "Epoch 418/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0814 - accuracy: 0.9721 - val_loss: 0.8582 - val_accuracy: 0.7697\n",
      "Epoch 419/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0819 - accuracy: 0.9713 - val_loss: 0.9286 - val_accuracy: 0.7815\n",
      "Epoch 420/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.9764 - val_loss: 0.8928 - val_accuracy: 0.7638\n",
      "Epoch 421/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0952 - accuracy: 0.9671 - val_loss: 0.8491 - val_accuracy: 0.7795\n",
      "Epoch 422/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0979 - accuracy: 0.9671 - val_loss: 0.8594 - val_accuracy: 0.7677\n",
      "Epoch 423/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1003 - accuracy: 0.9662 - val_loss: 0.8501 - val_accuracy: 0.7776\n",
      "Epoch 424/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9814 - val_loss: 0.7998 - val_accuracy: 0.7776\n",
      "Epoch 425/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.9679 - val_loss: 0.7758 - val_accuracy: 0.7677\n",
      "Epoch 426/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9730 - val_loss: 0.7493 - val_accuracy: 0.7874\n",
      "Epoch 427/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0796 - accuracy: 0.9696 - val_loss: 0.7552 - val_accuracy: 0.7894\n",
      "Epoch 428/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0725 - accuracy: 0.9797 - val_loss: 0.7786 - val_accuracy: 0.7717\n",
      "Epoch 429/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0802 - accuracy: 0.9730 - val_loss: 0.7914 - val_accuracy: 0.7776\n",
      "Epoch 430/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9755 - val_loss: 0.7874 - val_accuracy: 0.7835\n",
      "Epoch 431/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0556 - accuracy: 0.9806 - val_loss: 0.7474 - val_accuracy: 0.7854\n",
      "Epoch 432/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0726 - accuracy: 0.9780 - val_loss: 0.8218 - val_accuracy: 0.7854\n",
      "Epoch 433/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0663 - accuracy: 0.9780 - val_loss: 0.8767 - val_accuracy: 0.7736\n",
      "Epoch 434/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0912 - accuracy: 0.9730 - val_loss: 0.8714 - val_accuracy: 0.7717\n",
      "Epoch 435/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1142 - accuracy: 0.9696 - val_loss: 0.8315 - val_accuracy: 0.7677\n",
      "Epoch 436/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0619 - accuracy: 0.9806 - val_loss: 0.8681 - val_accuracy: 0.7697\n",
      "Epoch 437/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1038 - accuracy: 0.9628 - val_loss: 0.7992 - val_accuracy: 0.7736\n",
      "Epoch 438/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.9755 - val_loss: 0.8240 - val_accuracy: 0.7618\n",
      "Epoch 439/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0994 - accuracy: 0.9696 - val_loss: 0.7953 - val_accuracy: 0.7913\n",
      "Epoch 440/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0821 - accuracy: 0.9696 - val_loss: 0.7487 - val_accuracy: 0.8031\n",
      "Epoch 441/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.1008 - accuracy: 0.9713 - val_loss: 0.8446 - val_accuracy: 0.7618\n",
      "Epoch 442/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.9747 - val_loss: 0.8060 - val_accuracy: 0.7933\n",
      "Epoch 443/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9671 - val_loss: 0.7831 - val_accuracy: 0.7854\n",
      "Epoch 444/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9747 - val_loss: 0.7178 - val_accuracy: 0.8031\n",
      "Epoch 445/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9755 - val_loss: 0.7192 - val_accuracy: 0.7874\n",
      "Epoch 446/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9764 - val_loss: 0.7419 - val_accuracy: 0.7795\n",
      "Epoch 447/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9764 - val_loss: 0.7612 - val_accuracy: 0.7717\n",
      "Epoch 448/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9789 - val_loss: 0.7060 - val_accuracy: 0.7894\n",
      "Epoch 449/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.9747 - val_loss: 0.7817 - val_accuracy: 0.7756\n",
      "Epoch 450/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0842 - accuracy: 0.9738 - val_loss: 0.7805 - val_accuracy: 0.7677\n",
      "Epoch 451/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0740 - accuracy: 0.9696 - val_loss: 0.7759 - val_accuracy: 0.7717\n",
      "Epoch 452/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0850 - accuracy: 0.9738 - val_loss: 0.7712 - val_accuracy: 0.7913\n",
      "Epoch 453/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.9721 - val_loss: 0.8118 - val_accuracy: 0.7854\n",
      "Epoch 454/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9730 - val_loss: 0.7771 - val_accuracy: 0.7874\n",
      "Epoch 455/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0949 - accuracy: 0.9738 - val_loss: 0.8567 - val_accuracy: 0.7717\n",
      "Epoch 456/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.9721 - val_loss: 0.7833 - val_accuracy: 0.7717\n",
      "Epoch 457/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9671 - val_loss: 0.8314 - val_accuracy: 0.7756\n",
      "Epoch 458/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0671 - accuracy: 0.9730 - val_loss: 0.8499 - val_accuracy: 0.7776\n",
      "Epoch 459/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0861 - accuracy: 0.9730 - val_loss: 0.9223 - val_accuracy: 0.7697\n",
      "Epoch 460/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9662 - val_loss: 0.9239 - val_accuracy: 0.7697\n",
      "Epoch 461/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0906 - accuracy: 0.9747 - val_loss: 0.8832 - val_accuracy: 0.7657\n",
      "Epoch 462/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0954 - accuracy: 0.9679 - val_loss: 0.8214 - val_accuracy: 0.7756\n",
      "Epoch 463/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0721 - accuracy: 0.9772 - val_loss: 0.7911 - val_accuracy: 0.7835\n",
      "Epoch 464/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0659 - accuracy: 0.9797 - val_loss: 0.8905 - val_accuracy: 0.7717\n",
      "Epoch 465/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0853 - accuracy: 0.9738 - val_loss: 0.8286 - val_accuracy: 0.7677\n",
      "Epoch 466/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1019 - accuracy: 0.9654 - val_loss: 0.7964 - val_accuracy: 0.7795\n",
      "Epoch 467/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0750 - accuracy: 0.9772 - val_loss: 0.8197 - val_accuracy: 0.7717\n",
      "Epoch 468/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0474 - accuracy: 0.9772 - val_loss: 0.8012 - val_accuracy: 0.7874\n",
      "Epoch 469/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9814 - val_loss: 0.7951 - val_accuracy: 0.7736\n",
      "Epoch 470/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0750 - accuracy: 0.9780 - val_loss: 0.7540 - val_accuracy: 0.7736\n",
      "Epoch 471/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9738 - val_loss: 0.7474 - val_accuracy: 0.7953\n",
      "Epoch 472/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0724 - accuracy: 0.9738 - val_loss: 0.8209 - val_accuracy: 0.7854\n",
      "Epoch 473/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0702 - accuracy: 0.9721 - val_loss: 0.7715 - val_accuracy: 0.7815\n",
      "Epoch 474/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0517 - accuracy: 0.9814 - val_loss: 0.7913 - val_accuracy: 0.7894\n",
      "Epoch 475/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1242 - accuracy: 0.9645 - val_loss: 0.7744 - val_accuracy: 0.7776\n",
      "Epoch 476/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.9738 - val_loss: 0.7624 - val_accuracy: 0.7795\n",
      "Epoch 477/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9789 - val_loss: 0.8379 - val_accuracy: 0.7874\n",
      "Epoch 478/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0516 - accuracy: 0.9840 - val_loss: 0.8109 - val_accuracy: 0.7953\n",
      "Epoch 479/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0675 - accuracy: 0.9738 - val_loss: 0.8244 - val_accuracy: 0.7815\n",
      "Epoch 480/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9696 - val_loss: 0.7838 - val_accuracy: 0.7894\n",
      "Epoch 481/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.0604 - accuracy: 0.9797 - val_loss: 0.8304 - val_accuracy: 0.7638\n",
      "Epoch 482/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.9721 - val_loss: 0.6984 - val_accuracy: 0.8012\n",
      "Epoch 483/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0808 - accuracy: 0.9755 - val_loss: 0.7261 - val_accuracy: 0.7913\n",
      "Epoch 484/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9730 - val_loss: 0.7408 - val_accuracy: 0.7756\n",
      "Epoch 485/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9704 - val_loss: 0.7612 - val_accuracy: 0.7854\n",
      "Epoch 486/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9789 - val_loss: 0.7751 - val_accuracy: 0.7795\n",
      "Epoch 487/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0721 - accuracy: 0.9713 - val_loss: 0.7656 - val_accuracy: 0.7657\n",
      "Epoch 488/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0596 - accuracy: 0.9840 - val_loss: 0.8481 - val_accuracy: 0.7579\n",
      "Epoch 489/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0745 - accuracy: 0.9713 - val_loss: 0.8664 - val_accuracy: 0.7736\n",
      "Epoch 490/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9671 - val_loss: 0.7607 - val_accuracy: 0.7854\n",
      "Epoch 491/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0820 - accuracy: 0.9730 - val_loss: 0.7587 - val_accuracy: 0.7835\n",
      "Epoch 492/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0703 - accuracy: 0.9755 - val_loss: 0.7667 - val_accuracy: 0.7953\n",
      "Epoch 493/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0742 - accuracy: 0.9738 - val_loss: 0.7153 - val_accuracy: 0.7894\n",
      "Epoch 494/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0898 - accuracy: 0.9738 - val_loss: 0.7478 - val_accuracy: 0.7894\n",
      "Epoch 495/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0586 - accuracy: 0.9848 - val_loss: 0.7389 - val_accuracy: 0.7874\n",
      "Epoch 496/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0644 - accuracy: 0.9806 - val_loss: 0.7444 - val_accuracy: 0.7795\n",
      "Epoch 497/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9780 - val_loss: 0.7492 - val_accuracy: 0.7874\n",
      "Epoch 498/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0688 - accuracy: 0.9789 - val_loss: 0.8088 - val_accuracy: 0.7874\n",
      "Epoch 499/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0775 - accuracy: 0.9747 - val_loss: 0.7063 - val_accuracy: 0.7992\n",
      "Epoch 500/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0571 - accuracy: 0.9840 - val_loss: 0.7257 - val_accuracy: 0.8130\n",
      "Epoch 501/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9780 - val_loss: 0.8465 - val_accuracy: 0.7795\n",
      "Epoch 502/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0769 - accuracy: 0.9764 - val_loss: 0.8868 - val_accuracy: 0.7736\n",
      "Epoch 503/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 0.9671 - val_loss: 0.8515 - val_accuracy: 0.7776\n",
      "Epoch 504/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0605 - accuracy: 0.9764 - val_loss: 0.8263 - val_accuracy: 0.7874\n",
      "Epoch 505/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9713 - val_loss: 0.8240 - val_accuracy: 0.7815\n",
      "Epoch 506/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9764 - val_loss: 0.8485 - val_accuracy: 0.7835\n",
      "Epoch 507/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9679 - val_loss: 0.9005 - val_accuracy: 0.7854\n",
      "Epoch 508/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9747 - val_loss: 0.8444 - val_accuracy: 0.7717\n",
      "Epoch 509/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9755 - val_loss: 0.9059 - val_accuracy: 0.7677\n",
      "Epoch 510/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1207 - accuracy: 0.9679 - val_loss: 0.8153 - val_accuracy: 0.8012\n",
      "Epoch 511/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0966 - accuracy: 0.9654 - val_loss: 0.7705 - val_accuracy: 0.7854\n",
      "Epoch 512/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0595 - accuracy: 0.9780 - val_loss: 0.8167 - val_accuracy: 0.7835\n",
      "Epoch 513/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0861 - accuracy: 0.9704 - val_loss: 0.7888 - val_accuracy: 0.7776\n",
      "Epoch 514/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9831 - val_loss: 0.8013 - val_accuracy: 0.7815\n",
      "Epoch 515/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9780 - val_loss: 0.8147 - val_accuracy: 0.7657\n",
      "Epoch 516/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.9704 - val_loss: 0.7601 - val_accuracy: 0.7677\n",
      "Epoch 517/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0640 - accuracy: 0.9823 - val_loss: 0.7322 - val_accuracy: 0.7854\n",
      "Epoch 518/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0598 - accuracy: 0.9831 - val_loss: 0.8359 - val_accuracy: 0.7697\n",
      "Epoch 519/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.9755 - val_loss: 0.7974 - val_accuracy: 0.7874\n",
      "Epoch 520/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9721 - val_loss: 0.8551 - val_accuracy: 0.7795\n",
      "Epoch 521/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0858 - accuracy: 0.9671 - val_loss: 0.8614 - val_accuracy: 0.7657\n",
      "Epoch 522/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9704 - val_loss: 0.8860 - val_accuracy: 0.7854\n",
      "Epoch 523/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9823 - val_loss: 0.8270 - val_accuracy: 0.7815\n",
      "Epoch 524/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0598 - accuracy: 0.9789 - val_loss: 0.8172 - val_accuracy: 0.7913\n",
      "Epoch 525/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9789 - val_loss: 0.8084 - val_accuracy: 0.7795\n",
      "Epoch 526/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1036 - accuracy: 0.9671 - val_loss: 0.8437 - val_accuracy: 0.7717\n",
      "Epoch 527/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9797 - val_loss: 0.8694 - val_accuracy: 0.7913\n",
      "Epoch 528/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0918 - accuracy: 0.9747 - val_loss: 0.8991 - val_accuracy: 0.7559\n",
      "Epoch 529/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0582 - accuracy: 0.9789 - val_loss: 0.8693 - val_accuracy: 0.7736\n",
      "Epoch 530/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0761 - accuracy: 0.9747 - val_loss: 0.8371 - val_accuracy: 0.7795\n",
      "Epoch 531/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0635 - accuracy: 0.9789 - val_loss: 0.7831 - val_accuracy: 0.7697\n",
      "Epoch 532/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0705 - accuracy: 0.9738 - val_loss: 0.8098 - val_accuracy: 0.7894\n",
      "Epoch 533/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0896 - accuracy: 0.9713 - val_loss: 0.8109 - val_accuracy: 0.7835\n",
      "Epoch 534/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0654 - accuracy: 0.9730 - val_loss: 0.7264 - val_accuracy: 0.7835\n",
      "Epoch 535/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0654 - accuracy: 0.9772 - val_loss: 0.7427 - val_accuracy: 0.7874\n",
      "Epoch 536/700\n",
      "74/74 [==============================] - 0s 7ms/step - loss: 0.0629 - accuracy: 0.9840 - val_loss: 0.7287 - val_accuracy: 0.7953\n",
      "Epoch 537/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0715 - accuracy: 0.9755 - val_loss: 0.8263 - val_accuracy: 0.7776\n",
      "Epoch 538/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0715 - accuracy: 0.9772 - val_loss: 0.8148 - val_accuracy: 0.8051\n",
      "Epoch 539/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0610 - accuracy: 0.9780 - val_loss: 0.7968 - val_accuracy: 0.8051\n",
      "Epoch 540/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9789 - val_loss: 0.7946 - val_accuracy: 0.7913\n",
      "Epoch 541/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0719 - accuracy: 0.9764 - val_loss: 0.7567 - val_accuracy: 0.7992\n",
      "Epoch 542/700\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 0.0977 - accuracy: 0.9688 - val_loss: 0.7572 - val_accuracy: 0.7992\n",
      "Epoch 543/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0945 - accuracy: 0.9688 - val_loss: 0.7029 - val_accuracy: 0.8071\n",
      "Epoch 544/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0813 - accuracy: 0.9696 - val_loss: 0.7626 - val_accuracy: 0.7874\n",
      "Epoch 545/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9823 - val_loss: 0.7243 - val_accuracy: 0.7894\n",
      "Epoch 546/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0559 - accuracy: 0.9814 - val_loss: 0.7760 - val_accuracy: 0.8110\n",
      "Epoch 547/700\n",
      "74/74 [==============================] - 1s 7ms/step - loss: 0.0762 - accuracy: 0.9823 - val_loss: 0.8001 - val_accuracy: 0.7835\n",
      "Epoch 548/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0753 - accuracy: 0.9797 - val_loss: 0.7130 - val_accuracy: 0.8012\n",
      "Epoch 549/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0672 - accuracy: 0.9814 - val_loss: 0.7322 - val_accuracy: 0.7874\n",
      "Epoch 550/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 0.9780 - val_loss: 0.8220 - val_accuracy: 0.7756\n",
      "Epoch 551/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0708 - accuracy: 0.9738 - val_loss: 0.7816 - val_accuracy: 0.7854\n",
      "Epoch 552/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0938 - accuracy: 0.9721 - val_loss: 0.8442 - val_accuracy: 0.7854\n",
      "Epoch 553/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0789 - accuracy: 0.9755 - val_loss: 0.7185 - val_accuracy: 0.8031\n",
      "Epoch 554/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0791 - accuracy: 0.9688 - val_loss: 0.7307 - val_accuracy: 0.7874\n",
      "Epoch 555/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9831 - val_loss: 0.7178 - val_accuracy: 0.8051\n",
      "Epoch 556/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 0.9789 - val_loss: 0.7616 - val_accuracy: 0.7854\n",
      "Epoch 557/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.7631 - val_accuracy: 0.7815\n",
      "Epoch 558/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0774 - accuracy: 0.9764 - val_loss: 0.7895 - val_accuracy: 0.8012\n",
      "Epoch 559/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0680 - accuracy: 0.9789 - val_loss: 0.7675 - val_accuracy: 0.7894\n",
      "Epoch 560/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9840 - val_loss: 0.7998 - val_accuracy: 0.7992\n",
      "Epoch 561/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9721 - val_loss: 0.8353 - val_accuracy: 0.8051\n",
      "Epoch 562/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0904 - accuracy: 0.9688 - val_loss: 0.8157 - val_accuracy: 0.8031\n",
      "Epoch 563/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9755 - val_loss: 0.8361 - val_accuracy: 0.7756\n",
      "Epoch 564/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0679 - accuracy: 0.9755 - val_loss: 0.8444 - val_accuracy: 0.7874\n",
      "Epoch 565/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0615 - accuracy: 0.9806 - val_loss: 0.8292 - val_accuracy: 0.7795\n",
      "Epoch 566/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0920 - accuracy: 0.9671 - val_loss: 0.7963 - val_accuracy: 0.7657\n",
      "Epoch 567/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.8662 - val_accuracy: 0.7697\n",
      "Epoch 568/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0701 - accuracy: 0.9780 - val_loss: 0.8772 - val_accuracy: 0.7874\n",
      "Epoch 569/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0746 - accuracy: 0.9747 - val_loss: 0.8723 - val_accuracy: 0.7717\n",
      "Epoch 570/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0559 - accuracy: 0.9780 - val_loss: 0.8092 - val_accuracy: 0.7657\n",
      "Epoch 571/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9772 - val_loss: 0.8358 - val_accuracy: 0.7559\n",
      "Epoch 572/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9747 - val_loss: 0.7776 - val_accuracy: 0.7697\n",
      "Epoch 573/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.9780 - val_loss: 0.7922 - val_accuracy: 0.7835\n",
      "Epoch 574/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9772 - val_loss: 0.8617 - val_accuracy: 0.7815\n",
      "Epoch 575/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0814 - accuracy: 0.9738 - val_loss: 0.9530 - val_accuracy: 0.7441\n",
      "Epoch 576/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0807 - accuracy: 0.9772 - val_loss: 0.8669 - val_accuracy: 0.7815\n",
      "Epoch 577/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9772 - val_loss: 0.7906 - val_accuracy: 0.7874\n",
      "Epoch 578/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.9840 - val_loss: 0.7908 - val_accuracy: 0.7913\n",
      "Epoch 579/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9848 - val_loss: 0.8278 - val_accuracy: 0.7933\n",
      "Epoch 580/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0715 - accuracy: 0.9806 - val_loss: 0.8117 - val_accuracy: 0.7795\n",
      "Epoch 581/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0624 - accuracy: 0.9780 - val_loss: 0.8085 - val_accuracy: 0.7992\n",
      "Epoch 582/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0692 - accuracy: 0.9721 - val_loss: 0.8722 - val_accuracy: 0.7756\n",
      "Epoch 583/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0657 - accuracy: 0.9755 - val_loss: 0.8518 - val_accuracy: 0.7933\n",
      "Epoch 584/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.9789 - val_loss: 0.9059 - val_accuracy: 0.7874\n",
      "Epoch 585/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0490 - accuracy: 0.9814 - val_loss: 0.9403 - val_accuracy: 0.7835\n",
      "Epoch 586/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0495 - accuracy: 0.9806 - val_loss: 0.9688 - val_accuracy: 0.7776\n",
      "Epoch 587/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0846 - accuracy: 0.9721 - val_loss: 0.9122 - val_accuracy: 0.7776\n",
      "Epoch 588/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0572 - accuracy: 0.9831 - val_loss: 0.9644 - val_accuracy: 0.7776\n",
      "Epoch 589/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0708 - accuracy: 0.9772 - val_loss: 0.9772 - val_accuracy: 0.7697\n",
      "Epoch 590/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0522 - accuracy: 0.9848 - val_loss: 0.8762 - val_accuracy: 0.7677\n",
      "Epoch 591/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0673 - accuracy: 0.9814 - val_loss: 0.8978 - val_accuracy: 0.7854\n",
      "Epoch 592/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0755 - accuracy: 0.9730 - val_loss: 0.8462 - val_accuracy: 0.7992\n",
      "Epoch 593/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9755 - val_loss: 0.8261 - val_accuracy: 0.7953\n",
      "Epoch 594/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0765 - accuracy: 0.9797 - val_loss: 0.9083 - val_accuracy: 0.7835\n",
      "Epoch 595/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0572 - accuracy: 0.9797 - val_loss: 0.8845 - val_accuracy: 0.7894\n",
      "Epoch 596/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0860 - accuracy: 0.9688 - val_loss: 0.8888 - val_accuracy: 0.7677\n",
      "Epoch 597/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9679 - val_loss: 0.9371 - val_accuracy: 0.7717\n",
      "Epoch 598/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0656 - accuracy: 0.9797 - val_loss: 0.8688 - val_accuracy: 0.7854\n",
      "Epoch 599/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0885 - accuracy: 0.9764 - val_loss: 0.8471 - val_accuracy: 0.7598\n",
      "Epoch 600/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1035 - accuracy: 0.9679 - val_loss: 0.8644 - val_accuracy: 0.7598\n",
      "Epoch 601/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0712 - accuracy: 0.9696 - val_loss: 0.8457 - val_accuracy: 0.7874\n",
      "Epoch 602/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 0.9840 - val_loss: 0.8273 - val_accuracy: 0.7992\n",
      "Epoch 603/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9721 - val_loss: 0.8824 - val_accuracy: 0.7874\n",
      "Epoch 604/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 0.9856 - val_loss: 0.9120 - val_accuracy: 0.7736\n",
      "Epoch 605/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0692 - accuracy: 0.9797 - val_loss: 0.8462 - val_accuracy: 0.7933\n",
      "Epoch 606/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0525 - accuracy: 0.9806 - val_loss: 0.9298 - val_accuracy: 0.7854\n",
      "Epoch 607/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0559 - accuracy: 0.9814 - val_loss: 0.9541 - val_accuracy: 0.7835\n",
      "Epoch 608/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0573 - accuracy: 0.9848 - val_loss: 0.9439 - val_accuracy: 0.7697\n",
      "Epoch 609/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 0.9789 - val_loss: 0.8842 - val_accuracy: 0.7894\n",
      "Epoch 610/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0475 - accuracy: 0.9848 - val_loss: 0.9214 - val_accuracy: 0.7913\n",
      "Epoch 611/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0573 - accuracy: 0.9831 - val_loss: 0.9062 - val_accuracy: 0.7854\n",
      "Epoch 612/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9789 - val_loss: 0.8315 - val_accuracy: 0.7835\n",
      "Epoch 613/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0708 - accuracy: 0.9789 - val_loss: 0.9038 - val_accuracy: 0.7894\n",
      "Epoch 614/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0603 - accuracy: 0.9747 - val_loss: 0.8213 - val_accuracy: 0.7894\n",
      "Epoch 615/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0428 - accuracy: 0.9806 - val_loss: 0.8482 - val_accuracy: 0.8071\n",
      "Epoch 616/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9806 - val_loss: 0.8045 - val_accuracy: 0.7835\n",
      "Epoch 617/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0597 - accuracy: 0.9797 - val_loss: 0.8771 - val_accuracy: 0.7894\n",
      "Epoch 618/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0840 - accuracy: 0.9738 - val_loss: 0.8465 - val_accuracy: 0.7835\n",
      "Epoch 619/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0657 - accuracy: 0.9789 - val_loss: 0.8484 - val_accuracy: 0.8110\n",
      "Epoch 620/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0403 - accuracy: 0.9840 - val_loss: 0.9100 - val_accuracy: 0.7717\n",
      "Epoch 621/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.9780 - val_loss: 0.9632 - val_accuracy: 0.7736\n",
      "Epoch 622/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 0.9679 - val_loss: 0.9273 - val_accuracy: 0.7717\n",
      "Epoch 623/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9789 - val_loss: 0.9220 - val_accuracy: 0.7736\n",
      "Epoch 624/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9654 - val_loss: 0.9094 - val_accuracy: 0.7835\n",
      "Epoch 625/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0728 - accuracy: 0.9755 - val_loss: 0.9785 - val_accuracy: 0.7815\n",
      "Epoch 626/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.9789 - val_loss: 0.8758 - val_accuracy: 0.7815\n",
      "Epoch 627/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0718 - accuracy: 0.9789 - val_loss: 0.8486 - val_accuracy: 0.7874\n",
      "Epoch 628/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0921 - accuracy: 0.9713 - val_loss: 0.9726 - val_accuracy: 0.7657\n",
      "Epoch 629/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9840 - val_loss: 0.9328 - val_accuracy: 0.7717\n",
      "Epoch 630/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0571 - accuracy: 0.9831 - val_loss: 0.8927 - val_accuracy: 0.8031\n",
      "Epoch 631/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0726 - accuracy: 0.9747 - val_loss: 0.9092 - val_accuracy: 0.7854\n",
      "Epoch 632/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0456 - accuracy: 0.9831 - val_loss: 0.9476 - val_accuracy: 0.7677\n",
      "Epoch 633/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0676 - accuracy: 0.9721 - val_loss: 0.9412 - val_accuracy: 0.7539\n",
      "Epoch 634/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0594 - accuracy: 0.9806 - val_loss: 0.8955 - val_accuracy: 0.7461\n",
      "Epoch 635/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0567 - accuracy: 0.9814 - val_loss: 0.8935 - val_accuracy: 0.7776\n",
      "Epoch 636/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0687 - accuracy: 0.9764 - val_loss: 0.8963 - val_accuracy: 0.7756\n",
      "Epoch 637/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9789 - val_loss: 0.8996 - val_accuracy: 0.7795\n",
      "Epoch 638/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9772 - val_loss: 0.9258 - val_accuracy: 0.7815\n",
      "Epoch 639/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0744 - accuracy: 0.9755 - val_loss: 0.9609 - val_accuracy: 0.7717\n",
      "Epoch 640/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0775 - accuracy: 0.9764 - val_loss: 0.9521 - val_accuracy: 0.7874\n",
      "Epoch 641/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9721 - val_loss: 0.9119 - val_accuracy: 0.7933\n",
      "Epoch 642/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1047 - accuracy: 0.9628 - val_loss: 0.9636 - val_accuracy: 0.7776\n",
      "Epoch 643/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9713 - val_loss: 0.8868 - val_accuracy: 0.7736\n",
      "Epoch 644/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.9721 - val_loss: 0.9321 - val_accuracy: 0.7756\n",
      "Epoch 645/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0602 - accuracy: 0.9780 - val_loss: 0.8816 - val_accuracy: 0.7815\n",
      "Epoch 646/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0739 - accuracy: 0.9789 - val_loss: 0.9392 - val_accuracy: 0.7618\n",
      "Epoch 647/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9747 - val_loss: 0.8515 - val_accuracy: 0.7815\n",
      "Epoch 648/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0598 - accuracy: 0.9831 - val_loss: 0.9167 - val_accuracy: 0.7697\n",
      "Epoch 649/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0551 - accuracy: 0.9856 - val_loss: 0.9353 - val_accuracy: 0.7756\n",
      "Epoch 650/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9662 - val_loss: 0.9032 - val_accuracy: 0.7854\n",
      "Epoch 651/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0596 - accuracy: 0.9764 - val_loss: 1.0090 - val_accuracy: 0.7520\n",
      "Epoch 652/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9671 - val_loss: 0.9116 - val_accuracy: 0.7894\n",
      "Epoch 653/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0398 - accuracy: 0.9873 - val_loss: 0.8495 - val_accuracy: 0.7795\n",
      "Epoch 654/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0673 - accuracy: 0.9772 - val_loss: 0.9516 - val_accuracy: 0.7894\n",
      "Epoch 655/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0626 - accuracy: 0.9823 - val_loss: 0.9639 - val_accuracy: 0.7854\n",
      "Epoch 656/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0600 - accuracy: 0.9789 - val_loss: 0.9698 - val_accuracy: 0.7835\n",
      "Epoch 657/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9747 - val_loss: 0.9439 - val_accuracy: 0.7933\n",
      "Epoch 658/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0517 - accuracy: 0.9764 - val_loss: 0.9819 - val_accuracy: 0.7874\n",
      "Epoch 659/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.9764 - val_loss: 0.9570 - val_accuracy: 0.7815\n",
      "Epoch 660/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9848 - val_loss: 0.9546 - val_accuracy: 0.7874\n",
      "Epoch 661/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0861 - accuracy: 0.9713 - val_loss: 0.8972 - val_accuracy: 0.7815\n",
      "Epoch 662/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9780 - val_loss: 0.8297 - val_accuracy: 0.7894\n",
      "Epoch 663/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0638 - accuracy: 0.9814 - val_loss: 0.8322 - val_accuracy: 0.7933\n",
      "Epoch 664/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0732 - accuracy: 0.9747 - val_loss: 0.9036 - val_accuracy: 0.7933\n",
      "Epoch 665/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0696 - accuracy: 0.9764 - val_loss: 0.9568 - val_accuracy: 0.7894\n",
      "Epoch 666/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0646 - accuracy: 0.9797 - val_loss: 0.8786 - val_accuracy: 0.8051\n",
      "Epoch 667/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0778 - accuracy: 0.9730 - val_loss: 1.0147 - val_accuracy: 0.7776\n",
      "Epoch 668/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0614 - accuracy: 0.9780 - val_loss: 0.8777 - val_accuracy: 0.7756\n",
      "Epoch 669/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0664 - accuracy: 0.9780 - val_loss: 0.8855 - val_accuracy: 0.7835\n",
      "Epoch 670/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0482 - accuracy: 0.9823 - val_loss: 0.8411 - val_accuracy: 0.8012\n",
      "Epoch 671/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0489 - accuracy: 0.9856 - val_loss: 0.8831 - val_accuracy: 0.7835\n",
      "Epoch 672/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9848 - val_loss: 0.9398 - val_accuracy: 0.7933\n",
      "Epoch 673/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0650 - accuracy: 0.9814 - val_loss: 0.9477 - val_accuracy: 0.7776\n",
      "Epoch 674/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0815 - accuracy: 0.9755 - val_loss: 0.9811 - val_accuracy: 0.8012\n",
      "Epoch 675/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9747 - val_loss: 0.8889 - val_accuracy: 0.7795\n",
      "Epoch 676/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0731 - accuracy: 0.9738 - val_loss: 0.9406 - val_accuracy: 0.7894\n",
      "Epoch 677/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0870 - accuracy: 0.9688 - val_loss: 0.8919 - val_accuracy: 0.7972\n",
      "Epoch 678/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0640 - accuracy: 0.9823 - val_loss: 0.9304 - val_accuracy: 0.7992\n",
      "Epoch 679/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0663 - accuracy: 0.9840 - val_loss: 0.8990 - val_accuracy: 0.7933\n",
      "Epoch 680/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0776 - accuracy: 0.9738 - val_loss: 0.9202 - val_accuracy: 0.7835\n",
      "Epoch 681/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0586 - accuracy: 0.9789 - val_loss: 0.9485 - val_accuracy: 0.7854\n",
      "Epoch 682/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0711 - accuracy: 0.9764 - val_loss: 0.9812 - val_accuracy: 0.7894\n",
      "Epoch 683/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0648 - accuracy: 0.9755 - val_loss: 0.8650 - val_accuracy: 0.7953\n",
      "Epoch 684/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0640 - accuracy: 0.9806 - val_loss: 0.8977 - val_accuracy: 0.7913\n",
      "Epoch 685/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0414 - accuracy: 0.9882 - val_loss: 0.8343 - val_accuracy: 0.8012\n",
      "Epoch 686/700\n",
      "74/74 [==============================] - 0s 6ms/step - loss: 0.0373 - accuracy: 0.9848 - val_loss: 0.8897 - val_accuracy: 0.7953\n",
      "Epoch 687/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0608 - accuracy: 0.9780 - val_loss: 0.8953 - val_accuracy: 0.7874\n",
      "Epoch 688/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0848 - accuracy: 0.9704 - val_loss: 0.8957 - val_accuracy: 0.7874\n",
      "Epoch 689/700\n",
      "74/74 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.9797 - val_loss: 0.8964 - val_accuracy: 0.7992\n",
      "Epoch 690/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0663 - accuracy: 0.9772 - val_loss: 0.9137 - val_accuracy: 0.7657\n",
      "Epoch 691/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0943 - accuracy: 0.9654 - val_loss: 0.8598 - val_accuracy: 0.7776\n",
      "Epoch 692/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0543 - accuracy: 0.9789 - val_loss: 0.9183 - val_accuracy: 0.7835\n",
      "Epoch 693/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0807 - accuracy: 0.9797 - val_loss: 0.9586 - val_accuracy: 0.7657\n",
      "Epoch 694/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0734 - accuracy: 0.9738 - val_loss: 0.8577 - val_accuracy: 0.7992\n",
      "Epoch 695/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0793 - accuracy: 0.9764 - val_loss: 0.9022 - val_accuracy: 0.7933\n",
      "Epoch 696/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0579 - accuracy: 0.9797 - val_loss: 0.9532 - val_accuracy: 0.7815\n",
      "Epoch 697/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0675 - accuracy: 0.9755 - val_loss: 0.9520 - val_accuracy: 0.7776\n",
      "Epoch 698/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0613 - accuracy: 0.9772 - val_loss: 0.9662 - val_accuracy: 0.7756\n",
      "Epoch 699/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0509 - accuracy: 0.9831 - val_loss: 0.8812 - val_accuracy: 0.7815\n",
      "Epoch 700/700\n",
      "74/74 [==============================] - 0s 5ms/step - loss: 0.0789 - accuracy: 0.9713 - val_loss: 0.9122 - val_accuracy: 0.7874\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_6 (Conv1D)           (None, 40, 16)            96        \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPoolin  (None, 20, 16)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 20, 16)            0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 20, 32)            2592      \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPoolin  (None, 10, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 10, 32)            0         \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 10, 64)            10304     \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPoolin  (None, 5, 64)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 5, 64)             0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 320)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 1605      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14597 (57.02 KB)\n",
      "Trainable params: 14597 (57.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Accuracy: 97.13%\n",
      "Validation Accuracy: 78.74%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(16,5,padding='same',activation='relu',input_shape=(40,1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(32,5,padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(64,5,padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=5,activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta=0, mode='auto')\n",
    "\n",
    "cnnhistory=model.fit(x_traincnn, np.array(y_train), batch_size=16, epochs=700, validation_data=(x_testcnn, np.array(y_test)))\n",
    "model.summary()\n",
    "print(\"Accuracy: {:.2f}%\".format(cnnhistory.history['accuracy'][-1]*100))\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(cnnhistory.history['val_accuracy'][-1]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:23:45.873435Z",
     "start_time": "2024-03-14T16:19:12.329642100Z"
    }
   },
   "id": "ac3759561878615b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "LSTM MODEL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86fc866e82a1e06e"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n",
      "37/37 [==============================] - 3s 21ms/step - loss: 12.0743 - accuracy: 0.2010 - val_loss: 3.5249 - val_accuracy: 0.1850\n",
      "Epoch 2/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 1.9265 - accuracy: 0.1706 - val_loss: 1.5901 - val_accuracy: 0.2067\n",
      "Epoch 3/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 1.5935 - accuracy: 0.2230 - val_loss: 1.5848 - val_accuracy: 0.2303\n",
      "Epoch 4/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 1.5738 - accuracy: 0.2145 - val_loss: 1.5719 - val_accuracy: 0.2185\n",
      "Epoch 5/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.5602 - accuracy: 0.2483 - val_loss: 1.5579 - val_accuracy: 0.2480\n",
      "Epoch 6/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.5358 - accuracy: 0.2846 - val_loss: 1.5965 - val_accuracy: 0.3248\n",
      "Epoch 7/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.5010 - accuracy: 0.3209 - val_loss: 1.5231 - val_accuracy: 0.3504\n",
      "Epoch 8/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.6095 - accuracy: 0.3057 - val_loss: 1.5202 - val_accuracy: 0.3268\n",
      "Epoch 9/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.5088 - accuracy: 0.3353 - val_loss: 1.4959 - val_accuracy: 0.3465\n",
      "Epoch 10/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.4709 - accuracy: 0.3530 - val_loss: 1.4401 - val_accuracy: 0.3661\n",
      "Epoch 11/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.4008 - accuracy: 0.3851 - val_loss: 1.3846 - val_accuracy: 0.4252\n",
      "Epoch 12/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.3561 - accuracy: 0.4096 - val_loss: 1.3400 - val_accuracy: 0.4193\n",
      "Epoch 13/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.2859 - accuracy: 0.4409 - val_loss: 1.3072 - val_accuracy: 0.4232\n",
      "Epoch 14/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 1.2463 - accuracy: 0.4713 - val_loss: 1.2556 - val_accuracy: 0.4764\n",
      "Epoch 15/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.2190 - accuracy: 0.4797 - val_loss: 1.2776 - val_accuracy: 0.4528\n",
      "Epoch 16/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.1982 - accuracy: 0.4772 - val_loss: 1.2223 - val_accuracy: 0.4587\n",
      "Epoch 17/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.3712 - accuracy: 0.4519 - val_loss: 1.7942 - val_accuracy: 0.2343\n",
      "Epoch 18/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.5687 - accuracy: 0.3269 - val_loss: 1.4285 - val_accuracy: 0.3878\n",
      "Epoch 19/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 1.3458 - accuracy: 0.4519 - val_loss: 1.3302 - val_accuracy: 0.4311\n",
      "Epoch 20/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.2753 - accuracy: 0.4780 - val_loss: 1.2869 - val_accuracy: 0.4547\n",
      "Epoch 21/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 1.2296 - accuracy: 0.4797 - val_loss: 1.2387 - val_accuracy: 0.4961\n",
      "Epoch 22/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 1.1893 - accuracy: 0.4848 - val_loss: 1.2180 - val_accuracy: 0.4882\n",
      "Epoch 23/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.1640 - accuracy: 0.5084 - val_loss: 1.1734 - val_accuracy: 0.5177\n",
      "Epoch 24/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.1343 - accuracy: 0.5042 - val_loss: 1.1676 - val_accuracy: 0.5000\n",
      "Epoch 25/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.1181 - accuracy: 0.5160 - val_loss: 1.1145 - val_accuracy: 0.5217\n",
      "Epoch 26/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.0723 - accuracy: 0.5448 - val_loss: 1.0495 - val_accuracy: 0.5610\n",
      "Epoch 27/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 1.0417 - accuracy: 0.5465 - val_loss: 1.0410 - val_accuracy: 0.5256\n",
      "Epoch 28/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 1.0164 - accuracy: 0.5600 - val_loss: 1.0187 - val_accuracy: 0.5335\n",
      "Epoch 29/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.9966 - accuracy: 0.5735 - val_loss: 1.0198 - val_accuracy: 0.5354\n",
      "Epoch 30/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.9490 - accuracy: 0.5836 - val_loss: 1.0480 - val_accuracy: 0.5197\n",
      "Epoch 31/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.9577 - accuracy: 0.5988 - val_loss: 0.9875 - val_accuracy: 0.5669\n",
      "Epoch 32/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.9154 - accuracy: 0.6056 - val_loss: 0.9772 - val_accuracy: 0.5689\n",
      "Epoch 33/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.8956 - accuracy: 0.6242 - val_loss: 0.9743 - val_accuracy: 0.5906\n",
      "Epoch 34/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.8775 - accuracy: 0.6191 - val_loss: 0.9363 - val_accuracy: 0.6043\n",
      "Epoch 35/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.8412 - accuracy: 0.6470 - val_loss: 0.9727 - val_accuracy: 0.5748\n",
      "Epoch 36/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.8531 - accuracy: 0.6537 - val_loss: 0.8851 - val_accuracy: 0.6142\n",
      "Epoch 37/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.8266 - accuracy: 0.6436 - val_loss: 0.9394 - val_accuracy: 0.6043\n",
      "Epoch 38/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.8642 - accuracy: 0.6453 - val_loss: 0.8866 - val_accuracy: 0.6122\n",
      "Epoch 39/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.8022 - accuracy: 0.6571 - val_loss: 0.9152 - val_accuracy: 0.5984\n",
      "Epoch 40/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.8014 - accuracy: 0.6546 - val_loss: 0.8873 - val_accuracy: 0.6240\n",
      "Epoch 41/65\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7898 - accuracy: 0.6470 - val_loss: 0.9347 - val_accuracy: 0.5886\n",
      "Epoch 42/65\n",
      "37/37 [==============================] - 1s 17ms/step - loss: 0.7970 - accuracy: 0.6402 - val_loss: 1.0262 - val_accuracy: 0.6004\n",
      "Epoch 43/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.7694 - accuracy: 0.6579 - val_loss: 0.8663 - val_accuracy: 0.6457\n",
      "Epoch 44/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.8073 - accuracy: 0.6495 - val_loss: 1.0006 - val_accuracy: 0.5709\n",
      "Epoch 45/65\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.8107 - accuracy: 0.6461 - val_loss: 0.8793 - val_accuracy: 0.6358\n",
      "Epoch 46/65\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7255 - accuracy: 0.6850 - val_loss: 0.8467 - val_accuracy: 0.6398\n",
      "Epoch 47/65\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 0.7377 - accuracy: 0.6917 - val_loss: 0.9522 - val_accuracy: 0.6063\n",
      "Epoch 48/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.7620 - accuracy: 0.6706 - val_loss: 0.8520 - val_accuracy: 0.6280\n",
      "Epoch 49/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.7308 - accuracy: 0.6926 - val_loss: 0.8595 - val_accuracy: 0.6280\n",
      "Epoch 50/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.7466 - accuracy: 0.6681 - val_loss: 0.9450 - val_accuracy: 0.6063\n",
      "Epoch 51/65\n",
      "37/37 [==============================] - 1s 16ms/step - loss: 0.7443 - accuracy: 0.6841 - val_loss: 0.9437 - val_accuracy: 0.6004\n",
      "Epoch 52/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.7253 - accuracy: 0.6867 - val_loss: 0.8086 - val_accuracy: 0.6555\n",
      "Epoch 53/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.6857 - accuracy: 0.6959 - val_loss: 0.8105 - val_accuracy: 0.6673\n",
      "Epoch 54/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.6707 - accuracy: 0.7137 - val_loss: 0.7670 - val_accuracy: 0.6634\n",
      "Epoch 55/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.6589 - accuracy: 0.7171 - val_loss: 0.7969 - val_accuracy: 0.6693\n",
      "Epoch 56/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.7335 - accuracy: 0.6588 - val_loss: 0.8390 - val_accuracy: 0.6398\n",
      "Epoch 57/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.6677 - accuracy: 0.7019 - val_loss: 0.7866 - val_accuracy: 0.6654\n",
      "Epoch 58/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.7381 - accuracy: 0.6833 - val_loss: 0.8172 - val_accuracy: 0.6614\n",
      "Epoch 59/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.7564 - accuracy: 0.6715 - val_loss: 0.7937 - val_accuracy: 0.6417\n",
      "Epoch 60/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.6870 - accuracy: 0.6926 - val_loss: 0.8824 - val_accuracy: 0.6516\n",
      "Epoch 61/65\n",
      "37/37 [==============================] - 1s 15ms/step - loss: 0.7006 - accuracy: 0.7120 - val_loss: 0.8623 - val_accuracy: 0.6142\n",
      "Epoch 62/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.7788 - accuracy: 0.6639 - val_loss: 0.8832 - val_accuracy: 0.6142\n",
      "Epoch 63/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.6950 - accuracy: 0.6976 - val_loss: 0.8437 - val_accuracy: 0.6634\n",
      "Epoch 64/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.6751 - accuracy: 0.7171 - val_loss: 0.8174 - val_accuracy: 0.6398\n",
      "Epoch 65/65\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 0.6937 - accuracy: 0.7010 - val_loss: 0.7939 - val_accuracy: 0.6673\n",
      "Accuracy: 70.10%\n",
      "Validation Accuracy: 66.73%\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.add(LSTM(100, activation='relu', input_shape=(x_traincnn.shape[1],1)))\n",
    "model_lstm.add(Dense(5, activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "early_stop1 = EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta=0, mode='auto')\n",
    "\n",
    "lstmhistory=model_lstm.fit(x_traincnn, np.array(y_train), batch_size=32, epochs=65, validation_data=(x_testcnn, np.array(y_test)))\n",
    "print(\"Accuracy: {:.2f}%\".format(lstmhistory.history['accuracy'][-1]*100))\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(lstmhistory.history['val_accuracy'][-1]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T03:56:42.535784700Z",
     "start_time": "2024-02-28T03:56:05.395890500Z"
    }
   },
   "id": "8b8ef08de918f350"
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNN MODEL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "198c2f312c00e343"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n",
      "37/37 [==============================] - 2s 16ms/step - loss: 1.5438 - accuracy: 0.2821 - val_loss: 1.5022 - val_accuracy: 0.3031\n",
      "Epoch 2/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.4443 - accuracy: 0.3319 - val_loss: 1.4367 - val_accuracy: 0.3425\n",
      "Epoch 3/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.3735 - accuracy: 0.3851 - val_loss: 1.3907 - val_accuracy: 0.3622\n",
      "Epoch 4/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.3320 - accuracy: 0.4071 - val_loss: 1.3601 - val_accuracy: 0.3760\n",
      "Epoch 5/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.2868 - accuracy: 0.4383 - val_loss: 1.3356 - val_accuracy: 0.3819\n",
      "Epoch 6/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.2630 - accuracy: 0.4459 - val_loss: 1.3372 - val_accuracy: 0.3996\n",
      "Epoch 7/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.2448 - accuracy: 0.4671 - val_loss: 1.2772 - val_accuracy: 0.4134\n",
      "Epoch 8/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.2081 - accuracy: 0.4907 - val_loss: 1.2759 - val_accuracy: 0.4232\n",
      "Epoch 9/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.1939 - accuracy: 0.4882 - val_loss: 1.2711 - val_accuracy: 0.4134\n",
      "Epoch 10/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.1700 - accuracy: 0.4941 - val_loss: 1.2645 - val_accuracy: 0.4272\n",
      "Epoch 11/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.1649 - accuracy: 0.5051 - val_loss: 1.2610 - val_accuracy: 0.4311\n",
      "Epoch 12/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.1364 - accuracy: 0.5101 - val_loss: 1.2681 - val_accuracy: 0.4252\n",
      "Epoch 13/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.1158 - accuracy: 0.5296 - val_loss: 1.2164 - val_accuracy: 0.4646\n",
      "Epoch 14/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.0726 - accuracy: 0.5515 - val_loss: 1.2017 - val_accuracy: 0.4705\n",
      "Epoch 15/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 1.1000 - accuracy: 0.5515 - val_loss: 1.1852 - val_accuracy: 0.4705\n",
      "Epoch 16/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.0559 - accuracy: 0.5456 - val_loss: 1.1766 - val_accuracy: 0.4882\n",
      "Epoch 17/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 1.0414 - accuracy: 0.5743 - val_loss: 1.1854 - val_accuracy: 0.4724\n",
      "Epoch 18/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.0165 - accuracy: 0.5709 - val_loss: 1.2151 - val_accuracy: 0.4685\n",
      "Epoch 19/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.9987 - accuracy: 0.5870 - val_loss: 1.1284 - val_accuracy: 0.5118\n",
      "Epoch 20/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.9922 - accuracy: 0.5777 - val_loss: 1.1444 - val_accuracy: 0.4980\n",
      "Epoch 21/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.9670 - accuracy: 0.5912 - val_loss: 1.1480 - val_accuracy: 0.4980\n",
      "Epoch 22/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.9594 - accuracy: 0.5853 - val_loss: 1.2093 - val_accuracy: 0.4803\n",
      "Epoch 23/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 1.0518 - accuracy: 0.5515 - val_loss: 1.1834 - val_accuracy: 0.4705\n",
      "Epoch 24/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.9857 - accuracy: 0.5845 - val_loss: 1.1374 - val_accuracy: 0.4902\n",
      "Epoch 25/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.9569 - accuracy: 0.5997 - val_loss: 1.1354 - val_accuracy: 0.5256\n",
      "Epoch 26/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.9268 - accuracy: 0.6157 - val_loss: 1.1980 - val_accuracy: 0.5079\n",
      "Epoch 27/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.9193 - accuracy: 0.6208 - val_loss: 1.1359 - val_accuracy: 0.5138\n",
      "Epoch 28/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.9013 - accuracy: 0.6242 - val_loss: 1.1021 - val_accuracy: 0.5335\n",
      "Epoch 29/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.9030 - accuracy: 0.6284 - val_loss: 1.0997 - val_accuracy: 0.5374\n",
      "Epoch 30/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.8830 - accuracy: 0.6427 - val_loss: 1.1158 - val_accuracy: 0.5354\n",
      "Epoch 31/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.8880 - accuracy: 0.6453 - val_loss: 1.2451 - val_accuracy: 0.5098\n",
      "Epoch 32/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.8798 - accuracy: 0.6419 - val_loss: 1.0997 - val_accuracy: 0.5374\n",
      "Epoch 33/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.8656 - accuracy: 0.6402 - val_loss: 1.1103 - val_accuracy: 0.5472\n",
      "Epoch 34/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.8532 - accuracy: 0.6486 - val_loss: 1.1005 - val_accuracy: 0.5256\n",
      "Epoch 35/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.8442 - accuracy: 0.6605 - val_loss: 1.0607 - val_accuracy: 0.5531\n",
      "Epoch 36/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.8165 - accuracy: 0.6681 - val_loss: 1.0961 - val_accuracy: 0.5315\n",
      "Epoch 37/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.8213 - accuracy: 0.6630 - val_loss: 1.1296 - val_accuracy: 0.5394\n",
      "Epoch 38/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.8278 - accuracy: 0.6655 - val_loss: 1.1322 - val_accuracy: 0.5217\n",
      "Epoch 39/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.8217 - accuracy: 0.6664 - val_loss: 1.1156 - val_accuracy: 0.5492\n",
      "Epoch 40/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.7985 - accuracy: 0.6833 - val_loss: 1.1356 - val_accuracy: 0.5138\n",
      "Epoch 41/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.8013 - accuracy: 0.6740 - val_loss: 1.1127 - val_accuracy: 0.5669\n",
      "Epoch 42/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.7758 - accuracy: 0.6841 - val_loss: 1.0817 - val_accuracy: 0.5610\n",
      "Epoch 43/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7700 - accuracy: 0.6875 - val_loss: 1.1255 - val_accuracy: 0.5295\n",
      "Epoch 44/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7804 - accuracy: 0.6833 - val_loss: 1.1144 - val_accuracy: 0.5433\n",
      "Epoch 45/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7502 - accuracy: 0.7035 - val_loss: 1.0955 - val_accuracy: 0.5512\n",
      "Epoch 46/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7274 - accuracy: 0.7128 - val_loss: 1.1166 - val_accuracy: 0.5335\n",
      "Epoch 47/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.7198 - accuracy: 0.7078 - val_loss: 1.1604 - val_accuracy: 0.5433\n",
      "Epoch 48/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7410 - accuracy: 0.6976 - val_loss: 1.1703 - val_accuracy: 0.5413\n",
      "Epoch 49/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7395 - accuracy: 0.6951 - val_loss: 1.1061 - val_accuracy: 0.5630\n",
      "Epoch 50/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7138 - accuracy: 0.7213 - val_loss: 1.1529 - val_accuracy: 0.5354\n",
      "Epoch 51/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.7527 - accuracy: 0.6968 - val_loss: 1.1810 - val_accuracy: 0.5315\n",
      "Epoch 52/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7394 - accuracy: 0.7069 - val_loss: 1.1185 - val_accuracy: 0.5591\n",
      "Epoch 53/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.7144 - accuracy: 0.6976 - val_loss: 1.1293 - val_accuracy: 0.5276\n",
      "Epoch 54/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7368 - accuracy: 0.7019 - val_loss: 1.1627 - val_accuracy: 0.5315\n",
      "Epoch 55/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.6885 - accuracy: 0.7289 - val_loss: 1.1431 - val_accuracy: 0.5295\n",
      "Epoch 56/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.6950 - accuracy: 0.7247 - val_loss: 1.1338 - val_accuracy: 0.5512\n",
      "Epoch 57/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.6892 - accuracy: 0.7213 - val_loss: 1.1472 - val_accuracy: 0.5669\n",
      "Epoch 58/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.6803 - accuracy: 0.7280 - val_loss: 1.1796 - val_accuracy: 0.5512\n",
      "Epoch 59/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.7204 - val_loss: 1.1466 - val_accuracy: 0.5492\n",
      "Epoch 60/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.6674 - accuracy: 0.7272 - val_loss: 1.1430 - val_accuracy: 0.5315\n",
      "Epoch 61/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.7135 - accuracy: 0.7078 - val_loss: 1.1819 - val_accuracy: 0.5138\n",
      "Epoch 62/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.6658 - accuracy: 0.7323 - val_loss: 1.2135 - val_accuracy: 0.5276\n",
      "Epoch 63/65\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.7196 - val_loss: 1.1568 - val_accuracy: 0.5630\n",
      "Epoch 64/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.6351 - accuracy: 0.7627 - val_loss: 1.1625 - val_accuracy: 0.5650\n",
      "Epoch 65/65\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.6152 - accuracy: 0.7576 - val_loss: 1.1556 - val_accuracy: 0.5571\n",
      "Accuracy: 75.76%\n",
      "Validation Accuracy: 55.71%\n"
     ]
    }
   ],
   "source": [
    "model_rnn = Sequential()\n",
    "model_rnn.add(SimpleRNN(50, activation='relu', input_shape=(x_traincnn.shape[1],1)))\n",
    "model_rnn.add(Dense(5, activation='softmax'))\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "early_stop2 = EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta=0, mode='auto')\n",
    "\n",
    "rnnhistory=model_rnn.fit(x_traincnn, np.array(y_train), batch_size=32, epochs=65, validation_data=(x_testcnn, np.array(y_test)))\n",
    "print(\"Accuracy: {:.2f}%\".format(rnnhistory.history['accuracy'][-1]*100))\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(rnnhistory.history['val_accuracy'][-1]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T03:57:05.695629200Z",
     "start_time": "2024-02-28T03:56:45.792956900Z"
    }
   },
   "id": "e21b069632829b85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "DNN MODEL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53d7e8606e9b6760"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n",
      "37/37 [==============================] - 2s 8ms/step - loss: 4.1352 - accuracy: 0.2356 - val_loss: 1.6236 - val_accuracy: 0.2913\n",
      "Epoch 2/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5624 - accuracy: 0.3201 - val_loss: 1.5058 - val_accuracy: 0.2933\n",
      "Epoch 3/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4452 - accuracy: 0.3674 - val_loss: 1.4337 - val_accuracy: 0.3878\n",
      "Epoch 4/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3890 - accuracy: 0.4139 - val_loss: 1.3353 - val_accuracy: 0.4272\n",
      "Epoch 5/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3366 - accuracy: 0.4265 - val_loss: 1.3120 - val_accuracy: 0.4232\n",
      "Epoch 6/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4448 - accuracy: 0.4037 - val_loss: 1.2657 - val_accuracy: 0.4409\n",
      "Epoch 7/65\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2708 - accuracy: 0.4611 - val_loss: 1.2393 - val_accuracy: 0.4331\n",
      "Epoch 8/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2246 - accuracy: 0.4966 - val_loss: 1.2218 - val_accuracy: 0.4508\n",
      "Epoch 9/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2438 - accuracy: 0.4772 - val_loss: 1.3350 - val_accuracy: 0.4783\n",
      "Epoch 10/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2319 - accuracy: 0.4882 - val_loss: 1.2529 - val_accuracy: 0.4705\n",
      "Epoch 11/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2521 - accuracy: 0.4856 - val_loss: 1.2182 - val_accuracy: 0.4528\n",
      "Epoch 12/65\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.1878 - accuracy: 0.5101 - val_loss: 1.2246 - val_accuracy: 0.5098\n",
      "Epoch 13/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1438 - accuracy: 0.5287 - val_loss: 1.2135 - val_accuracy: 0.4724\n",
      "Epoch 14/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1513 - accuracy: 0.5262 - val_loss: 1.1552 - val_accuracy: 0.5472\n",
      "Epoch 15/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1023 - accuracy: 0.5465 - val_loss: 1.2810 - val_accuracy: 0.4291\n",
      "Epoch 16/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1280 - accuracy: 0.5541 - val_loss: 1.0911 - val_accuracy: 0.5768\n",
      "Epoch 17/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0897 - accuracy: 0.5667 - val_loss: 1.0821 - val_accuracy: 0.5650\n",
      "Epoch 18/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0297 - accuracy: 0.5861 - val_loss: 1.1007 - val_accuracy: 0.5433\n",
      "Epoch 19/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0905 - accuracy: 0.5448 - val_loss: 1.1115 - val_accuracy: 0.5335\n",
      "Epoch 20/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1045 - accuracy: 0.5507 - val_loss: 1.1887 - val_accuracy: 0.5020\n",
      "Epoch 21/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0412 - accuracy: 0.5811 - val_loss: 1.1241 - val_accuracy: 0.5295\n",
      "Epoch 22/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9811 - accuracy: 0.5980 - val_loss: 1.0775 - val_accuracy: 0.5630\n",
      "Epoch 23/65\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.0337 - accuracy: 0.5954 - val_loss: 1.2333 - val_accuracy: 0.5531\n",
      "Epoch 24/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0147 - accuracy: 0.6022 - val_loss: 1.0401 - val_accuracy: 0.5669\n",
      "Epoch 25/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0785 - accuracy: 0.5718 - val_loss: 1.2273 - val_accuracy: 0.4980\n",
      "Epoch 26/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9990 - accuracy: 0.6064 - val_loss: 0.9980 - val_accuracy: 0.5945\n",
      "Epoch 27/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9576 - accuracy: 0.6309 - val_loss: 1.0103 - val_accuracy: 0.5866\n",
      "Epoch 28/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9959 - accuracy: 0.6014 - val_loss: 1.0568 - val_accuracy: 0.5650\n",
      "Epoch 29/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9696 - accuracy: 0.6047 - val_loss: 1.0890 - val_accuracy: 0.5394\n",
      "Epoch 30/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0288 - accuracy: 0.5921 - val_loss: 1.0669 - val_accuracy: 0.5748\n",
      "Epoch 31/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9360 - accuracy: 0.6174 - val_loss: 1.0073 - val_accuracy: 0.6083\n",
      "Epoch 32/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9635 - accuracy: 0.6073 - val_loss: 0.9642 - val_accuracy: 0.6161\n",
      "Epoch 33/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9740 - accuracy: 0.6267 - val_loss: 0.9902 - val_accuracy: 0.5945\n",
      "Epoch 34/65\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9374 - accuracy: 0.6250 - val_loss: 0.9199 - val_accuracy: 0.6181\n",
      "Epoch 35/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9571 - accuracy: 0.6157 - val_loss: 0.9710 - val_accuracy: 0.6220\n",
      "Epoch 36/65\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9219 - accuracy: 0.6453 - val_loss: 0.9493 - val_accuracy: 0.6339\n",
      "Epoch 37/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8867 - accuracy: 0.6706 - val_loss: 0.9236 - val_accuracy: 0.6201\n",
      "Epoch 38/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8324 - accuracy: 0.6740 - val_loss: 0.9574 - val_accuracy: 0.5846\n",
      "Epoch 39/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9461 - accuracy: 0.6453 - val_loss: 0.9768 - val_accuracy: 0.5906\n",
      "Epoch 40/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8755 - accuracy: 0.6622 - val_loss: 0.8875 - val_accuracy: 0.6378\n",
      "Epoch 41/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8518 - accuracy: 0.6613 - val_loss: 1.0143 - val_accuracy: 0.5787\n",
      "Epoch 42/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8592 - accuracy: 0.6672 - val_loss: 0.9883 - val_accuracy: 0.6142\n",
      "Epoch 43/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8224 - accuracy: 0.6765 - val_loss: 0.8822 - val_accuracy: 0.6457\n",
      "Epoch 44/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8352 - accuracy: 0.6715 - val_loss: 0.9213 - val_accuracy: 0.6339\n",
      "Epoch 45/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8075 - accuracy: 0.6782 - val_loss: 0.9764 - val_accuracy: 0.6102\n",
      "Epoch 46/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7799 - accuracy: 0.6900 - val_loss: 0.9139 - val_accuracy: 0.6299\n",
      "Epoch 47/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8126 - accuracy: 0.6841 - val_loss: 0.9445 - val_accuracy: 0.6142\n",
      "Epoch 48/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8208 - accuracy: 0.6689 - val_loss: 0.9623 - val_accuracy: 0.6063\n",
      "Epoch 49/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8130 - accuracy: 0.6715 - val_loss: 0.9476 - val_accuracy: 0.6496\n",
      "Epoch 50/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7547 - accuracy: 0.7061 - val_loss: 0.9990 - val_accuracy: 0.6240\n",
      "Epoch 51/65\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7726 - accuracy: 0.6985 - val_loss: 0.8699 - val_accuracy: 0.6614\n",
      "Epoch 52/65\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7713 - accuracy: 0.6993 - val_loss: 0.8400 - val_accuracy: 0.6693\n",
      "Epoch 53/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7692 - accuracy: 0.7019 - val_loss: 1.0681 - val_accuracy: 0.5906\n",
      "Epoch 54/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8611 - accuracy: 0.6588 - val_loss: 0.9059 - val_accuracy: 0.6516\n",
      "Epoch 55/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7608 - accuracy: 0.6985 - val_loss: 0.9322 - val_accuracy: 0.6260\n",
      "Epoch 56/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7326 - accuracy: 0.7145 - val_loss: 0.8865 - val_accuracy: 0.6457\n",
      "Epoch 57/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8148 - accuracy: 0.6672 - val_loss: 0.9792 - val_accuracy: 0.6043\n",
      "Epoch 58/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7809 - accuracy: 0.6799 - val_loss: 0.9832 - val_accuracy: 0.5925\n",
      "Epoch 59/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7986 - accuracy: 0.6943 - val_loss: 0.9674 - val_accuracy: 0.6142\n",
      "Epoch 60/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7436 - accuracy: 0.6959 - val_loss: 0.8376 - val_accuracy: 0.6693\n",
      "Epoch 61/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6997 - accuracy: 0.7196 - val_loss: 0.8878 - val_accuracy: 0.6555\n",
      "Epoch 62/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6991 - accuracy: 0.7213 - val_loss: 0.8898 - val_accuracy: 0.6417\n",
      "Epoch 63/65\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.7053 - accuracy: 0.7213 - val_loss: 0.8077 - val_accuracy: 0.6791\n",
      "Epoch 64/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7096 - accuracy: 0.7297 - val_loss: 0.8549 - val_accuracy: 0.6614\n",
      "Epoch 65/65\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7051 - accuracy: 0.7120 - val_loss: 0.8382 - val_accuracy: 0.6772\n",
      "Accuracy: 71.20%\n",
      "Validation Accuracy: 67.72%\n"
     ]
    }
   ],
   "source": [
    "model_dnn = Sequential()\n",
    "model_dnn.add(Flatten())\n",
    "model_dnn.add(Dense(128, activation='relu', input_shape=(40, 1)))\n",
    "model_dnn.add(Dense(64, activation='relu'))\n",
    "#model_dnn.add(Dense(64, activation='relu'))\n",
    "model_dnn.add(Dense(32, activation='relu'))\n",
    "model_dnn.add(Dense(16, activation='relu'))\n",
    "model_dnn.add(Dense (5, activation='softmax'))\n",
    "\n",
    "model_dnn.compile(loss='categorical_crossentropy', optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "dnnhistory=model_dnn.fit(x_traindnn, np.array(y_train), batch_size=32, epochs=65, validation_data=(x_testdnn, np.array(y_test)))\n",
    "print(\"Accuracy: {:.2f}%\".format(dnnhistory.history['accuracy'][-1]*100))\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(dnnhistory.history['val_accuracy'][-1]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T03:57:17.434787800Z",
     "start_time": "2024-02-28T03:57:08.793048600Z"
    }
   },
   "id": "cd71f46819af501"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c09f62e9991db0d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "CNN-RNN HYBRID MODEL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a74d9ddbca9047d3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_9 (Conv1D)           (None, 4, 32)             160       \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 4, 32)             8320      \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPoolin  (None, 2, 32)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 2, 16)             4112      \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 2, 64)             20736     \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPoolin  (None, 1, 64)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 1, 16)             8208      \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 128)               74240     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116421 (454.77 KB)\n",
      "Trainable params: 116421 (454.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/65\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 4, 1), found shape=(32, 40, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m model_hybrid\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     14\u001B[0m model_hybrid\u001B[38;5;241m.\u001B[39msummary()\n\u001B[1;32m---> 16\u001B[0m hybridhistory \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_hybrid\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_traincnn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m65\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy: \u001B[39m\u001B[38;5;132;01m{:.2f}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(hybridhistory\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m))\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation Accuracy: \u001B[39m\u001B[38;5;132;01m{:.2f}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(hybridhistory\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefdigjd1o.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\mirza\\PycharmProjects\\FYP(Music Recommender SER)\\.venv\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 4, 1), found shape=(32, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "model_hybrid = Sequential()\n",
    "\n",
    "model_hybrid.add(Conv1D(32, 4, activation='relu', padding='same', input_shape=(4, 1)))\n",
    "model_hybrid.add(LSTM(32, return_sequences=True))\n",
    "model_hybrid.add(MaxPooling1D(2))\n",
    "model_hybrid.add(Conv1D(16, 8, activation=\"relu\", padding='same'))\n",
    "model_hybrid.add(LSTM(64, return_sequences=True))\n",
    "model_hybrid.add(MaxPooling1D(2))\n",
    "model_hybrid.add(Conv1D(16, 8, activation=\"relu\", padding='same'))\n",
    "model_hybrid.add(LSTM(128))\n",
    "model_hybrid.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_hybrid.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_hybrid.summary()\n",
    "\n",
    "hybridhistory = model_hybrid.fit(x_traincnn, np.array(y_train), batch_size=32, epochs=65)\n",
    "print(\"Accuracy: {:.2f}%\".format(hybridhistory.history['accuracy'][-1]*100))\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(hybridhistory.history['val_accuracy'][-1]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T18:08:25.360141100Z",
     "start_time": "2024-03-20T18:08:23.864525100Z"
    }
   },
   "id": "5f4056661651279c",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN (Accuracy: 71.20%\tValidation Accuracy: 67.72%)\n",
      "CNN (Accuracy: 80.41%\tValidation Accuracy: 75.00%)\n",
      "LSTM (Accuracy: 70.10%\tValidation Accuracy: 66.73%)\n"
     ]
    }
   ],
   "source": [
    "print(\"DNN (Accuracy: {:.2f}%\".format(dnnhistory.history['accuracy'][-1]*100) + \"\\tValidation Accuracy: {:.2f}%)\".format(dnnhistory.history['val_accuracy'][-1]*100))\n",
    "print(\"CNN (Accuracy: {:.2f}%\".format(cnnhistory.history['accuracy'][-1]*100) + \"\\tValidation Accuracy: {:.2f}%)\".format(cnnhistory.history['val_accuracy'][-1]*100))\n",
    "print(\"LSTM (Accuracy: {:.2f}%\".format(lstmhistory.history['accuracy'][-1]*100) + \"\\tValidation Accuracy: {:.2f}%)\".format(lstmhistory.history['val_accuracy'][-1]*100))\n",
    "#print(\"RNN (Accuracy: {:.2f}%\".format(rnnhistory.history['accuracy'][-1]*100) + \"\\tValidation Accuracy: {:.2f}%)\".format(rnnhistory.history['val_accuracy'][-1]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T03:57:18.933537600Z",
     "start_time": "2024-02-28T03:57:18.927215600Z"
    }
   },
   "id": "2173b39a9b0b7deb"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CNN model at model/CNN_model.keras\n"
     ]
    }
   ],
   "source": [
    "cnn_model_path = 'model/CNN_model.keras'\n",
    "model.save(cnn_model_path)\n",
    "print('Saved CNN model at %s' % cnn_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T15:56:08.602223100Z",
     "start_time": "2024-03-14T15:56:08.492505700Z"
    }
   },
   "id": "ffc842a1bfcff578"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m cnn_model_json \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mto_json()\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel/CNN_model.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m json_file:\n\u001B[0;32m      3\u001B[0m     json_file\u001B[38;5;241m.\u001B[39mwrite(cnn_model_json)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "cnn_model_json = model.to_json()\n",
    "with open(\"model/CNN_model.json\", \"w\") as json_file:\n",
    "    json_file.write(cnn_model_json)\n",
    "    print('Saved CNN model architecture in JSON format')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T15:55:18.721705200Z",
     "start_time": "2024-03-14T15:55:18.678986200Z"
    }
   },
   "id": "7a5c9ba3303b47ab"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "testing = extract_features('test/hi.wav', mfcc=True, chroma=False, mel=False)\n",
    "live = testing\n",
    "live = pd.DataFrame(live)\n",
    "live = live.stack().to_frame().T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:18:05.194988500Z",
     "start_time": "2024-03-14T16:18:05.085762200Z"
    }
   },
   "id": "3f76e141048050ac"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "           0           1          2        3          4         5          6   \\\n            0           0          0        0          0         0          0   \n0 -440.154572  174.390961 -32.720245  4.98528  17.061033 -4.497399  10.589602   \n\n         7        8         9   ...        30        31        32        33  \\\n          0        0         0  ...         0         0         0         0   \n0  1.622181 -5.33541 -7.630628  ... -3.490438 -5.427524 -5.014283 -0.404643   \n\n         34        35        36        37        38        39  \n          0         0         0         0         0         0  \n0 -1.671365 -6.687959 -3.230276 -0.485932 -3.645858 -4.259221  \n\n[1 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>...</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-440.154572</td>\n      <td>174.390961</td>\n      <td>-32.720245</td>\n      <td>4.98528</td>\n      <td>17.061033</td>\n      <td>-4.497399</td>\n      <td>10.589602</td>\n      <td>1.622181</td>\n      <td>-5.33541</td>\n      <td>-7.630628</td>\n      <td>...</td>\n      <td>-3.490438</td>\n      <td>-5.427524</td>\n      <td>-5.014283</td>\n      <td>-0.404643</td>\n      <td>-1.671365</td>\n      <td>-6.687959</td>\n      <td>-3.230276</td>\n      <td>-0.485932</td>\n      <td>-3.645858</td>\n      <td>-4.259221</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows  40 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "live"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:18:08.035151200Z",
     "start_time": "2024-03-14T16:18:07.975472200Z"
    }
   },
   "id": "6d90f0bb2d9f7457"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "input_test = np.expand_dims(live, axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:18:11.570279400Z",
     "start_time": "2024-03-14T16:18:11.525822400Z"
    }
   },
   "id": "7a79b1a096428ab"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 40, 1)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:18:12.989658300Z",
     "start_time": "2024-03-14T16:18:12.945710300Z"
    }
   },
   "id": "c485a68deeeec7af"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "1/1 [==============================] - 0s 110ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('model/CNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model/Testmodel.keras\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#score = loaded_model.evaluate(, y_test, verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n",
    "\n",
    "preds = loaded_model.predict(input_test, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:18:15.037801700Z",
     "start_time": "2024-03-14T16:18:14.693491Z"
    }
   },
   "id": "600da37080b74c05"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "\n",
    "preds1=preds.argmax(axis=1)\n",
    "\n",
    "abc = preds1.astype(int).flatten()\n",
    "predictions = (lb.inverse_transform((abc)))\n",
    "\n",
    "preddf = pd.DataFrame({'predictedvalues': predictions})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:18:17.074390700Z",
     "start_time": "2024-03-14T16:18:17.050167100Z"
    }
   },
   "id": "9c273dfc876b5f60"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "  predictedvalues\n0         neutral",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>predictedvalues</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:18:19.628629Z",
     "start_time": "2024-03-14T16:18:19.595203300Z"
    }
   },
   "id": "a4a55f645ae85f9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a6ec9a8364252c12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
